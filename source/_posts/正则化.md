---
title: Regularization
date: 2023/3/14 20:46:25
categories:
  - [ML, Basic]
---

.

<!-- more -->

# 什么是正则化

目的：防止模型过拟合

原理：在损失函数上加上某些规则（限制），缩小解空间，从而减少求出过拟合解的可能性

https://www.zhihu.com/question/20924039

最直接的防止过拟合的方法就是减少特征数量，就是减少0范数（向量中非零元素的个数），但是0范数很难求，所以就有了1范数，2范数。

# 常见正则化

## l1正则化

$$
l1=\lambda||\vec{w}||_1=\sum_i|w_i|
$$

$\lambda$控制约束程度

l1不仅可以**约束参数量**，还可以使**参数更稀疏**。因为对目标函数经过优化后，一部分参数会变为0，另一部分参数为非零实值。**非零实值说明这部分参数是最重要的特征**。

### 稀疏原因

https://blog.csdn.net/b876144622/article/details/81276818

https://www.zhihu.com/question/37096933/answer/70426653

0处导数突变，如果此时0+导数为正，优化时放负方向跑，0-导数为负数，优化时往正方向跑，就很容易落入0

![image-20230314211808962](https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230314211808962.png)

### 缺点

L1正则要算绝对值，算绝对值比较麻烦；直接平方要比算绝对值来得简单，这一点上**L2正则**计算更加简便**（优化时求导方便）**。

## l2正则化

$$
l2=\frac{1}{2}\lambda||\vec{w}||_2^2=\sum_i|w_i|^2
$$

l2正则化会使部分特征**趋近于0**，也就达到正则化的目的了。

此外，l1正则化和l2正则化也可以联合使用，这种形式也被称为“**Elastic网络正则化**”。

## Dropout

在训练的时候让一定量的神经元失活，在该epoch中不参与网络训练

## 早停

每一个epoch训练结束后使用**验证集**验证模型效果，画出训练曲线，这样就可以判断是否过拟合了。当发现网络有点过拟合了，当然就是“**早停**”了，可以直接停止训练了。

## 扩充数据集

Augmentation，增加变化增加多样性

## BN（Batch Normalization）

目的：用于解决深度网络**梯度消失**和**梯度爆炸**的问题，加速网络收敛速度。

批规范化，即在模型每次随机梯度下降训练时，通过mini-batch来对每一层的输出做**规范化操作**，使得结果（各个维度）的**均值为0**，**方差为1**，然后在进行尺度变换和偏移。

![image-20230316191341497](https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230316191341497.png)

m是mini-batch中的数据个数。前面的散步是对input数据进行白化操作（线性），**最后的“尺度变换和偏移”操作是为了让BN能够在线性和非线性之间做一个权衡**，而这个偏移的参数是神经网络在训练时学出来的。

经过BN操作，网络每一层的输出小值被“拉大”，大值被“缩小”，所以就有效避免了梯度消失和梯度爆炸。**总而言之，BN是一个可学习、有参数（γ、β）的网络层**。

### 尺度变换和偏移的作用：

归一会影响到本层网络A所学习到的特征（比如网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，如果强制把它给归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于这一层网络所学习到的特征分布**被搞坏**了）

于是**BN**最后的“**尺度变换和偏移**”操作，让我们的网络可以学习恢复出原始网络所要学习的特征分布（衡量线性和非线性）

### BN训练和测试有什么不同

训练时，均值和方差针对一个**Batch**。

测试时，均值和方差针对**整个数据集**而言。因此，在训练过程中除了正常的前向传播和反向求导之外，我们还要记录**每一个Batch的均值和方差**。

### BN和LN的差别

LN：Layer Normalization，LN是“横”着来的，**对一个样本，经过同一层的所有神经元**做**归一化**。LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；LN不依赖于batch的大小和输入sequence的深度，因此可以用于**batchsize为1**和RNN中对边长的输入sequence的normalize操作。

BN：Batch Normalization，BN是“竖”着来的，**经过一个神经元的所有样本**做**归一化**，所以与**batch size**有关系。

二者提出的目的都是为了加快模型收敛，减少训练时间。

## Bagging 和Bootstrap？

**Bootstrap**是一种抽样方法，即随机抽取数据并将其放回。如一次抽取一个样本，然后放回样本集中，下次可能再抽取这个样本。接着将每轮未抽取的数据合并形成**袋外数据集**（Out of Bag, OOB），用于模型中的测试集。

**Bagging算法**使用**Bootstrap方法**从原始样本集中随机抽取样本。共提取K个轮次，得到K个独立的训练集，元素可以重复。用K个训练集训练K个模型。分类问题以结果中的多个值投票作为最终结果，回归问题以平均值作为最终结果。结果采用投票法，避免了决策树的过拟合问题。

**Boosting**是为每个训练样本设置一个权重，在下一轮分类中，误分类的样本权重较大，即每轮样本相同，但样本权重不同；对于分类器来说，分类误差小的分类器权重较大，反之则小。

**采用模型融合的方式也可以避免过拟合**。

# 常见正则化问题

