<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>算法</title>
    <url>/2023/02/14/Algorithm_example/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="动态规划">动态规划</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">有一排深渊法师，他们的护盾值用数组 a 表示；</span><br><span class="line">你可以对深渊法师这样破盾：</span><br><span class="line">1) 用重击破盾，每消耗1MP破1点护盾</span><br><span class="line">2) 如果有两个相邻的深渊法师他们的元素不同，且护盾都不为0，你可以对他们俩使用高天之歌，消耗xMP破所有护盾</span><br><span class="line">问最少消耗的MP？</span><br><span class="line">下面输入的 els 表示深渊法师的元素，I 表示冰，W 表示水，F 表示火</span><br><span class="line"></span><br><span class="line">输入：</span><br><span class="line">a = [4, 8, 10, 2, 15, 2]</span><br><span class="line">x = 9</span><br><span class="line">els = WIFFII</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i]=min(dp[i-1]+a[i],dp[i-2]+x) if 能使用高天之歌 else dp[i-1]+a[i]</span><br></pre></td></tr></table></figure>
<h1 id="找规律题目">找规律题目</h1>
<h2 id="找规律后拼接">找规律后拼接</h2>
<h3 id="构建长度为n的字符串">构建长度为n的字符串</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A希望你构造一个长度为n的数组，满足以下条件：</span><br><span class="line">1. 所有元素绝对值不大于3</span><br><span class="line">2. 相邻两个元素乘积小于0，且和不为0</span><br><span class="line">3. 所有元素之和=0</span><br><span class="line">input: 2 output:no answer</span><br><span class="line">input: 3 output: -1 2 -1</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">根据题目信息：</span><br><span class="line">1. 数组的元素在&#123;1,2,3&#125;里挑选</span><br><span class="line">2. 数组相邻元素一个为正一个为负，且互不相等，正号组和负号组可以互换。</span><br><span class="line">3. 正负号数值和相同，且可拼接！！！因为0+0=0</span><br><span class="line"></span><br><span class="line">考虑拼接条件：只要首尾元素不相同就可以拼接！！</span><br><span class="line"></span><br><span class="line">开始找规律以及能用于拼接的元素：</span><br><span class="line">input: 2 output:no answer</span><br><span class="line">input: 3 output: -1 3 -2（这个可以用于拼接，首位元素不同） -1 2 -1（这个不可以）</span><br><span class="line">input: 4 output: -1 2 -3 2（这个可以用于拼接，首位元素不同）</span><br><span class="line">input: 5 output: no answer</span><br><span class="line">我们发现所以大于5的数都可以由若干个3和若干个4相加得到，且3、4中有答案可以随意拼接，所以我们能利用3、4的答案拼接出n&gt;5的满足条件的数组</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="拼接mhy字符串">拼接mhy字符串</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">如果不能定义：字符串的“权重”是字符串中出现的字符种类数量，比如 mmm 权重为1，mmh 为 2，mhy 为 3</span><br><span class="line">输入：x, y, z 三个变量</span><br><span class="line">输出：一个长度为 x + y + z + 2 的字符串，这个字符串只能由 m h y 三种字符组成，这个字符串一共有 x + y + z 个长度为 3 的子串，其中有 x 个权重为 1 的子串，y 个权重为 2 的子串，z 个权重为 3 的子串。</span><br><span class="line">只用输出一种情况，若无法组成，输出-1</span><br><span class="line"></span><br><span class="line">范例输入：x = 2, y = 1, z = 1</span><br><span class="line">范例输出：mmmmhy</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 找规律，发现权重3字符串后只能接权重2或权重3的字符串，不能跟权重1；所以如果x,z&gt;0,y=0这种情况无解</span><br><span class="line">2. 我们可以先拼接权重3字符串（若有）mhymhy...</span><br><span class="line">3. 然后拼接权重2，mhymhyhyhy...；如果无权重三直接拼接权重2,hyhyh....</span><br><span class="line">4. 只剩下一个权重2没拼时，拼一个可以接权重1的权重2，mhymhyhyhyy</span><br><span class="line">5. 拼接权重1：mhymhyhyhyyyyyyy</span><br></pre></td></tr></table></figure>
<h2 id="找规律后计算">找规律后计算</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A拿到了3元无限长字符串&#123;1,2,3；4,5,6；7,8,9；....&#125;</span><br><span class="line">其中3的倍数后用；分割，其他用逗号</span><br><span class="line">求l个字符到r个字符之间有几个逗号和分号。</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 求l到r字符之间有几个逗号分号——》r前逗号分号-l前逗号分号</span><br><span class="line">2. 难点：数字为1-9时，每个数字只占1个字符，为10-99，每个数字就占两个字符了。</span><br><span class="line">	开始找规律！</span><br><span class="line">	1-9			每6个字符为一组&#123;1,2,3;&#125;			这样的组有3个</span><br><span class="line">	10-99		每9个字符为一组&#123;10,11,12;&#125;			这样的组有30个</span><br><span class="line">	100-999		每12个字符为一组&#123;100,111,112;&#125;		这样的组有300个</span><br><span class="line">	以此类推</span><br><span class="line">	</span><br></pre></td></tr></table></figure>
<h1 id="审题题">审题题</h1>
<h2 id="交换字母使字典序尽可能大">交换字母使字典序尽可能大</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A拿到一个仅有小写字母组成的字符串，她准备进行恰好一次操作：交换连个相邻字母，在操作结束后使字符串的字典序尽可能大。</span><br><span class="line"></span><br><span class="line">input: ba</span><br><span class="line">output: ab</span><br><span class="line">2&lt;=len(input)&lt;=200000</span><br></pre></td></tr></table></figure>
<p>知识点：</p>
<ol type="1">
<li><p>字典序，就是按照字典排列顺序，英文字母按下面方式排列：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ABCDEFG HIJKLMN OPQRST UVWXYZ</span><br><span class="line">abcdefg hijklmn opqrst uvwxyz</span><br></pre></td></tr></table></figure></li>
<li><p>题目说的是<strong>恰好一次</strong>操作！！！！就算交换之后会让原来字符串字典序减小也需要进行操作！</p></li>
</ol>
<h1 id="寻找用例">寻找用例</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。</span><br><span class="line"></span><br><span class="line">计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。</span><br><span class="line"></span><br><span class="line">你可以认为每种硬币的数量是无限的。</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><p>找正例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：coins = [1, 2, 5], amount = 11</span><br><span class="line">输出：3 </span><br></pre></td></tr></table></figure></li>
<li><p>找负例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：coins = [2], amount = 3</span><br><span class="line">输出：-1</span><br></pre></td></tr></table></figure></li>
<li><p>找边界条件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：coins = [1], amount = 0</span><br><span class="line">输出：0</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>Big Data Computing</title>
    <url>/2024/04/15/BDC/</url>
    <content><![CDATA[<p>BDC</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
  </entry>
  <entry>
    <title>CV项目准备</title>
    <url>/2023/08/30/CVProject/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="linksee">Linksee</h1>
<h2 id="multinet分类模型">MultiNet分类模型</h2>
<h1 id="other-point">Other point</h1>
<h2 id="multimodel-learning-多模态">MultiModel Learning 多模态</h2>
<p><strong>相较于图像、语音、文本等多媒体(Multi-media)数据划分形式，“模态”是一个更为细粒度的概念，同一媒介下可存在不同的模态。</strong>
比如我们可以把两种不同的语言当做是两种模态，甚至在两种不同情况下采集到的数据集，亦可认为是两种模态。</p>
<p><strong>多模态机器学习</strong>是从多种模态的数据中学习并且提升自身的算法，它不是某一个具体的算法，它是一类算法的总称。</p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>Classical Deep Learning Method</title>
    <url>/2023/07/25/ClassicalDLMethod/</url>
    <content><![CDATA[<p>经典深度学习模型</p>
<span id="more"></span>
<h1 id="residual-network">Residual Network</h1>
<h1 id="transformer">Transformer</h1>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>DHGCN(2HRDR)</title>
    <url>/2023/06/21/DHGCN/</url>
    <content><![CDATA[<p>propose DHGCN,2HRDR</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>task:
基于知识图谱的问答系统，在知识图谱中检索与问题相关的多个元组</p>
<p>contribution: propose a convolutional network for directed
hypergraph</p>
<h1 id="dhgcn">DHGCN</h1>
<h2 id="hgcn">HGCN</h2>
<p>given a hypergraph <span class="math inline">\(G=(V,E,W)\)</span>, as
well as the incidence matrix <span class="math inline">\(H\in
R^{|V|\times |E|}\)</span></p>
<p>the edge and vertex degrees:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621153922205.png"
alt="image-20230621153922205" />
<figcaption aria-hidden="true">image-20230621153922205</figcaption>
</figure>
<p>while the hypergraph convolutional networks is:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621155931211.png"
alt="image-20230621155931211" />
<figcaption aria-hidden="true">image-20230621155931211</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/IMG_0183(20230621-201655).JPG"
alt="IMG_0183(20230621-201655)" />
<figcaption aria-hidden="true">IMG_0183(20230621-201655)</figcaption>
</figure>
<h2 id="dhgcn-1">DHGCN</h2>
<p>the directed hypergraph can be denoted by two incidence matrices
<span class="math inline">\(H^{head}\)</span> and <span
class="math inline">\(H^{tail}\)</span></p>
<p>the degree:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621160733846.png"
alt="image-20230621160733846" />
<figcaption aria-hidden="true">image-20230621160733846</figcaption>
</figure>
<p>the directed hypergraph convolutional networks is:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621160751671.png"
alt="image-20230621160751671" />
<figcaption aria-hidden="true">image-20230621160751671</figcaption>
</figure>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/IMG_0184(20230621-202928).JPG" alt="IMG_0184(20230621-202928)" style="zoom:50%;" /></p>
<h1 id="hrdr">2HRDR</h1>
<h2 id="task-definition">Task Definition</h2>
<p>given a knowledge graph <span
class="math inline">\(K=(V,E,T)\)</span> and <span
class="math inline">\(q=(w_1,w_2,\cdots ,w_{|q|})\)</span>.</p>
<p>the task aims to pick the answers from <em>V</em>.</p>
<h2 id="method">Method</h2>
<h3 id="directed-hypergraph-retrieval-and-construction"><strong>Directed
Hypergraph Retrieval and</strong> <strong>Construction</strong></h3>
<p>find subgraph</p>
<ol type="1">
<li>obtain seed entities from the question by entity linking</li>
<li>get the entities set within L hops to form a subgraph</li>
<li>get <span class="math inline">\(H^{head}\)</span> and <span
class="math inline">\(H^{tail}\)</span></li>
</ol>
<h3 id="input-encoder">Input Encoder</h3>
<ol type="1">
<li><p>apply a bi-LSTM to encode question and obtain hidden states <span
class="math inline">\(H\in R^{|q|\times h}\)</span>,we assume
h=d</p></li>
<li><p>employ co-attention to learn query-aware entity
representation</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621170223607.png"
alt="image-20230621170223607" />
<figcaption aria-hidden="true">image-20230621170223607</figcaption>
</figure></li>
</ol>
<h3 id="reasoning-over-hypergraph">Reasoning over Hypergraph</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621163315446.png"
alt="image-20230621163315446" />
<figcaption aria-hidden="true">image-20230621163315446</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621172646592.png"
alt="image-20230621172646592" />
<figcaption aria-hidden="true">image-20230621172646592</figcaption>
</figure>
<ol type="1">
<li><p>Learn Relation Representation Explicitly</p>
<ol type="1">
<li><p>combine entity embedding and co-attention</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621173702999.png"
alt="image-20230621173702999" />
<figcaption aria-hidden="true">image-20230621173702999</figcaption>
</figure></li>
<li><p>propagation</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621173728920.png"
alt="image-20230621173728920" />
<figcaption aria-hidden="true">image-20230621173728920</figcaption>
</figure></li>
<li><p>aggregation</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621173815550.png"
alt="image-20230621173815550" />
<figcaption aria-hidden="true">image-20230621173815550</figcaption>
</figure></li>
</ol></li>
<li><p>Allocate Relation Weights Dynamically(dynamically allocated
hop-by-hop)</p>
<ol type="1">
<li><p>use co-attention to cal <span
class="math inline">\(R_{co\_attn}\)</span></p></li>
<li><p>compute the weight of edge</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621174053983.png"
alt="image-20230621174053983" />
<figcaption aria-hidden="true">image-20230621174053983</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621174101797.png"
alt="image-20230621174101797" />
<figcaption aria-hidden="true">image-20230621174101797</figcaption>
</figure></li>
</ol></li>
<li><p>Update Entity Adaptively</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621174229381.png"
alt="image-20230621174229381" />
<figcaption aria-hidden="true">image-20230621174229381</figcaption>
</figure></li>
</ol>
]]></content>
      <categories>
        <category>GNN</category>
        <category>hypergraph</category>
      </categories>
  </entry>
  <entry>
    <title>DHT-Relation Work</title>
    <url>/2023/05/26/DHT-relation/</url>
    <content><![CDATA[<p>Relation work of DHT</p>
<span id="more"></span>
<h1 id="edgeformers">Edgeformers</h1>
<p>EDGEFORMERS: G RAPH-E MPOWERED TRANSFORMERS FOR REPRESENTATION L
EARNING ON T EXTUALE DGE NETWORKS</p>
<h2 id="background">Background</h2>
<ol type="1">
<li><p>Edge-aware GNNs:</p>
<p>studies assume the information carried by edges can be directly
described as an attribute vector.</p>
<ol type="1">
<li>This assumption holds well when edge features are categorical</li>
<li>cannot fully capture contextualized text semantic</li>
</ol></li>
<li><p>PLM-GNN</p>
<p>text information is first encoded by a PLM and then aggregated by a
GNN</p>
<ol type="1">
<li>such architectures process text and graph signals one after the
other, and fail to simultaneously model the deep interactions</li>
</ol></li>
<li><p>GNN-nested PLM</p>
<p>inject network information into the text encoding process</p>
<ol type="1">
<li>cannot be easily adapted to handle text-rich edges</li>
</ol></li>
</ol>
<h2 id="proposed-method">Proposed method</h2>
<ol type="1">
<li>we conduct edge representation learning by jointly considering text
and network information via a Transformer-based architecture
(Edgeformer-E).</li>
<li>perform node representation learning using the edge representation
learning module as building blocks (Edgeformer-N)</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230602122224210.png"
alt="image-20230602122224210" />
<figcaption aria-hidden="true">image-20230602122224210</figcaption>
</figure>
<h3
id="network-aware-edge-text-encoding-with-virtual-node-tokens">Network-aware
Edge Text Encoding with Virtual Node Tokens</h3>
<p>Given an edge <span
class="math inline">\(e_{ij}=(v_i,v_j)\)</span></p>
<p>Use a transformer to deal with text</p>
<p>introduce two virtual node tokens to represent $ v_i$ and <span
class="math inline">\(v_j\)</span> to transformer</p>
<p>$ v_i$ 和 <span class="math inline">\(v_j\)</span>
是连接边两个node的embedding]</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230602122208124.png"
alt="image-20230602122208124" />
<figcaption aria-hidden="true">image-20230602122208124</figcaption>
</figure>
<h3 id="text-a-ware-node-representation-learning-edgeformer-n">TEXT-A
WARE NODE REPRESENTATION LEARNING (EDGEFORMER-N)</h3>
<h4 id="node-aggregating-edge-representations">node-Aggregating Edge
Representations</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230602122457727.png"
alt="image-20230602122457727" />
<figcaption aria-hidden="true">image-20230602122457727</figcaption>
</figure>
<h3
id="enhancing-edge-representations-with-the-nodes-local-network-structure">Enhancing
Edge Representations with the Node’s Local Network Structure</h3>
<p>add one more virtual node in edge learning
:获取与边节点连接的邻居边的信息</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230602122540378.png"
alt="image-20230602122540378" />
<figcaption aria-hidden="true">image-20230602122540378</figcaption>
</figure>
<p>邻居边的embedding经过一个新tranformer，获取<cls>节点的embedding，作为参加edge
learning的虚拟节点</p>
<h1 id="gratis">GRATIS</h1>
<p>Paper：GRATIS: Deep Learning <strong>G</strong>raph
<strong>R</strong>epresentation with T<strong>a</strong>sk-specifific
<strong>T</strong>opology and Mult<strong>i</strong>-dimensional Edge
Feature<strong>s</strong></p>
<p>总结：计算一个全局representation-X，X经过MLP和reshape、softmax等操作变成和邻接矩阵大小相同的权重矩阵，然后得到一个edge出现的概率矩阵，概率大于一定阈值就补全边。</p>
<p>edge用向量表示而不是一个一维数（一维权重）。</p>
<h2 id="methology">Methology</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617164254419.png"
alt="image-20230617164254419" />
<figcaption aria-hidden="true">image-20230617164254419</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617164314513.png"
alt="image-20230617164314513" />
<figcaption aria-hidden="true">image-20230617164314513</figcaption>
</figure>
<h3 id="backbone">Backbone</h3>
<p>反正就是各种方法得到一个全局表示X</p>
<h3 id="graph-definition">Graph Definition</h3>
<p>采用图原来的点和边</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617164457095.png"
alt="image-20230617164457095" />
<figcaption aria-hidden="true">image-20230617164457095</figcaption>
</figure>
<h3 id="task-specific-topology-prediction">Task-specific Topology
Prediction</h3>
<p>用X计算出一个概率矩阵</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617164752154.png"
alt="image-20230617164752154" />
<figcaption aria-hidden="true">image-20230617164752154</figcaption>
</figure>
<p>h（x）为mlp</p>
<p>如果概率大于某个阈值，增加新边</p>
<h3
id="multi-dimensional-edge-feature-generation"><strong>Multi-dimensional
Edge Feature Generation</strong></h3>
<p>根据边两端的节点计算边</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617165302836.png"
alt="image-20230617165302836" />
<figcaption aria-hidden="true">image-20230617165302836</figcaption>
</figure>
<p><strong>VCR</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617165318193.png"
alt="image-20230617165318193" />
<figcaption aria-hidden="true">image-20230617165318193</figcaption>
</figure>
<p><strong>VVR</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617165420670.png"
alt="image-20230617165420670" />
<figcaption aria-hidden="true">image-20230617165420670</figcaption>
</figure>
<p>fifinally employ either a pooling layer or a fully-connected layer,
to flatten <span class="math inline">\(F_{i,x,j}\)</span> and <span
class="math inline">\(F_{ *j,x,i*}\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617165554284.png"
alt="image-20230617165554284" />
<figcaption aria-hidden="true">image-20230617165554284</figcaption>
</figure>
<h1 id="surge">SURGE</h1>
<p>Paper： Knowledge-Consistent Dialogue Generation with Knowledge
Graphs</p>
<p>总结：在KG大图中检索与文本相关的子图，用GCN计算node
representation，用ENGNN计算 edge representation。然后用在后面的任务</p>
]]></content>
      <categories>
        <category>GNN</category>
        <category>EdgeLearning</category>
      </categories>
  </entry>
  <entry>
    <title>DHT</title>
    <url>/2023/03/20/DHT/</url>
    <content><![CDATA[<p>DHT, which transforms the edges of a graph into the nodes of a
hypergraph.</p>
<p>ENGNN, use hypergraph after DHT to propagation</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>Before methods only capture edge information implicitly, e.g. used as
weight.</p>
<h1 id="contribute">Contribute</h1>
<ol type="1">
<li>propose DHT, Dual Hypergraph Transformation</li>
<li>propose a novel edge representation learning scheme ENGNN by using
DHT.</li>
<li>propose novel edge pooling methods.</li>
</ol>
<h1 id="method">Method</h1>
<h2 id="dht-how-to-transfer-graph-to-hypergraph">DHT： how to transfer
graph to hypergraph</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230320171020430.png"
alt="image-20230320171020430" />
<figcaption aria-hidden="true">image-20230320171020430</figcaption>
</figure>
<h3 id="step1-get-origin-graph-representation">Step1: Get origin graph
representation</h3>
<p>Firstly, we get the initial node feature and edge feature. <span
class="math display">\[
node \space  feature: X\in R^{n\times d}
\]</span></p>
<p><span class="math display">\[
edge\space feature: E\in R^{m\times d&#39;}
\]</span></p>
<p>Than we use an incidence matrix M rather than an adjacency matrix to
represent graph structure. <span class="math display">\[
incidence\space matrix: M\in \{0,1\}^{n\times m}
\]</span> So the origin graph is <span class="math display">\[
G=(X,M,E)
\]</span></p>
<h3 id="step-2-use-dht-to-get-hypergraph-g">Step 2: Use DHT to get
hypergraph <span class="math inline">\(G^*\)</span></h3>
<p>The hypergraph represent <span class="math display">\[
G^*=(X^*,M^*,E^*)
\]</span></p>
<p><span class="math display">\[
X^*=E
\]</span></p>
<p><span class="math display">\[
M^*=M^T
\]</span></p>
<p><span class="math display">\[
E^*=X
\]</span></p>
<p><span class="math display">\[
DHT:G=(X,M,E)-&gt;G^*=(E,M^T,X)
\]</span></p>
<p>While DHT is a bijective transformation: <span
class="math display">\[
DHT:G^*=(E,M^T,X)-&gt;G=(X,M,E)
\]</span></p>
<h2
id="ehgnn-an-edge-representation-learning-framework-using-dht">EHGNN: an
edge representation learning framework using DHT</h2>
<p><span class="math display">\[
E^{(l+1)}=ENGNN(X^{(l)},M,E^{(l)})=GNN(DHT(X^{(l)},M,E^{(l)}))
\]</span></p>
<p>So ENGNN consists of DHT and GNN, while GNN can be any GNN
function.</p>
<p>After ENGNN, EHGNN, <span class="math inline">\(E^{(L)}\)</span> is
returned to the original graph by applying DHT to dual hypergraph <span
class="math inline">\(G^∗\)</span>. Then, the remaining step is how to
make use of these edge-wise representations to finish the task.</p>
<h2 id="pooling">Pooling</h2>
<p>To be continue...</p>
<h1 id="advantage">Advantage</h1>
<h2 id="dht">DHT</h2>
<ol type="1">
<li>low time complexity</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230320165122363.png"
alt="image-20230320165122363" />
<figcaption aria-hidden="true">image-20230320165122363</figcaption>
</figure>
]]></content>
      <categories>
        <category>GNN</category>
        <category>EdgeLearning</category>
      </categories>
  </entry>
  <entry>
    <title>Data Structure and algorithms</title>
    <url>/2023/04/14/DataStructure/</url>
    <content><![CDATA[<ol type="1">
<li>Complexity</li>
<li>Linear structures</li>
<li>Tree structures</li>
<li>Other common data structures</li>
<li>Search algorithms</li>
<li>Sorting algorithms</li>
</ol>
<span id="more"></span>
<h1 id="complexity">Complexity</h1>
<h1 id="array">Array</h1>
<p>fixed length, indexable, should shift after insertions and
deletions</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230419185630699.png"
alt="image-20230419185630699" />
<figcaption aria-hidden="true">image-20230419185630699</figcaption>
</figure>
<h2 id="array-operation">Array Operation</h2>
<h3 id="insertion-on">Insertion O(n)</h3>
<ol type="1">
<li>insert an element in the specified index</li>
<li>shift the subsequent item to the right</li>
</ol>
<h3 id="deletion-on">Deletion O(n)</h3>
<ol type="1">
<li>delete the specified element</li>
<li>shift the subsequent item to the left</li>
</ol>
<h3 id="searching-in-a-sorted-array">Searching in a sorted array</h3>
<h4 id="linear-search">Linear search</h4>
<p>Best case-O(1)</p>
<p>Worst case-O(n)</p>
<p>Average case-O(n/2)-&gt;O(n)</p>
<h4 id="binary-search-ologn">Binary search O(logn)</h4>
<h3 id="sorting-array">Sorting array</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230419191311851.png"
alt="image-20230419191311851" />
<figcaption aria-hidden="true">image-20230419191311851</figcaption>
</figure>
<h1 id="stack">Stack</h1>
<p>Last In First Out</p>
<h2 id="stack-operation">Stack Operation</h2>
<p>Push(S,x): insert x to the top of the stack S</p>
<p>Pop(S): extract the top of the stack S</p>
<p>Top(s): return the topmost element of stack S without removing it</p>
<p>isEmpty(s): return whether the stack S is empty</p>
<h2 id="implementation">Implementation</h2>
<h3 id="array-1">Array</h3>
<h4 id="push-o1">Push() O(1)</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if s.top=s.len</span><br><span class="line">	error &#x27;full&#x27;</span><br><span class="line">else</span><br><span class="line">	s.top=s.top+1</span><br><span class="line">	s[s.top]=x</span><br></pre></td></tr></table></figure>
<h4 id="pop-o1">Pop() O(1)</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if isEmpty(s)</span><br><span class="line">	error &#x27;empty&#x27;</span><br><span class="line">else</span><br><span class="line">	s.top=s.top-1</span><br><span class="line">	return s[s.top+1]</span><br></pre></td></tr></table></figure>
<h4 id="top-o1">Top() O(1)</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if isEmpty(s)</span><br><span class="line">	error &#x27;empty&#x27;</span><br><span class="line">else</span><br><span class="line">	return s[s.top]</span><br></pre></td></tr></table></figure>
<h4 id="isempty-o1">isEmpty() O(1)</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if s.len=0</span><br><span class="line">	return true</span><br><span class="line">else</span><br><span class="line">	return false</span><br></pre></td></tr></table></figure>
<h4 id="search-on">Search O(n)</h4>
<h2 id="use-stack-implements-a-simple-calculator">Use stack implements a
simple calculator</h2>
<ol type="1">
<li>transfer to postfix notation</li>
</ol>
<p>3-2-1&gt;&gt;32-1-</p>
<p>3-2*1&gt;&gt;321*-</p>
<ol start="2" type="1">
<li>use stack to calculate</li>
</ol>
<p>3-2-1&gt;&gt;32-1-</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">push(3)</span><br><span class="line">push(2)</span><br><span class="line">Meet(-);Pop;Pop;Push(3-2=1)</span><br><span class="line">Push(1)</span><br><span class="line">Meet(-);Pop;Pop;Push(1-1=0)</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>exercise</li>
</ol>
<p>10 + (44 − 1) * 3 + 9 / 2</p>
<p>((1 - 2) - 5) + (6 / 5)</p>
<p>(((22 / 7) + 4) * (6 - 2))</p>
<h1 id="queue">Queue</h1>
<p>First In First Out</p>
<h2 id="operation">Operation</h2>
<p>Enqueue(Q,x): put an element x at the end of queue Q</p>
<p>Dequeue(Q): extract the first element from queue Q</p>
<h2 id="implementation-1">Implementation</h2>
<h3 id="array-1-1">Array 1</h3>
<p>Enqueue in the tail -O(1)</p>
<p>Dequeue in the position0 -O(n)</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230419194745117.png"
alt="image-20230419194745117" />
<figcaption aria-hidden="true">image-20230419194745117</figcaption>
</figure>
<h3 id="array-2">Array 2</h3>
<p>Enqueue in the position0 -O(n)</p>
<p>Dequeue in the tail -O(1)</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230419195104009.png"
alt="image-20230419195104009" />
<figcaption aria-hidden="true">image-20230419195104009</figcaption>
</figure>
<h3 id="array-3--circular-queue">Array 3 -Circular queue</h3>
<p>Enqueue: O(1)</p>
<p>Dequeue: O(1)</p>
<p>Peeking (get the front item without removing it) O(1)</p>
<p>isFull: O(1)</p>
<p>isEmpty: O(1)</p>
<p>search: O(n)</p>
<h4 id="algorithm-implement">algorithm implement</h4>
<p>Data members:</p>
<p>• Q: an array of items • Q.len: length of array • Q.front: position
of the front item • Q.rear: rear item position + 1 (not an item)</p>
<h5 id="enqueue">Enqueue</h5>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if isFull(Q)</span><br><span class="line">	error &#x27;full&#x27;</span><br><span class="line">else</span><br><span class="line">	Q[Q.rear] = x</span><br><span class="line">	if Q.rear==Q.len:</span><br><span class="line">		Q.rear=1</span><br><span class="line">	else</span><br><span class="line">		Q.rear=Q.rear+1</span><br></pre></td></tr></table></figure>
<h5 id="dequeue">Dequeue</h5>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if isEmpty(W)</span><br><span class="line">	error &#x27;Empty&#x27;</span><br><span class="line">else</span><br><span class="line">	x=Q[Q.front]</span><br><span class="line">	if Q.front==Q.len</span><br><span class="line">		Q.front=1</span><br><span class="line">	else</span><br><span class="line">		Q.front=Q.front+1</span><br></pre></td></tr></table></figure>
<h5 id="isfull-and-isempty">isFull and isEmpty</h5>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230419201711632.png"
alt="image-20230419201711632" />
<figcaption aria-hidden="true">image-20230419201711632</figcaption>
</figure>
<h1 id="linked-list">Linked List</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230420110345241.png"
alt="image-20230420110345241" />
<figcaption aria-hidden="true">image-20230420110345241</figcaption>
</figure>
<p>Node: An object containing data and pointer(s) Pointer: Reference to
another node Head: The first node in a linked list Tail: The last node
in a linked list</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230420112447328.png"
alt="image-20230420112447328" />
<figcaption aria-hidden="true">image-20230420112447328</figcaption>
</figure>
<p>No limited to size</p>
<p>require more space per element</p>
<h2 id="initial">Initial</h2>
<p>...</p>
<h2 id="operation-1">Operation</h2>
<ol type="1">
<li><p>print</p></li>
<li><p>insert</p></li>
<li><p>Delete</p></li>
</ol>
<h1 id="hash-table">Hash table</h1>
<p>Dictionary ADT</p>
<p>key:value</p>
<h2 id="implement-of-adt">implement of ADT</h2>
<h3 id="array-3">array</h3>
<p>Space usage: O(n)</p>
<p>Search usage: O(n)</p>
<table>
<thead>
<tr class="header">
<th>Array index</th>
<th>Key</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>3</td>
<td>Coffee</td>
</tr>
<tr class="even">
<td>1</td>
<td>15</td>
<td>Bread</td>
</tr>
<tr class="odd">
<td>2</td>
<td>8</td>
<td>Tea</td>
</tr>
</tbody>
</table>
<h3 id="large-array">Large array</h3>
<p>Space usage: O(U)-max key</p>
<p>Search usage: O(1)</p>
<table>
<thead>
<tr class="header">
<th>Array index</th>
<th>Key</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="even">
<td>3</td>
<td>3</td>
<td>Coffee</td>
</tr>
<tr class="odd">
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="even">
<td>8</td>
<td>8</td>
<td>Tea</td>
</tr>
<tr class="odd">
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="even">
<td>15</td>
<td>15</td>
<td>Bread</td>
</tr>
</tbody>
</table>
<h3 id="hash-table-1">Hash table</h3>
<p>Converts a key (of a large range) to a hash value (of a small range).
e.g. k mod m</p>
<p>Space usage: O(n)</p>
<p>Search usage: O(1)</p>
<h4 id="collision-solution">Collision solution</h4>
<p>collision: different keys have the same hash value</p>
<h5 id="chaining">Chaining</h5>
<h6 id="use-a-linked-list">use a linked list</h6>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230422183421651.png"
alt="image-20230422183421651" />
<figcaption aria-hidden="true">image-20230422183421651</figcaption>
</figure>
<h6 id="load-factor">load factor</h6>
<p><span class="math display">\[
\lambda = \frac{n}{m}
\]</span></p>
<p>n is the number of keys, m is the total number of buckets.</p>
<p>Measures how full the hash table is</p>
<p>it is suggested to keep <span class="math inline">\(\lambda\)</span>
&lt; 1</p>
<h6 id="cost">cost</h6>
<ol type="1">
<li><p>time</p>
<p>cost of search: O(1)+O(l), l is the length of the linked list</p>
<p>worst case: O(1)+O(n)</p>
<p>Average case: O(1)+O(<span
class="math inline">\(\lambda\)</span>)</p></li>
<li><p>space</p>
<p>requires additional space to store the pointers in linked lists of
entries.</p>
<p>Worst case: n-1 additional space</p>
<p>Average case: <span class="math inline">\(\lambda\)</span>-1
additional space</p></li>
</ol>
<h5 id="opening-addressing-probing">Opening addressing: probing</h5>
<h6 id="insert">Insert</h6>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230422185347171.png"
alt="image-20230422185347171" />
<figcaption aria-hidden="true">image-20230422185347171</figcaption>
</figure>
<h6 id="search">Search</h6>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230422185455352.png"
alt="image-20230422185455352" />
<figcaption aria-hidden="true">image-20230422185455352</figcaption>
</figure>
<h6 id="delete">Delete</h6>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230422190114349.png"
alt="image-20230422190114349" />
<figcaption aria-hidden="true">image-20230422190114349</figcaption>
</figure>
<h6 id="load-factor-1">Load factor</h6>
<p>must <span class="math inline">\(\lambda &lt; 1\)</span></p>
<p>How to deal with the hash table when ! becomes large?</p>
<ol type="1">
<li>Make a large hash table and move all elements into it.</li>
<li>Simply add an additional hash table</li>
</ol>
<h6 id="probing-type">probing type</h6>
<ol type="1">
<li><p>Linear probing</p>
<p><span class="math display">\[
H(k,i) = (H_0(k) + i)\space mod \space m
\]</span> have clustering problem, multiple keys are hashed to
consecutive slots.</p>
<p>performance degrade significantly when <span
class="math inline">\(\lambda\)</span>&gt; 0.5.</p></li>
<li><p>Quadratic probing <span class="math display">\[
H(k,i) = (H_0(k) + a\cdot i+b\cdot i^2)\space mod \space m
\]</span> reduces the clustering problem.</p></li>
</ol>
<h3 id="string-key">string key</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230422191105400.png"
alt="image-20230422191105400" />
<figcaption aria-hidden="true">image-20230422191105400</figcaption>
</figure>
<h1 id="tree">Tree</h1>
<p>a tree is an abstract model of a hierarchical structure consists of a
set of nodes and a set of edges.</p>
<p>Every node except the root has exactly one parent node.</p>
<h2 id="definition">Definition</h2>
<ol type="1">
<li><p>Length of a path： The number of edges in the path.</p></li>
<li><p>The height of a node：</p>
<p>The largest path length from that node to any leaf node (not
including ancestors).</p>
<p>Each leaf node has the height 0.</p></li>
<li><p>The height of a tree: The maximum level of a node in a tree is
the tree’s height.</p></li>
<li><p>The depth of a node:</p>
<p>The node's level (depth) of a node is the length of the path from
that node to the root.</p>
<p>The depth of the root is zero.</p></li>
</ol>
<h2 id="binary-tree">Binary Tree</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230424201038461.png"
alt="image-20230424201038461" />
<figcaption aria-hidden="true">image-20230424201038461</figcaption>
</figure>
<h3 id="full-binary-tree">Full Binary Tree</h3>
<p>Every node has either 0 or 2 children.</p>
<h3 id="complete-binary-tree">Complete Binary Tree</h3>
<p>Every level, except the last level, is completely filled, and all
nodes in the last level are as far left as possible (left
justified).</p>
<h3 id="perfect-binary-tree">Perfect Binary Tree</h3>
<p>Every node except the leaf nodes have two children and every level is
completely filled.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230424201302544.png"
alt="image-20230424201302544" />
<figcaption aria-hidden="true">image-20230424201302544</figcaption>
</figure>
<p>list representation: [Root, left-sub-tree, right-sub-tree]</p>
<h3 id="binary-search-tree">Binary Search Tree</h3>
<p>Insertion - O(h)</p>
<p>Search - O(h)</p>
<p>Deletion: O(h)</p>
<p>ℎ is O(log⁡n) if tree is balanced.</p>
<p>all is O(n) in worst case</p>
<p>左小右大</p>
<h4 id="search-operation">Search Operation</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Algorithm SearcℎBST(t, target)</span><br><span class="line">  Input: the BST t <span class="keyword">and</span> the target</span><br><span class="line"></span><br><span class="line">  p = t.root</span><br><span class="line">  <span class="keyword">while</span> p≠null do</span><br><span class="line">      <span class="keyword">if</span> target=p.value do</span><br><span class="line">          <span class="keyword">return</span> p</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> target&lt;p.value do</span><br><span class="line">          p=p.left</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">          p=p.rigℎt</span><br><span class="line">  end</span><br><span class="line">  <span class="keyword">return</span> null</span><br></pre></td></tr></table></figure>
<h4 id="insert-1">Insert</h4>
<p>average case: O(n) = logn</p>
<p>worst case: O(n) = n</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Algorithm Insert(t, node)</span><br><span class="line">  Input: the BST t <span class="keyword">and</span> the node</span><br><span class="line"></span><br><span class="line">  p = t.root</span><br><span class="line">  <span class="keyword">if</span> p=null do</span><br><span class="line">      t.root=node</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  end</span><br><span class="line">  <span class="keyword">while</span> p≠null do</span><br><span class="line">      prev=p</span><br><span class="line">      <span class="keyword">if</span> node.value&lt;p.value do</span><br><span class="line">          p=p.left</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> node.value&gt;p.value do</span><br><span class="line">          p=p.rigℎt</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">          <span class="keyword">return</span></span><br><span class="line">  end</span><br><span class="line">  <span class="keyword">if</span> node.value&lt;prev do</span><br><span class="line">      prev.left=node</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">      prev.rigℎt=node</span><br></pre></td></tr></table></figure>
<h4 id="find-minimun">find minimun</h4>
<p>O(h)-ℎ is the depth of the tree</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Algorithm Minimum(t)</span><br><span class="line">  Input: the BST t</span><br><span class="line"></span><br><span class="line">  p = t.root</span><br><span class="line">  <span class="keyword">while</span> p.left≠null do</span><br><span class="line">      p=p.left</span><br><span class="line">  end</span><br><span class="line">  <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure>
<h4 id="deletion-in-bst">Deletion in BST</h4>
<ol type="1">
<li><p>has no child</p>
<p>delete the node</p></li>
<li><p>has one child</p>
<p>use the child to replace z</p></li>
<li><p>has two children</p>
<p>delete the minimum node x of the right subtree of z (i.e., x is the
successor of z), then replace z by x.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230424233325065.png"
alt="image-20230424233325065" />
<figcaption aria-hidden="true">image-20230424233325065</figcaption>
</figure></li>
</ol>
<h3 id="tree-traversal">Tree Traversal</h3>
<h4 id="depth-first-tree-traversal">Depth-first Tree Traversal</h4>
<h5 id="implement-stack">implement: stack</h5>
<h6 id="preorder-前序">preorder 前序</h6>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preorder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span> </span><br><span class="line">    <span class="built_in">print</span>(root.val)</span><br><span class="line">    preorder(root.left)</span><br><span class="line">    preorder(root.right)</span><br></pre></td></tr></table></figure>
<h6 id="postorder-后序">postorder 后序</h6>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">postorder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span> </span><br><span class="line">    preorder(root.left)</span><br><span class="line">    preorder(root.right)</span><br><span class="line">    <span class="built_in">print</span>(root.val)</span><br></pre></td></tr></table></figure>
<h6 id="inorder-中序">inorder 中序</h6>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inorder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span> </span><br><span class="line">    preorder(root.left)</span><br><span class="line">    <span class="built_in">print</span>(root.val)</span><br><span class="line">    preorder(root.right)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h4 id="breadth-first-tree-traversal">Breadth-first Tree Traversal</h4>
<h5 id="implement-queue">implement: queue</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bfs</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> roor: <span class="keyword">return</span></span><br><span class="line">    q=[root]</span><br><span class="line">    result=[]</span><br><span class="line">    <span class="keyword">while</span> q:</span><br><span class="line">        child=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> q:</span><br><span class="line">            result.append(i)</span><br><span class="line">            <span class="keyword">if</span> i.left:</span><br><span class="line">                child.append(i.left)</span><br><span class="line">            <span class="keyword">if</span> i.right:</span><br><span class="line">                child.append(i.right)</span><br><span class="line">        q=child</span><br><span class="line">     <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h1 id="sorting">Sorting</h1>
<p>stable, unstable, inplace, outplace</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230425140502996.png"
alt="image-20230425140502996" />
<figcaption aria-hidden="true">image-20230425140502996</figcaption>
</figure>
<h2 id="selection-sort">Selection sort</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230425130213964.png"
alt="image-20230425130213964" />
<figcaption aria-hidden="true">image-20230425130213964</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230425130244294.png"
alt="image-20230425130244294" />
<figcaption aria-hidden="true">image-20230425130244294</figcaption>
</figure>
<p>time-<span class="math inline">\(O(n^2)\)</span></p>
<p>in-place</p>
<p>unstable</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">selection_sort</span>(<span class="params">arr</span>):</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr)-<span class="number">1</span>):</span><br><span class="line">		min_ind = i</span><br><span class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, <span class="built_in">len</span>(arr)):</span><br><span class="line">            <span class="keyword">if</span> arr[j]&lt;arr[min_ind]:</span><br><span class="line">                min_index=j</span><br><span class="line">        <span class="keyword">if</span> i!=min_ind:</span><br><span class="line">            arr[min_ind],arr[i]=arr[i],arr[min_index]</span><br><span class="line">    <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure>
<h2 id="insertion-sort">Insertion Sort</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230425130404747.png"
alt="image-20230425130404747" />
<figcaption aria-hidden="true">image-20230425130404747</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230425130733053.png"
alt="image-20230425130733053" />
<figcaption aria-hidden="true">image-20230425130733053</figcaption>
</figure>
<p>time -<span class="math inline">\(O(n^2)\)</span></p>
<p>in-place</p>
<p>stable</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def insertionSort(arr): </span><br><span class="line">    for i in range(1, len(arr)): </span><br><span class="line">        key = arr[i] </span><br><span class="line">        j = i-1</span><br><span class="line">        while j &gt;=0 and key &lt; arr[j] : </span><br><span class="line">                arr[j+1] = arr[j] </span><br><span class="line">                j -= 1</span><br><span class="line">        arr[j+1] = key </span><br></pre></td></tr></table></figure>
<h2 id="merge-sort">Merge Sort</h2>
<p>time - O(nlogn)</p>
<p>not in-place</p>
<p>stable</p>
<ol type="1">
<li>Divide: divide the array A into two sub-arrays (L and R) of n/2
numbers each.</li>
<li>Conquer: sort two sub-arrays recursively.</li>
<li>Combine: merge two sorted sub-arrays into a sorted array.</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230425141231598.png"
alt="image-20230425141231598" />
<figcaption aria-hidden="true">image-20230425141231598</figcaption>
</figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Algorithm MergeSort(A, n)</span><br><span class="line">  Input: the n-size array A</span><br><span class="line"></span><br><span class="line">  if n=1 then</span><br><span class="line">      return A</span><br><span class="line">  (L,R)=A</span><br><span class="line">  L’=MergeSort(L)</span><br><span class="line">  R’=MergeSort(R)</span><br><span class="line">  return Merge(L’, R’)</span><br></pre></td></tr></table></figure>
<h3 id="merge-function---on">Merge function - O(n)</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Algorithm Merge(A, n_A,B,n_B,C)</span><br><span class="line">  Input: the n_A-size array A and n_B-size array B</span><br><span class="line">  Output: the (n_A+n_B−1)-size array C</span><br><span class="line"></span><br><span class="line">  i=0,j=0,k=0</span><br><span class="line">  while i&lt;n_A and j&lt;n_B do</span><br><span class="line">      if A[i]≤B[j] then</span><br><span class="line">          C[k]=A[i], i=i+1</span><br><span class="line">      else</span><br><span class="line">          C[k]=B[j], j=j+1</span><br><span class="line">      end</span><br><span class="line">      k=k+1</span><br><span class="line">  end</span><br><span class="line">  if i=n_A then</span><br><span class="line">      C[k,⋯]=B[j,⋯]</span><br><span class="line">  else</span><br><span class="line">      C[k,⋯]=A[i,⋯]</span><br><span class="line">  end</span><br></pre></td></tr></table></figure>
<h2 id="quick-sort">Quick Sort</h2>
<p>O(nlogn)</p>
<p>in-place</p>
]]></content>
      <categories>
        <category>DS</category>
      </categories>
  </entry>
  <entry>
    <title>GeneralReading</title>
    <url>/2023/03/29/GeneralReading/</url>
    <content><![CDATA[<p>MKR,RKGE,HAGERec,entity2rec,HAKG</p>
<span id="more"></span>
<h1 id="mkr">MKR</h1>
<h2 id="framework">Framework</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230329182212440.png"
alt="image-20230329182212440" />
<figcaption aria-hidden="true">image-20230329182212440</figcaption>
</figure>
<p>The framework of MKR is illustrated in Figure 1a.</p>
<p>MKR consists of three main components: recommendation module, KGE
module, and cross&amp;compress units.</p>
<ol type="1">
<li><p>The recommendation module on the left takes a user and an item as
input, and uses a multi-layer perceptron (MLP) and cross&amp;compress
units to extract short and dense features for the user and the item,
respectively. The extracted features are then fed into another MLP
together to output the predicted probability.</p></li>
<li><p>Similar to the left part, the KGE module in the right part also
uses multiple layers to extract features from the head and relation of a
knowledge triple, and outputs the representation of the predicted tail
under the supervision of a score function f and the real tail.</p></li>
<li><p>The recommendation module and the KGE module are bridged by
specially designed cross&amp;compress units. The proposed unit can
automatically learn high-order feature interactions of items in
recommender systems and entities in the knowledge graph.</p></li>
</ol>
<p>u: MLP to update</p>
<p>v: cross&amp;compress units</p>
<p>r: MLP to update</p>
<p>h: cross&amp;compress units</p>
<h2 id="loss-function">Loss function</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230329184018259.png"
alt="image-20230329184018259" />
<figcaption aria-hidden="true">image-20230329184018259</figcaption>
</figure>
<h1 id="rkge">RKGE</h1>
<p>RKGE first automatically mines all qualified paths between entity
pairs from the KG, which are then encoded via a batch of recurrent
networks, with each path modeled by a single recurrent network.</p>
<p>It then employs a pooling operation to discriminate the importance of
different paths for characterizing user preferences towards items.</p>
<h2 id="framework-1">framework</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230329205324109.png"
alt="image-20230329205324109" />
<figcaption aria-hidden="true">image-20230329205324109</figcaption>
</figure>
<h3 id="semantic-path-mining">Semantic Path Mining</h3>
<p>Strategy</p>
<ol type="1">
<li>We only consider user-to-item paths <span
class="math inline">\(P(u_i,v_j)\)</span> that connect <span
class="math inline">\(u, i\)</span> with all her rated items.</li>
<li>We enumerate paths with a length constraint.</li>
</ol>
<h3 id="encode-path">Encode path</h3>
<p>use recurrent networks</p>
<h4 id="embedding-layer">Embedding layer</h4>
<p>generate the embedding of entities</p>
<h4 id="attention-gated-hidden-layer">Attention-Gated Hidden Layer</h4>
<p>就是一个RNN网络的变种</p>
<h1 id="hagerec">HAGERec</h1>
<h2 id="framework-2">Framework</h2>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230330191130895.png"
alt="image-20230330191130895" />
<figcaption aria-hidden="true">image-20230330191130895</figcaption>
</figure>
<p>four components:</p>
<ol type="1">
<li><p>Flatten and embedding layer: flatten complex high-order relations
and embedding the entities and relations as vectors.</p></li>
<li><p>GCN learning layer: uses GCN model to propagate and update user’s
and item’s embedding via a bi-directional entity propagation
strategy</p></li>
<li><p>Interaction signals unit: preserves interaction signals structure
of an entity and its neighbor network to give a more complete picture
for user’s and item’s representation.</p></li>
<li><p>Prediction layer: utilizes the user’s and item’s aggregated
representation with prediction-level attention to output the predicted
score.</p></li>
</ol>
<h3 id="flatten-and-embedding">Flatten and embedding</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330193624700.png"
alt="image-20230330193624700" />
<figcaption aria-hidden="true">image-20230330193624700</figcaption>
</figure>
<p>flatten high-order connection to the path: <span
class="math inline">\(u\rightarrow^{r1}v\rightarrow^{r2}e_{u1}\rightarrow^{r3}e_{v3}\)</span></p>
<p>embedding: initialized embedding vectors.</p>
<h3 id="gcn-learning-unit">GCN learning unit</h3>
<p>user and item use the same propagation and aggregate strategy.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330200154040.png"
alt="image-20230330200154040" />
<figcaption aria-hidden="true">image-20230330200154040</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330200204043.png"
alt="image-20230330200204043" />
<figcaption aria-hidden="true">image-20230330200204043</figcaption>
</figure>
<p><span class="math inline">\(h^T, W, b\)</span>: learned
parameters</p>
<p>neighbor sample(only get fixed number neighbor): <span
class="math inline">\(\alpha_{e_v,e_{nv}}\)</span> would be regarded as
the similarity of each neighbor entity and central entity. Through this
evidence, those neighbors with lower similarity would be filtered.</p>
<h3 id="interaction-signals-unit">Interaction signals unit</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330201306937.png"
alt="image-20230330201306937" />
<figcaption aria-hidden="true">image-20230330201306937</figcaption>
</figure>
<p>区别：上面是相加，下面事相乘</p>
<p>so GCN unit + interaction unit =</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330201407720.png"
alt="image-20230330201407720" />
<figcaption aria-hidden="true">image-20230330201407720</figcaption>
</figure>
<h3 id="predict">Predict</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330201630216.png"
alt="image-20230330201630216" />
<figcaption aria-hidden="true">image-20230330201630216</figcaption>
</figure>
<h1 id="entity2rec">Entity2rec</h1>
<h2 id="framework-3">Framework</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331170104462.png"
alt="image-20230331170104462" />
<figcaption aria-hidden="true">image-20230331170104462</figcaption>
</figure>
<h3 id="node2vec">node2vec</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331165454743.png"
alt="image-20230331165454743" />
<figcaption aria-hidden="true">image-20230331165454743</figcaption>
</figure>
<p>将图用random walk转化为word格式，用词袋模型计算vector。</p>
<h3 id="property-specific-knowledge-graph-embedding">Property-specific
knowledge graph embedding</h3>
<p>在node2vec基础上加上relation embedding，基于p子图在p空间上优化node
vector</p>
<p>maximize the dot product between vectors of the same neighborhood</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331170246301.png"
alt="image-20230331170246301" />
<figcaption aria-hidden="true">image-20230331170246301</figcaption>
</figure>
<p>Ze-negative sampling</p>
<p>N(e): neighbor of entity</p>
<h3 id="subgraph">subgraph</h3>
<h4 id="collaborative-content-subgraphs">Collaborative-content
subgraphs</h4>
<p>只保留单一relation，但连接性很差，对random walk效果不好</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331170405382.png"
alt="image-20230331170405382" />
<figcaption aria-hidden="true">image-20230331170405382</figcaption>
</figure>
<p>所有子图可以分成两张类型：feedback子图(user-item图)和其他子图</p>
<p>用下面方法来计算推荐分数：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331170819790.png"
alt="image-20230331170819790" />
<figcaption aria-hidden="true">image-20230331170819790</figcaption>
</figure>
<p>R+(u) denotes a set of items liked by the user u in the past.</p>
<p>s(x): similarity socre</p>
<h4 id="hybrid-subgraphs">Hybrid subgraphs</h4>
<p><span class="math inline">\(K_p^+=K_p \cup(u,feedback,i)\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331171032188.png"
alt="image-20230331171032188" />
<figcaption aria-hidden="true">image-20230331171032188</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331171139039.png"
alt="image-20230331171139039" />
<figcaption aria-hidden="true">image-20230331171139039</figcaption>
</figure>
<h1 id="hakg">HAKG</h1>
<p>总结：抽取u，i子图，进行正常的propagation之后，得到自图中所有entity的embedding，包括user和item。用self-attention提取entity相互影响信息，得到矩阵g（u，i），再用use
embedding，item
embedding和g（u，i）进行预测，充分利用了子图中的所有信息。</p>
<h2 id="framework-4">Framework</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230401160018626.png"
alt="image-20230401160018626" />
<figcaption aria-hidden="true">image-20230401160018626</figcaption>
</figure>
<ol type="1">
<li>Subgraph Construction ：it automatically constructs the expressive
subgraph that links the user-item pair to represent their
connectivity;</li>
<li>Hierarchical Attentive Subgraph Encoding ： the subgraph is further
encoded via a hierarchical attentive embedding learning procedure, which
first learns embeddings for entities in the subgraph with a layer-wise
propagation mechanism, and then attentively aggregates the entity
embeddings to derive the holistic subgraph embedding;</li>
<li>Preference Prediction ： with the well-learned embeddings of the
user-item pair and their subgraph connectivity, it uses non-linear
layers to predict the user’s preference towards the item.</li>
</ol>
<h3 id="subgraph-construction">Subgraph Construction</h3>
<p>path sampling and then reconstructs the subgraphs by assembling the
sampled paths between user-item pairs</p>
<h4 id="path-sampling">path sampling</h4>
<p>use random walk get path from u to i and length&lt;=6, uniformly
sample K paths</p>
<h4 id="path-assembling">Path Assembling</h4>
<p>just assemb the K paths</p>
<h3 id="hierarchical-attentive-subgraph-encoding">Hierarchical attentive
subgraph encoding</h3>
<h4 id="entity-embedding-learning">entity embedding learning</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230401173559684.png"
alt="image-20230401173559684" />
<figcaption aria-hidden="true">image-20230401173559684</figcaption>
</figure>
<h5 id="embedding-initialization">Embedding Initialization</h5>
<ol type="1">
<li>initial</li>
<li><span class="math inline">\(e_h^{(0)}=MLP(e_h \space concatenation
\space t_h)\)</span></li>
</ol>
<h5 id="semantics-propagation">Semantics Propagation</h5>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230401173656033.png"
alt="image-20230401173656033" />
<figcaption aria-hidden="true">image-20230401173656033</figcaption>
</figure>
<h5 id="semantics-aggregation">Semantics Aggregation</h5>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230401173801411.png"
alt="image-20230401173801411" />
<figcaption aria-hidden="true">image-20230401173801411</figcaption>
</figure>
<p>final entity embedding is <span
class="math inline">\(e_h^{(L)}\)</span></p>
<p>constitute an entity embedding matrix H(u,i) for the whole subgraph
:</p>
<p><span
class="math inline">\(H_{(u,i)}=[e_1,e_2,\cdots,e_n]\)</span></p>
<h4 id="sub-graph-embedding-learning">sub-graph embedding learning</h4>
<p>use self-attention mechanism optimize the entities embeding of
subgraph</p>
<p>Than use pooling method to get subgraph embedding</p>
<h3 id="prediction">Prediction</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230401175552413.png"
alt="image-20230401175552413" />
<figcaption aria-hidden="true">image-20230401175552413</figcaption>
</figure>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>HyperGraph</title>
    <url>/2023/05/26/HyperGraph/</url>
    <content><![CDATA[<p>Hypergraph</p>
<span id="more"></span>
<h1 id="hgnn">HGNN</h1>
<p>use the matrix to represent hypergraph</p>
<p>a hyperedge convolution operation is designed</p>
<p>can incorporate with multi-modal data and complicated data
correlations(use below figure2 method to combine different data type
)</p>
<h2 id="hypergraph-and-adjacency-matrix">Hypergraph and adjacency
matrix</h2>
<p>hypergraph-一条边可以同时连接多个点</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230605145232861.png"
alt="image-20230605145232861" />
<figcaption aria-hidden="true">image-20230605145232861</figcaption>
</figure>
<p>adjacency matrix:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230605145324681.png"
alt="image-20230605145324681" />
<figcaption aria-hidden="true">image-20230605145324681</figcaption>
</figure>
<h2 id="method">Method</h2>
<h3 id="hypergraph-learning-statement">hypergraph learning
statement</h3>
<p>hypergraph defined as <span class="math inline">\(G = (V,\varepsilon,
W)\)</span></p>
<p>the degree of vertex <span class="math inline">\(v\)</span> defined
as <span class="math inline">\(d(v)=\sum_{e\in
\varepsilon}h(v,e)\)</span>——一个点与多少条边相连</p>
<p>the degree of hyperedge defined as <span class="math inline">\(\delta
(v)=\sum_{v\in V}h(v,e)\)</span>——一条边与多少个点相连</p>
<p><span class="math inline">\(D_e\)</span> and <span
class="math inline">\(D_v\)</span> denote the diagonal matrices of the
edge degrees and the vertex degrees, respectively</p>
<p>example:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230605153837827.png"
alt="image-20230605153837827" />
<figcaption aria-hidden="true">image-20230605153837827</figcaption>
</figure>
<h3 id="spectral-convolution-on-hypergraph">Spectral convolution on
hypergraph</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230605155756968.png"
alt="image-20230605155756968" />
<figcaption aria-hidden="true">image-20230605155756968</figcaption>
</figure>
<p>频域理解不深，要完全看懂要好久，先简单略一下。</p>
<h1 id="hypergcn">HyperGCN</h1>
<p>Hypergraph, a novel way of training a GCN for SSL on hypergraphs
based on tools from sepctral theory of hypergraphs</p>
<p>主要是把hypergraph转为简单拉普拉斯图</p>
<h2 id="method-1">Method</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230606094309361.png"
alt="image-20230606094309361" />
<figcaption aria-hidden="true">image-20230606094309361</figcaption>
</figure>
<h3 id="hypergraph-generation">hypergraph generation</h3>
<ol type="1">
<li>对图上任意边<span class="math inline">\(e\in E\)</span>, 令<span
class="math inline">\((i_e,j_e):=argmax_{i,j\in e}|S_i-S_j|\)</span>
,即返回同一个边上距离最远的两个顶点表示<span
class="math inline">\((i_e,j_e)\)</span>为随机,切断点的联系。</li>
<li>为剩下的边添加权重，权重为hyperedge的权重，构造简单的邻接矩阵。</li>
<li>归一化计算拉普拉斯矩阵</li>
</ol>
<h3 id="gnn">GNN</h3>
<p>利用带权重的拉普拉斯矩阵计算GCN</p>
<h1 id="hypergraph-convolution-and-hypergraph-attention">Hypergraph
Convolution and Hypergraph Attention</h1>
]]></content>
      <categories>
        <category>GNN</category>
        <category>hypergraph</category>
      </categories>
  </entry>
  <entry>
    <title>Initialization</title>
    <url>/2023/06/12/Initialization/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<p><a href="https://zhuanlan.zhihu.com/p/653754525">Xavier
初始化【简单易懂】 - 知乎 (zhihu.com)</a></p>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>KGAT</title>
    <url>/2023/02/24/KGAT/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>利用KG作为辅助信息，并将KG与user-item graph 整合为一个图</p>
<h2 id="background-1">Background</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224155340260.png"
alt="image-20230224155340260" />
<figcaption aria-hidden="true">image-20230224155340260</figcaption>
</figure>
<p><strong>Previous model:</strong></p>
<p>CF: behaviorally similar users would exhibit similar preference on
items.</p>
<p><strong>focus on the histories of similar users who also watched
<span class="math inline">\(i1\)</span>, i.e., <span
class="math inline">\(u4\)</span> and <span
class="math inline">\(u5\)</span>;</strong></p>
<p>SL: transform side information into a generic feature vector,
together with user ID and item ID, and feed them into a supervised
learning (SL) model to predict the score.</p>
<p><strong>emphasize the similar items with the attribute <span
class="math inline">\(e1\)</span>, i.e.$ i2$.</strong></p>
<p><strong>current problem:</strong></p>
<p>existing SL methods fail to unify them, and ignore other
relationships in the graph:</p>
<ol type="1">
<li>the users in the yellow circle who watched other movies directed by
the same person <span class="math inline">\(e_1\)</span>.</li>
<li>the items in the grey circle that share other common relations with
<span class="math inline">\(e_1\)</span>.</li>
</ol>
<h2 id="user-item-bipartite-graph-g_1">User-Item Bipartite Graph: <span
class="math inline">\(G_1\)</span></h2>
<p><span class="math display">\[
\{(u,y_{ui},i)|u\in U, i\in I\}
\]</span> <span class="math inline">\(U\)</span>: user sets</p>
<p><span class="math inline">\(I\)</span>: item sets</p>
<p><span class="math inline">\(y_{ui}\)</span>: if user <span
class="math inline">\(u\)</span> interacts with item <span
class="math inline">\(i\)</span> <span
class="math inline">\(y_{ui}\)</span>=, else <span
class="math inline">\(y_{ui}\)</span>=0.</p>
<h2 id="knowledge-graph-g2">Knowledge Graph <span
class="math inline">\(G2\)</span></h2>
<p><span class="math display">\[
\{(h,r,t)|h,t\in E, r\in R\}
\]</span></p>
<p><span class="math inline">\(t\)</span> there is a relationship <span
class="math inline">\(r\)</span> from head entity <em>h</em> to tail
entity <span class="math inline">\(t\)</span>.</p>
<h2 id="ckg-combination-of-g1-and-g2"><span
class="math inline">\(CKG\)</span>: Combination of <span
class="math inline">\(G1\)</span> and <span
class="math inline">\(G2\)</span></h2>
<ol type="1">
<li>represent each user-item behavior as a triplet $ (u,
Interact,i)<span class="math inline">\(, where\)</span> y^{ui}$ =
1.</li>
<li>we establish a set of item-entity alignments</li>
</ol>
<p><span class="math display">\[
A = \{(i, e)|i ∈ I, e ∈ E \}
\]</span></p>
<ol start="3" type="1">
<li>based on the item-entity alignment set, the user-item graph can be
integrated with KG as a unified graph.</li>
</ol>
<p><span class="math display">\[
G = \{(h,r,t)|h,t ∈ E^′,r ∈R^′\}
\]</span></p>
<p><span class="math display">\[
E^′ = E ∪ U
\]</span></p>
<p><span class="math display">\[
R^′ = R ∪ {Interact}
\]</span></p>
<h1 id="methodology">Methodology</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222185609261.png"
alt="image-20230222185609261" />
<figcaption aria-hidden="true">image-20230222185609261</figcaption>
</figure>
<p>KGAT has three main components:</p>
<ol type="1">
<li>Embedding layer</li>
<li>Attentive embedding propagation layer</li>
<li>prediction layer</li>
</ol>
<h2 id="embedding-layer">Embedding layer</h2>
<p>Using <strong>TransR</strong> to calculate embedding</p>
<p><strong>Assumption</strong>: if a triplet (h,r,t) exist in the graph,
<span class="math display">\[
e^r_h+e_r\approx e_t^r
\]</span> Herein, <span class="math inline">\(e^h\)</span>, <span
class="math inline">\(e^t\)</span> ∈ <span
class="math inline">\(R^d\)</span> and <span
class="math inline">\(e^r\)</span> ∈ <span
class="math inline">\(R^k\)</span>are the embedding for <em>h</em>,
<em>t</em>, and <em>r</em>; and <span
class="math inline">\(e^r_h\)</span>, <span
class="math inline">\(e^r_t\)</span> are the projected representations
of <span class="math inline">\(e^h\)</span>, <span
class="math inline">\(e^t\)</span> in the relation <em>r</em>’s
space.</p>
<p><strong>Plausibility score</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222193700417.png"
alt="image-20230222193700417" />
<figcaption aria-hidden="true">image-20230222193700417</figcaption>
</figure>
<p><span class="math inline">\(W_r ∈ R^{k\times d}\)</span> is the
transformation matrix of relation <em>r</em>, which projects entities
from the <em>d</em>-dimension entity space into the <em>k</em> dimension
relation space.</p>
<p>A lower score suggests that the triplet is more likely to be
true.</p>
<p><strong>Loss</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222195306105.png"
alt="image-20230222195306105" />
<figcaption aria-hidden="true">image-20230222195306105</figcaption>
</figure>
<p><span class="math inline">\(\{(h,r,t,t^′ )|(h,r,t) \in G, (h,r,t^′ )
\notin G\}\)</span>, <span class="math inline">\((h,r,t^′ )\)</span> is
a negative sample constructed by replacing one entity in a valid triplet
randomly.</p>
<p><em>σ</em>(·): sigmoid function, ——》将分数映射再0-1区间，归一化</p>
<p>？？？？？？？？？？？why this layer model working as a
regularizer</p>
<h2 id="attentive-embedding-propagation-layersupon-gcn">Attentive
Embedding Propagation Layers(upon GCN)</h2>
<h3 id="first-order-propagation">First-order propagation</h3>
<p>和之前模型不同，这个的propagation layer encode了<span
class="math inline">\(e_r\)</span>.</p>
<p>For entity h, the information propagating from neighbor is :</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222201217039.png"
alt="image-20230222201217039" />
<figcaption aria-hidden="true">image-20230222201217039</figcaption>
</figure>
<p><span class="math inline">\(π(h,r,t)\)</span>: to controls the decay
factor on each propagation on edge (<em>h</em>,<em>r</em>,<em>t</em>),
indicating how much information is propagated</p>
<p>from <em>t</em> to <em>h</em> conditioned to relation <em>r</em>.</p>
<p>For <span class="math inline">\(π(h,r,t)\)</span>, we use attention
mechanism:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222204949423.png"
alt="image-20230222204949423" />
<figcaption aria-hidden="true">image-20230222204949423</figcaption>
</figure>
<p>This makes the attention score dependent on the distance between
<span class="math inline">\(e^h\)</span> and <span
class="math inline">\(e^t\)</span> in the relation <em>r</em>’s
space.</p>
<p>这里，tanh用于增加非线性因素；但不缺定是否有归一化作用？？？？？归一化就可以把这个function的大小集中在角度上，但是这样<span
class="math inline">\(e^h_t\)</span>也没有归一化，到时候看看输出参数</p>
<p>and than use softmax to normalize(no need to use as<span
class="math inline">\(\frac1{|N_t |}\)</span><span
class="math inline">\(\frac1{|N_t ||N_h |}\)</span>)</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222234256082.png"
alt="image-20230222234256082" />
<figcaption aria-hidden="true">image-20230222234256082</figcaption>
</figure>
<p>The final part is aggregation, threre are three choices:</p>
<ol type="1">
<li>GCN aggregator</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222235616598.png"
alt="image-20230222235616598" />
<figcaption aria-hidden="true">image-20230222235616598</figcaption>
</figure>
<ol start="2" type="1">
<li>GraphSage aggregator</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222235806956.png"
alt="image-20230222235806956" />
<figcaption aria-hidden="true">image-20230222235806956</figcaption>
</figure>
<ol start="3" type="1">
<li>Bi-Interaction aggregator</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223000647293.png"
alt="image-20230223000647293" />
<figcaption aria-hidden="true">image-20230223000647293</figcaption>
</figure>
<h3 id="multi-layer-propagation">Multi-layer propagation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223000834658.png"
alt="image-20230223000834658" />
<figcaption aria-hidden="true">image-20230223000834658</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223000850960.png"
alt="image-20230223000850960" />
<figcaption aria-hidden="true">image-20230223000850960</figcaption>
</figure>
<h2 id="model-prediction">Model Prediction</h2>
<p>multi-layers combination and inner product</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223001515690.png"
alt="image-20230223001515690" />
<figcaption aria-hidden="true">image-20230223001515690</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223001526127.png"
alt="image-20230223001526127" />
<figcaption aria-hidden="true">image-20230223001526127</figcaption>
</figure>
<h2 id="optimizazion">Optimizazion</h2>
<h3 id="loss">loss</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223002144083.png"
alt="image-20230223002144083" />
<figcaption aria-hidden="true">image-20230223002144083</figcaption>
</figure>
<p><span class="math inline">\(L_{cf}\)</span> is BPR Loss</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223002439536.png"
alt="image-20230223002439536" />
<figcaption aria-hidden="true">image-20230223002439536</figcaption>
</figure>
<p><span class="math inline">\(L_{kg}\)</span> is loss forTranR .</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222195306105.png"
alt="image-20230222195306105" />
<figcaption aria-hidden="true">image-20230222195306105</figcaption>
</figure>
<h3 id="optimizer">Optimizer</h3>
<p>Adam</p>
<h3 id="updata-method">updata method</h3>
<p>we update the embeddings for all nodes;</p>
<p>hereafter, we sample a batch of (<em>u</em>,<em>i</em>, <em>j</em>)
randomly, retrieve their representations after <em>L</em> steps of
propagation, and then update model parameters by using the gradients of
the prediction loss.</p>
<p>在同一个epoch中，先把所以数据扔进tranR训练，得到loss（此时不更新参数）</p>
<p>然后sample算BPR LOSS</p>
<h1 id="experiments">EXPERIMENTS</h1>
<h2 id="rq1-performance-comparison">RQ1: Performance Comparison</h2>
<ol type="1">
<li>regular dataset</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223004843914.png"
alt="image-20230223004843914" />
<figcaption aria-hidden="true">image-20230223004843914</figcaption>
</figure>
<ol start="2" type="1">
<li><p>Sparsity Levels</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223005253034.png"
alt="image-20230223005253034" />
<figcaption aria-hidden="true">image-20230223005253034</figcaption>
</figure></li>
</ol>
<p>KGAT outperforms the other models in most cases, especially on the
two sparsest user groups.</p>
<p>说明KGAT能够缓解稀疏性影响</p>
<h2 id="rq2study-of-kgat">RQ2：Study of KGAT</h2>
<ol type="1">
<li>study of layer influence and effect of aggregators</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223010038345.png"
alt="image-20230223010038345" />
<figcaption aria-hidden="true">image-20230223010038345</figcaption>
</figure>
<ol start="2" type="1">
<li><p>cut attention layer and TransR layer</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223010347815.png"
alt="image-20230223010347815" />
<figcaption aria-hidden="true">image-20230223010347815</figcaption>
</figure></li>
</ol>
<h1 id="source-code">Source code</h1>
<h2 id="dataprocess">DataProcess</h2>
<h3 id="load-data">Load data</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data:[[u1,interacted_item1],[u1,interacted_item2],[u2,interacted_item1]]</span><br><span class="line"></span><br><span class="line">train_user_dict:&#123;</span><br><span class="line">    user_id1:[interacted_item1,interacted_item2,...],</span><br><span class="line">    user_id2:[...]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">kg_data:[[head_e,relation,tail_e],[head_e,relation,tail_e]]</span><br><span class="line"></span><br><span class="line">kg_dict:&#123;</span><br><span class="line">    head:[(tail,relation), (tail,relation),...]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">relation_dict:&#123;</span><br><span class="line">    relation:[(head,tail),(head,tail),...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3
id="generate-the-adjacency-matrices-and-matrices-after-laplacian">generate
the adjacency matrices and matrices after Laplacian</h3>
<ol type="1">
<li><p>regard interacted as relation 0, now the number of relations is
<span class="math inline">\(self.n\_relations+1\)</span></p></li>
<li><p>every relation <span class="math inline">\((idx)\)</span> convert
to 2 adjacency matrix (by inversing cols and rows), which representate
as 2 new relations <span class="math inline">\((idx,
self.n\_relations+idx)\)</span>：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/643ED74EDA6A241DF772EA9C9435EFBD.png"
alt="643ED74EDA6A241DF772EA9C9435EFBD" />
<figcaption
aria-hidden="true">643ED74EDA6A241DF772EA9C9435EFBD</figcaption>
</figure></li>
</ol>
<p>As a result: we get adj_list, adj_r_list</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">adj_list: [adjancy matrix1, adjancy matrix2,adjancy matrix3,...]</span><br><span class="line">adj_r_list: The relation adjancy matrix Correspondento to</span><br><span class="line">			e.g.[0,self.n_relations+0,1,self.n_relations+1,2,self.n_relations+2,...]</span><br></pre></td></tr></table></figure>
<p>Than, genarate adjancy matrix after laplacian normalization and save
in self.lap_list.</p>
<h3 id="update-kg-dict">Update kg dict</h3>
<p>according to the change of relation, update kg dict</p>
<h3 id="generate-batch-data">Generate batch data</h3>
<h2 id="build_model">build_model</h2>
<h3 id="placeholder-definition">Placeholder definition</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_build_inputs</span>(<span class="params">self</span>):</span><br><span class="line">    tf.compat.v1.disable_eager_execution()</span><br><span class="line">    <span class="comment"># placeholder definition</span></span><br><span class="line">    self.users = tf.placeholder(tf.int32, shape=(<span class="literal">None</span>,))</span><br><span class="line">    self.pos_items = tf.placeholder(tf.int32, shape=(<span class="literal">None</span>,))</span><br><span class="line">    self.neg_items = tf.placeholder(tf.int32, shape=(<span class="literal">None</span>,))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># for knowledge graph modeling (TransD)</span></span><br><span class="line">    self.A_values = tf.placeholder(tf.float32, shape=[<span class="built_in">len</span>(self.all_v_list)],</span><br><span class="line">                                   name=<span class="string">&#x27;A_values&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    self.h = tf.placeholder(tf.int32, shape=[<span class="literal">None</span>], name=<span class="string">&#x27;h&#x27;</span>)</span><br><span class="line">    self.r = tf.placeholder(tf.int32, shape=[<span class="literal">None</span>], name=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    self.pos_t = tf.placeholder(tf.int32, shape=[<span class="literal">None</span>], name=<span class="string">&#x27;pos_t&#x27;</span>)</span><br><span class="line">    self.neg_t = tf.placeholder(tf.int32, shape=[<span class="literal">None</span>], name=<span class="string">&#x27;neg_t&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="trainable-weight-definition">trainable weight definition</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_build_weights</span>(<span class="params">self</span>):</span><br><span class="line">    all_weights = <span class="built_in">dict</span>()</span><br><span class="line">    initializer = tf.keras.initializers.glorot_normal()</span><br><span class="line"></span><br><span class="line">    all_weights[<span class="string">&#x27;user_embed&#x27;</span>] = tf.Variable(initializer([self.n_users, self.emb_dim]),</span><br><span class="line">                                            name=<span class="string">&#x27;user_embed&#x27;</span>)</span><br><span class="line">    all_weights[<span class="string">&#x27;entity_embed&#x27;</span>] = tf.Variable(initializer([self.n_entities,</span><br><span class="line">                                                           self.emb_dim]),</span><br><span class="line">                                                           name=<span class="string">&#x27;entity_embed&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    all_weights[<span class="string">&#x27;relation_embed&#x27;</span>] = tf.Variable(initializer([self.n_relations,</span><br><span class="line">                                             self.kge_dim]),name=<span class="string">&#x27;relation_embed&#x27;</span>)</span><br><span class="line">    <span class="comment"># E_h, E_t to E_r space</span></span><br><span class="line">    all_weights[<span class="string">&#x27;trans_W&#x27;</span>] = tf.Variable(initializer([self.n_relations, </span><br><span class="line">                                                      self.emb_dim, self.kge_dim]))</span><br><span class="line">    self.weight_size_list = [self.emb_dim] + self.weight_size</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>KGCN</title>
    <url>/2023/03/02/KGCN/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<h2 id="cf-questions">CF Questions</h2>
<ol type="1">
<li>sparsity</li>
<li>cold start</li>
</ol>
<h2 id="kg-benefits">KG Benefits</h2>
<ol type="1">
<li>The rich semantic relatedness among items in a KG can help explore
their latent connections and improve the <em>precision</em> of
results;</li>
<li>The various types of relations in a KG are helpful for extending a
user’s interests reasonably and increasing the <em>diversity</em> of
recommended items;</li>
<li>KG connects a user’s historically-liked and recommended items,
thereby bringing <em>explainability</em> to recommender systems.</li>
</ol>
<h2 id="previous-kg-method">Previous KG Method</h2>
<h3 id="knowledge-graph-embedding">Knowledge graph embedding</h3>
<p>Example: TransE [1] and TransR [12] assume <em>head</em> +
<em>relation</em> = <em>tail</em>, which focus on modeling rigorous
semantic relatedness</p>
<p>Problem: KGE methods are more suitable for in-graph applications such
as KG completion and link prediction rather than the recommendation
system.</p>
<h3 id="path-base-method">Path-base Method</h3>
<p>Example: PER, FMG</p>
<p>problem: Labor sensitivity</p>
<h2 id="ripple-net">Ripple Net</h2>
<p>problem:</p>
<ol type="1">
<li>the importance of relations is weakly characterized in RippleNet,
because the relation <strong>R</strong> can hardly be trained to capture
the sense of importance in the quadratic form <strong>v</strong>
⊤<strong>Rh</strong> (<strong>v</strong> and <strong>h</strong> are
embedding vectors of two entities).</li>
<li>The size of ripple set may go unpredictably with the increase of the
size of KG, which incurs heavy computation and storage overhead.</li>
</ol>
<h2 id="solution-kgcn">Solution: KGCN</h2>
<ol type="1">
<li>Propagation and aggregation mechanism.</li>
<li>Attention mechanism.</li>
<li>sample a fixed-size neighborhood to control compute cost.</li>
</ol>
<h1 id="model">Model</h1>
<h2 id="single-layer">Single layer</h2>
<p>Consider a pair(u,v)</p>
<h3 id="overall-of-single-layer">Overall of single layer</h3>
<p>!!!Propagation only use for updating of item's vector</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114017500.png"
alt="image-20230302114017500" />
<figcaption aria-hidden="true">image-20230302114017500</figcaption>
</figure>
<h3 id="propagation">Propagation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302111400335.png"
alt="image-20230302111400335" />
<figcaption aria-hidden="true">image-20230302111400335</figcaption>
</figure>
<p><span class="math inline">\(N(v)\)</span> is the neighbor set of
v</p>
<p>e$ is the embedding of entity(parameter to train)</p>
<p><span class="math inline">\(\pi^u_{r_{v,e}}\)</span> is attention
weight</p>
<p><span class="math inline">\(r_{v,e}\)</span> represent the relation
of v and e</p>
<h3 id="attention">Attention</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302112401231.png"
alt="image-20230302112401231" />
<figcaption aria-hidden="true">image-20230302112401231</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302112315690.png"
alt="image-20230302112315690" />
<figcaption aria-hidden="true">image-20230302112315690</figcaption>
</figure>
<p><span class="math inline">\(g : R^d ×R^d → R\)</span> (e.g., inner
product:内积能计算相似度)to compute the score between a user and a
relation</p>
<p><span class="math inline">\(u\in R\)</span>, <span
class="math inline">\(r\in R\)</span> : embedding of user and
relation(parameter to train)</p>
<p><span class="math inline">\(\pi^u_{r_{v,e}}\)</span>characterizes the
importance of relation <em>r</em> to user <em>u</em>. E.g. example, a
user may have more interests in the movies that share the same “star"
with his historically liked ones, while another user may be more
concerned about the “genre" of movies.</p>
<p>!!!!!!!!!!个性化！！！用户对不同关系重视程度不同！！</p>
<p>所以KGCN不用propagation更新用户的原因是否是因为希望user的embedding能专注于提取个性化信息（提高用户和重要relation的相似度），但是这样是否会让user和item没那么好聚类？</p>
<h3 id="sample-the-number-of-neighbors">Sample the number of
neighbors</h3>
<p>limit the neighbor number in K(can be config)</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302113734945.png"
alt="image-20230302113734945" />
<figcaption aria-hidden="true">image-20230302113734945</figcaption>
</figure>
<p>S(<em>v</em>) is also called the (single layer) <em>receptive
field</em> of entity <em>v</em></p>
<p>example K=2:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302113907523.png"
alt="image-20230302113907523" />
<figcaption aria-hidden="true">image-20230302113907523</figcaption>
</figure>
<h3 id="aggregation">aggregation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114047762.png"
alt="image-20230302114047762" />
<figcaption aria-hidden="true">image-20230302114047762</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114057869.png"
alt="image-20230302114057869" />
<figcaption aria-hidden="true">image-20230302114057869</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114106975.png"
alt="image-20230302114106975" />
<figcaption aria-hidden="true">image-20230302114106975</figcaption>
</figure>
<p><span class="math inline">\(\sigma\)</span> is ReLU</p>
<h2 id="multi-layer">Multi layer</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/9C6A6E4346910DDDD43EBB55E1A2FE6C.png"
alt="9C6A6E4346910DDDD43EBB55E1A2FE6C" />
<figcaption
aria-hidden="true">9C6A6E4346910DDDD43EBB55E1A2FE6C</figcaption>
</figure>
<p>First we consider the Receptive-Field:</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/1739B546615C1AEDC69F7A3514E2E458.png" alt="1739B546615C1AEDC69F7A3514E2E458" style="zoom:67%;" /></p>
<p>We first update eneities in M[0] by using propagation and aggregation
to get the high-hop neighbor information.</p>
<p>And then gradually narrow it down, and finally focus on v.</p>
<p>Note that we have only one user in one pair, every relations will
calculate the score with this user embedding</p>
<h2 id="predict">Predict</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114437063.png"
alt="image-20230302114437063" />
<figcaption aria-hidden="true">image-20230302114437063</figcaption>
</figure>
<h2 id="loss-function">Loss function</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302120154981.png"
alt="image-20230302120154981" />
<figcaption aria-hidden="true">image-20230302120154981</figcaption>
</figure>
<p><span class="math inline">\(J\)</span> is cross-entropy loss</p>
<p><em>P</em> is a negative sampling distribution, and <span
class="math inline">\(T_u\)</span> is the number of negative samples for
user <em>u</em>. In this paper,</p>
<h1 id="experiment">Experiment</h1>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>KR-GCN</title>
    <url>/2023/04/26/KR-GCN/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>previous study:</p>
<ol type="1">
<li>error propagation: consider all paths between every user-item pair
might involve irrelevant one</li>
<li>weak explainability</li>
</ol>
<h1 id="model">Model</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230426111943016.png"
alt="image-20230426111943016" />
<figcaption aria-hidden="true">image-20230426111943016</figcaption>
</figure>
<p>4 parts:</p>
<ol type="1">
<li>the Graph encoding module: learn the representations of nodes in the
heterogeneous graph.</li>
<li>the Path Extraction and Selection module: extract paths between
users and items from the heterogeneous graph and select higher-quality
reasoning paths</li>
<li>the Path Encoding module: learn the representations of the selection
reasoning paths.</li>
<li>the Preference Prediction module: predicts users’ preferences
according to the reasoning paths.</li>
</ol>
<h2 id="graph-encoding---gcn">Graph Encoding - GCN</h2>
<ol type="1">
<li>propagation and aggregation</li>
</ol>
<p>initialized randomly</p>
<p>weighted sum aggregator : the neighborhood nodes are aggregated via
mean function.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230426113233853.png"
alt="image-20230426113233853" />
<figcaption aria-hidden="true">image-20230426113233853</figcaption>
</figure>
<ol start="2" type="1">
<li>weight sum to merge every layers</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230426114004010.png"
alt="image-20230426114004010" />
<figcaption aria-hidden="true">image-20230426114004010</figcaption>
</figure>
<h2 id="path-extraction">Path Extraction</h2>
<p>we prune irrelevant paths between each user-item pair.</p>
<p>we extract multi-hop paths with the limitation that hops in every
single path are less than l.</p>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>LLM</title>
    <url>/2024/04/10/LLM/</url>
    <content><![CDATA[<p>LLM</p>
<span id="more"></span>
<h1 id="nlp">NLP</h1>
<h2 id="transformer">Transformer</h2>
<ul>
<li>self-attention表达式</li>
</ul>
<p><span class="math display">\[
Softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</span></p>
<ul>
<li><p><strong>why scaling</strong></p>
<p><span class="math inline">\(QK^t\)</span>相乘会让数值变大，scaling
后进行softmax可以使得输入的数据的分布变得更好，避免数据进入softmax敏感区间，防止梯度消失，让模型能够更容易训练。</p></li>
<li><p><strong>可以不除以<span
class="math inline">\(\sqrt{d_k}\)</span>吗</strong> ？</p>
<p>可以，只要有别的方法可以缓解梯度消失即可</p></li>
<li><p><strong>self-attention一定要这样表达吗</strong></p>
<p>不一定，只要可以建模相关性就可以。这样表示的优点：高速计算（矩阵乘法）；表达能力强（query可以主动去关注到其他的key并在value上进行强化，并且忽略不相关的其他部分），模型容量够（引入了project_q/k/v，att_out，多头）。</p></li>
<li><p><strong>为什么transformer用Layer Norm？有什么用？</strong></p>
<p>任何norm的意义都是为了让使用norm的网络的输入的数据分布变得更好，也就是转换为标准正态分布，避免数值进入敏感度区间，以减缓梯度消失，从而更容易训练。当然，这也意味着舍弃了除此维度之外其他维度的其他信息。layer
norm舍弃了batch维度信息； batch Norm舍弃了layer维度信息。</p></li>
<li><p><strong>为什么不用BN</strong></p>
<p>BN广泛应用于CV，针对同一特征，以跨样本的方式开展归一化，也就是对不同样本的同一channel间的所有像素值进行归一化，因此不会破坏不同样本同一特征之间的关系，毕竟“减均值，除标准差”只是一个平移加缩放的线性操作。在“身高体重”的例子中，这就意味着“归一化前是高个儿的归一化后仍然是高个儿，归一化前胖的归一化后也不会变瘦”。这一性质进而决定了经过归一化操作后，样本之间仍然具有可比较性。但是，同一样本特征与特征之间的不再具有可比较性，也就是上一个问题中我所说的“舍弃了除此维度之外其他维度的其他信息”。</p></li>
</ul>
<p>​
但是在NLP中对不同样本同一特征的信息进行归一化没有意义，而且不能舍弃同一个样本的不同维度的信息。</p>
<ul>
<li><p><strong>Bert为什么要搞一个position embedding？</strong></p>
<p>self-attention无法表达位置信息，用三角函数避免句子过长时位置编码太大。</p></li>
<li><p><strong>Bert为什么三个embedding可以相加？</strong></p>
<p>在实际场景中，叠加是一个更为常态的操作。比如声音、图像等信号。一个时序的波可以用多个不同频率的正弦波叠加来表示。只要叠加的波的频率不同，我们就可以通过傅里叶变换进行逆向转换。</p>
<p>一串文本也可以看作是一些时序信号，也可以有很多信号进行叠加，只要频率不同，都可以在后面的复杂神经网络中得到解耦（但也不一定真的要得到解耦）。在BERT这个设定中，token，segment，position明显可以对应三种非常不同的频率。</p></li>
<li><p><strong>transformer为什么要用三个不一样的QKV？</strong></p>
<p>增强网络的表达能力。</p></li>
<li><p><strong>为什么要多头？举例说明多头相比单头注意力的优势？</strong></p>
<p>增强网络的表达能力。不同的头关注不同信息。</p>
<p>假设有一个句子"the cat, which is black, sat on the
mat"。在处理"sat"这个词时，一个头可能会更注"cat"，因为"cat"是"sat"的主语；另一个头可能会更关注"on
the mat"，因为这是"sat"的宾语；还有一个头可能会关注"which is
black"，因为这是对"cat"的修饰。</p>
<p>经过多头之后，我们还需要att_out线性层来做线性变换，以自动决定（通过训练）对每个头的输出赋予多大的权重，从而在最终的输出中强调一些头的信息，而忽视其他头的信息。</p></li>
<li><p><strong>为什么Bert中要用WordPiece/BPE这样的subword
Token？</strong></p>
<p>避免OOV（Out Of
Vocabulary），也就是词汇表外的词。在NLP中，通常会预先构建一个词汇表，包含所有模型能够识别的词。然而，总会有一些词没有出现在预先构建的词汇表中，这些词就是
OOV。</p>
<p>传统的处理方式往往是将这些 OOV 映射到一个特殊的符号，如
<code>&lt;UNK&gt;</code>，但这种方式无法充分利用 OOV
中的信息。例如，对于词汇表中没有的词 "unhappiness"，如果直接映射为
<code>&lt;UNK&gt;</code>，则模型就无法理解它的含义。</p>
<p>WordPiece/Byte Pair Encoding (BPE) 等基于子词的分词方法提供了一种解决
OOV
问题的方式。现在更多的语言大模型选择基于BPE的方式，只不过BERT时代更多还是WordPiece。BPE
通过将词分解为更小的单元（子词或字符），可以有效地处理词汇表外的词。对于上面的
"unhappiness" 例子，即使 "unhappiness"
本身不在词汇表中，但是它可以被分解为 "un"、"happiness"
等子词，而这些子词可能在词汇表中。这样，模型就可以通过这些子词来理解
"unhappiness" 的含义。</p>
<p>另一方面就是，BPE本身的语义粒度也很合适，一个token不会太大，也不会小到损失连接信息（如一个字母）。</p></li>
<li><p><strong>Bert中为什么要在开头加个[CLS]?</strong></p>
<p>用[cls]学习整个句子的表示</p></li>
<li><p><strong>不用[CLS]的语义输出，有其他方式可以代替吗？</strong></p>
<p>这个问题还是考察到了[CLS]的核心内涵，也就是如何获得整个sentence的语义表示。既然不让使用特意训好的[CLS]，那我们就从每个token得到的embedding入手，把所有的token弄到一起。</p>
<p>很直观的思路，就是对BERT的所有输出词向量（忽略[CLS]和[SEP]）应用MaxPooling和AvgPooling，然后将得到的两个向量拼接起来，作为整个序列的表示。这样做的话可以同时保留序列中最显著的特征（通过MaxPooling）和整体的，均衡的特征（通过AvgPooling）。</p></li>
<li><p><strong>Bert中有哪些地方用到了mask?</strong></p>
<p><strong>预训练任务Masked Language Model (MLM)：</strong></p>
<p>主要的思想是，把输入的其中一部分词汇随机掩盖，模型的目标是预测这些掩盖词汇。这种训练方式使得每个位置的BERT都能学习到其上下文的信息。</p>
<p><strong>self-attention的计算：</strong></p>
<p>不同样本的seq_len不一样。但是由于输出的seq_len需要一致，所以需要通过补padding来对齐。而在attention中我们不希望一个token去注意到这些padding的部分，attention中的mask就是来处理掉这些无效的信息的。</p>
<p>具体来说就是在softmax前每个都设为-inf，然后过完softmax后"padding"部分的权重就会接近于零，query
token就不会分配注意力权重了。</p>
<p><strong>下游任务的decoder</strong>：</p>
<p>在做next token
prediction的时候，模型是根据前面已有的tokens来做的，也就是看不到未来的tokens的信息。而在我们训练的过程中，通常采用teacher
forcing的策略，也就是我们当然会把完整的标签喂给模型，但是由于在一个一个生成next
token的过程中，模型应该是一个一个往外“蹦“字的过程（想想chatgpt回复你的样子）所以我们会遮盖掉seqence中当前位置之后信息，以防止模型利用未来信息，也就是信息泄露。mask掉后模型的注意力只会集中在此前的序列上。</p></li>
<li><p><strong>Bert中self attention 计算复杂度如何？</strong></p>
<p><span
class="math inline">\(O(d_L^2)\)</span>，因为输入的序列的每一个token都要对这个序列上的所有token去求一个attention
score。</p></li>
<li><p><strong>有什么技术降低复杂度提升输入长度的？</strong></p>
<p>比如Sparse
Attention，放弃对全文的关注，只关心局部的语义组合，相当于self-attention上又加了一些mask，这样的话就可以降低复杂度，而且下游任务的语义关联性的体现往往是局部/稀疏的。</p></li>
<li><p><strong>为什么以前char level/subword
level的NLP模型表现一般都比较差，但是到了bert这里就比较好？</strong></p>
<p>还是归功于Transformers，因为对于字符级别（char-level）或者子词级别（subword-level）的NLP模型，挑战在于需要模型能够理解字符或者子词组合起来形成词语和句子的语义，这对模型的能力有很高的要求。</p>
<p>然而，以前NLP模型没办法做到很深，两层lstm基本就到极限了，非线性成长路线过分陡峭，所以增加网络容量的时候，降低了泛化能力。</p>
<p>Bert降低了输入的复杂度，提升了模型的复杂度。模型多层产生的非线性增长平滑，可以加大网络容量，同时增强容量和泛化能力。</p></li>
<li><p><strong>Bert为什么要使用warmup的学习率trick</strong></p>
<p>主要是考虑到训练的初始阶段params更新比较大，可能会使模型陷入local
minima或者overfitting。</p>
<p>warmup就是把lr从一个较小的值线性增大到预设，以减缓参数震荡，让训练变得比较smooth，当模型参数量上来后这种策略变得更重要了。</p></li>
<li><p><strong>为什么说GPT是单向的Bert是双向的？</strong></p>
<p>这也是decoder-only和encoder-only的区别。</p>
<p>decoder-only架构的生成模型在输出的时候只能看到当前位置前的tokens，也就是屏蔽了序列后面的位置，以适配NTP任务。</p>
<p>encoder-only架构的编码模型在输出的时候可以利用前后位置的tokens，以适配MLM任务。</p>
<p>具体的做法是self-attention加不加casual
mask，也就是遮不遮住序列后面的内容。</p></li>
<li><p><strong>Bert如何处理一词多义？</strong></p>
<p>一词多义指的是在不同句子中token有不同的含义。</p>
<p>这正是self-attention解决的，搭配上MLM的任务，就可以让每个token会注意到上下文的其他token来得到自己的embedding。</p></li>
<li><p><strong>Bert中的transformer和原生的transformer有什么区别？</strong></p>
<p>其实很多，如果我们只讨论模型架构，也就是对比<a
href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762">Attention
is All You Need</a>的encoder和BERT的话，最重点的区别在于位置编码。</p>
<p>原生的transformer是最经典的Sinusoidal绝对位置编码。</p>
<p>而BERT中变成了可以学习的参数，也就是可学习位置编码。</p>
<h2 id="四种norm">四种Norm</h2>
<ul>
<li><p>Batch
Norm：把每个Batch中，每句话的相同位置的字向量看成一组做归一化。</p></li>
<li><p>Layer Norm：在每一个句子中进行归一化。</p></li>
<li><p>Instance Norm：每一个字的字向量的看成一组做归一化。</p></li>
<li><p>Group Norm：把每句话的每几个字的字向量看成一组做归一化。</p></li>
<li><p><strong>Batch Normalization（Batch Norm）</strong>：
<strong>缺点</strong>：在处理序列数据（如文本）时，Batch
Norm可能不会表现得很好，因为序列数据通常长度不一，并且一次训练的Batch中的句子的长度可能会有很大的差异；此外，Batch
Norm对于Batch大小也非常敏感。对于较小的Batch大小，Batch
Norm可能会表现得不好，因为每个Batch的统计特性可能会有较大的波动。</p></li>
<li><p><strong>Layer Normalization（Layer Norm）</strong>：
<strong>优点</strong>：Layer
Norm是对每个样本进行归一化，因此它对Batch大小不敏感，这使得它在处理序列数据时表现得更好；另外，Layer
Norm在处理不同长度的序列时也更为灵活。</p></li>
<li><p><strong>Instance Normalization（Instance Norm）</strong>：
<strong>优点</strong>：Instance
Norm是对每个样本的每个特征进行归一化，因此它可以捕捉到更多的细节信息。Instance
Norm在某些任务，如风格迁移，中表现得很好，因为在这些任务中，细节信息很重要。
<strong>缺点</strong>：Instance
Norm可能会过度强调细节信息，忽视了更宏观的信息。此外，Instance
Norm的计算成本相比Batch Norm和Layer Norm更高。</p></li>
<li><p><strong>Group Normalization（Group Norm）</strong>：
<strong>优点</strong>：Group Norm是Batch Norm和Instance
Norm的折中方案，它在Batch的一个子集（即组）上进行归一化。这使得Group
Norm既可以捕捉到Batch的统计特性，又可以捕捉到样本的细节信息。此外，Group
Norm对Batch大小也不敏感。 <strong>缺点</strong>：Group
Norm的性能取决于组的大小，需要通过实验来确定最优的组大小。此外，Group
Norm的计算成本也比Batch Norm和Layer Norm更高。</p></li>
</ul>
<h1 id="大模型算法">大模型算法</h1>
<h2 id="并行计算">并行计算</h2>
<p>并行化指的是拆分任务并将它们分布到多个处理器或设备(如gpu)上，以便它们可以同时完成。</p>
<h3 id="data-parallenlism">Data Parallenlism</h3>
<p>数据并行性将训练数据分成多个分片(partition)，并将其分布到不同的节点上。
每个节点首先用自己的局部数据训练自己的子模型，然后与其他节点通信，以一定的间隔将结果组合在一起，从而得到全局模型。
最大的缺点是，在向后传递期间，您必须将整个梯度传递给所有其他gpu。它还在所有工作线程中复制模型和优化器，这是相当低内存效率的。</p>
<h3 id="tensor-parallelism">Tensor Parallelism</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20240411135309145.png"
alt="image-20240411135309145" />
<figcaption aria-hidden="true">image-20240411135309145</figcaption>
</figure>
<p>张量并行将大型矩阵乘法划分为较小的子矩阵计算，然后使用多个gpu同时执行。</p>
<p>然而，缺点是它在每次前向和后向传播中引入了额外的激活通信，因此需要高通信带宽才能高效</p>
<h3 id="pipeline-parallelism-and-model-parallelism">Pipeline parallelism
and model parallelism</h3>
<p><img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20240411135707760.png"
alt="image-20240411135707760" />
管道并行通过将模型的层划分为可以并行处理的阶段来提高深度学习训练的内存和计算效率。这有助于显著提高总体吞吐量速度，同时增加最小的通信开销。你可以把管道并行看作是“层间并行”(张量并行可以看作是“层内并行”)。</p>
<p>与管道并行类似，模型并行是指在gpu之间分割模型并为每个模型使用相同的数据;所以每个GPU只处理模型的一部分，而不是数据的一部分。管道和模型并行的缺点是它不能无限扩展，因为管道并行的程度受模型深度的限制。</p>
<h3 id="ptd-p">PTD-P</h3>
<p>combines pipeline, tensor, and data parallelism</p>
<h3 id="gradient-accumulation">Gradient accumulation</h3>
<p>在一次对所有累积的梯度执行一个权重更新步骤之前，将多个批次的梯度相加。</p>
<p>这种方法减少了之间的通信开销,gpu通过允许它们独立地处理自己的本地批数据，直到它们再次相互同步，在为单个优化步骤积累足够的梯度之后。</p>
<h3
id="asynchronous-stochastic-gradient-descent-optimization">Asynchronous
stochastic gradient descent optimization</h3>
<ul>
<li>从参数服务器获取处理当前小批量所需的模型的最新参数。</li>
<li>我们根据这些参数计算损失的梯度.</li>
<li>这些梯度被发送回参数服务器，然后参数服务器相应地更新模型。</li>
</ul>
<h3 id="micro-batching">Micro-batching</h3>
<p>将 small
mini-batches合并成大批，这样在反向传播操作期间，可以在更短的时间内处理更多的批，并且在设备之间使用更少的同步点。</p>
<h2 id="数据集预处理">数据集预处理</h2>
<ul>
<li>Data Sampling</li>
</ul>
<p>某些数据组件可以被上采样以获得更平衡的数据分布。一些研究对低质量的数据集进行了降采样，如未过滤的网络爬虫数据。其他研究根据模型目标对特定领域的数据进行采样。</p>
<p>还有一些先进的方法可以过滤高质量的数据，例如将训练好的分类器模型应用到数据集上。例如，Meta
AI的模型Galactica是专门为科学而建立的，特别是存储、组合和推理科学知识。</p>
<ul>
<li>Data cleaning</li>
</ul>
<p>删除样板文本和HTML代码或标记，修复拼写错误，处理跨域单应词，删除有偏见、有害的言论。</p>
<p>有的项目，这些技术并没有被使用，因为模型应该看到真实世界的公平表示，并学习处理拼写错误和毒性作为模型能力的一部分。</p>
<ul>
<li>Non-standard textual components handling</li>
</ul>
<p>非标准文本组件转换为文本，例如，将表情符号雪花转换为对应的文本“snowflake”。</p>
<ul>
<li>Data deduplication</li>
</ul>
<p>擅长重复数据</p>
<ul>
<li>Downstream task data removal</li>
</ul>
<p>在训练集中删除测试集中也存在的数据</p>
<h2 id="tokenization">Tokenization</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20240411152753420.png"
alt="image-20240411152753420" />
<figcaption aria-hidden="true">image-20240411152753420</figcaption>
</figure>
<p>将文本字符串编码为transformer可读的token ID整数。</p>
<p>Most state-of-the-art LLMs use subword-based tokenizers like
byte-pair encoding (BPE) as opposed to word-based approaches.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20240411153647039.png"
alt="image-20240411153647039" />
<figcaption aria-hidden="true">image-20240411153647039</figcaption>
</figure></li>
</ul>
<p>subword-based methods</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20240411154107909.png"
alt="image-20240411154107909" />
<figcaption aria-hidden="true">image-20240411154107909</figcaption>
</figure>
<h2 id="pre-train">Pre-Train</h2>
<h2 id="supervised-fine-tuning-sft"><strong>Supervised fine-tuning
(SFT)</strong></h2>
<p><a
href="https://blog.csdn.net/sinat_39620217/article/details/131751780">人工智能大语言模型微调技术：SFT
监督微调、LoRA 微调方法、P-tuning v2 微调方法、Freeze 监督微调方法_lora
ptuningv2-CSDN博客</a></p>
<p>lora：</p>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20240411183752670.png"
alt="image-20240411183752670" />
<figcaption aria-hidden="true">image-20240411183752670</figcaption>
</figure>
<p><a
href="https://www.zhihu.com/tardis/zm/art/623543497?source_id=1005">LORA：大模型轻量级微调
(zhihu.com)</a></p>
<p><a
href="https://blog.csdn.net/sinat_39620217/article/details/131751780">人工智能大语言模型微调技术：SFT
监督微调、LoRA 微调方法、P-tuning v2 微调方法、Freeze 监督微调方法_lora
ptuningv2-CSDN博客</a></p>
]]></content>
      <categories>
        <category>ML</category>
        <category>LLM</category>
      </categories>
  </entry>
  <entry>
    <title>LightGCN</title>
    <url>/2023/02/24/LightGCN/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>question: why concentrated to sum</p>
<h2 id="main-contributes">Main contributes</h2>
<ol type="1">
<li><p>We empirically show that two common designs in GCN, feature
transformation and nonlinear activation, have no positive effect on the
effectiveness of collaborative filtering.</p>
<p>GCN is originally proposed for node classification on the attributed
graph, where each node has rich attributes as input features; whereas in
the user-item interaction graph for CF, each node (user or item) is only
described by a one-hot ID, which has no concrete semantics besides being
an identifier.</p></li>
<li><p>Propose LightGCN.</p></li>
</ol>
<h1 id="analyze-about-ngcf">Analyze about NGCF</h1>
<h2 id="brief">Brief</h2>
<p>完全想不起来的话建议先看NGCF的笔记</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213192715250.png"
alt="image-20230213192715250" />
<figcaption aria-hidden="true">image-20230213192715250</figcaption>
</figure>
<p>## Some experiment</p>
<h3 id="method">Method</h3>
<p>Using ablation studies, implement three simplified variants of
NGCF:</p>
<ol type="1">
<li>NGCF-f: which removes the feature transformation matrices <span
class="math inline">\(W1\)</span> and <span
class="math inline">\(W2\)</span>.</li>
<li>NGCF-n: which removes the non-linear activation function $ σ$.</li>
<li>NGCF-fn: which removes both the feature transformation matrices and
non-linear activation function.</li>
</ol>
<p><strong>Note</strong>: Since the core of GCN is to refine embeddings
by propagation, we are more interested in the embedding quality under
the same embedding size. Thus, we change the way of obtaining final
embedding from concatenation (i.e., <span
class="math inline">\(e_u^*=e_u^{(0)}\|e_u^{(1)}\|...\|e_u^{(L)}\)</span>)
to sum(i.e., <span
class="math inline">\(e_u^*=e_u^{(0)}+e_u^{(1)}+...+e_u^{(L)}\)</span>).</p>
<p>This change has little effect on NGCF’s performance but makes the
following ablation studies more indicative of the embedding quality
refined by GCN.</p>
<h3 id="result">Result</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213193937873.png"
alt="image-20230213193937873" />
<figcaption aria-hidden="true">image-20230213193937873</figcaption>
</figure>
<ol type="1">
<li>Adding feature transformation imposes negative effect on NGCF, since
removing it in both models of NGCF and NGCF-n improves the performance
significantly;</li>
<li>Adding nonlinear activation affects slightly when feature
transformation is included, but it imposes negative effect when feature
transformation is disabled.</li>
<li>As a whole, feature transformation and nonlinear activation impose
rather negative effect on NGCF, since by removing them simultaneously,
NGCF-fn demonstrates large improvements over NGCF.</li>
</ol>
<h3 id="conclusion">Conclusion</h3>
<p>The deterioration of NGCF stems from the training
difficulty(underfitting), rather than overfitting, because:</p>
<ol type="1">
<li><p>Such lower training loss of NGCF-fn successfully transfers to
better recommendation accuracy.</p></li>
<li><p>NGCF is more powerful and complex, but it demonstrates higher
training loss and worse generalization performance than NGCF-f.</p></li>
</ol>
<h1 id="model-of-lightgcn">Model of LightGCN</h1>
<p>Consisting four parts:</p>
<ol type="1">
<li>initialize users and items embedding.</li>
<li>Light Graph Convolution (LGC)</li>
<li>Layer Combination</li>
<li>Model Prediction</li>
</ol>
<h2 id="light-graph-convolution-lgc">Light Graph Convolution (LGC)</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213194939594.png"
alt="image-20230213194939594" />
<figcaption aria-hidden="true">image-20230213194939594</figcaption>
</figure>
<p>$ $： symmetric normalization, which can avoid the scale of
embeddings increasing with graph convolution operations. Here can use
other normalization, but symmetric normalization has good
performance.</p>
<p><strong>Note</strong>: Without self-connection, because the layer
combination operation of LightGCN captures the same effect as
self-connections.</p>
<h2 id="layer-combination">Layer Combination</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213195607414.png"
alt="image-20230213195607414" />
<figcaption aria-hidden="true">image-20230213195607414</figcaption>
</figure>
<p><span class="math inline">\(α_k\)</span>can be treated as a
hyperparameter to be tuned manually, or as a model parameter, and
setting <span class="math inline">\(α_k\)</span> uniformly as <span
class="math inline">\(1/(K + 1)\)</span> leads to good performance in
general.</p>
<p>This is probably because the training data does not contain
sufficient signal to learn good α that can generalize to unknown
data.</p>
<p>The reason of using the Layer Combination:</p>
<ol type="1">
<li>With the increasing of the number of layers, the embeddings will be
over-smoothed [27]. Thus simply using the last layer is
problematic.</li>
<li>The embeddings at different layers capture different semantics.</li>
<li>Combining embeddings at different layers with weighted sum captures
the effect of graph convolution with self-connections, an important
trick in GCNs.</li>
</ol>
<h2 id="model-prediction">Model Prediction</h2>
<p>inner product</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213200915280.png"
alt="image-20230213200915280" />
<figcaption aria-hidden="true">image-20230213200915280</figcaption>
</figure>
<h2 id="matrix-form">Matrix form</h2>
<p>Similar to NGCF, and there are some explanations in detail in NGCF
note.</p>
<p>Light Graph Convolution:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201201896.png"
alt="image-20230213201201896" />
<figcaption aria-hidden="true">image-20230213201201896</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201107785.png"
alt="image-20230213201107785" />
<figcaption aria-hidden="true">image-20230213201107785</figcaption>
</figure>
<p>Layer combination:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201221159.png"
alt="image-20230213201221159" />
<figcaption aria-hidden="true">image-20230213201221159</figcaption>
</figure>
<h1 id="analyze-about-lightgcn">Analyze about LightGCN</h1>
<h2 id="relation-with-sgcn">Relation with SGCN</h2>
<p><strong>Purpose</strong>: by doing layer combination, LightGCN
subsumes the effect of self-connection thus there is no need for
LightGCN to add self-connection in adjacency matrix.</p>
<p>SGCN: a recent linear GCN model that integrates self-connection into
graph convolution.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201725744.png"
alt="image-20230213201725744" />
<figcaption aria-hidden="true">image-20230213201725744</figcaption>
</figure>
<p>In the following analysis, we omit the <span class="math inline">\((D
+ I)^{-\frac{1}{2}}\)</span> terms for simplicity, since they only
re-scale embeddings.</p>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230213212117430.png"
alt="image-20230213212117430" />
<figcaption aria-hidden="true">image-20230213212117430</figcaption>
</figure>
<p>The above derivation shows that, inserting self-connection into A and
propagating embeddings on it, is essentially equivalent to a weighted
sum of the embeddings propagated at each LGC layer.</p>
<p>because <span class="math inline">\(AE^{(0)}=E^{(1)}\)</span>...<span
class="math inline">\(A^KE^{(0)}=E^{(K)}\)</span></p>
<h2 id="relation-with-appnp">Relation with APPNP</h2>
<p><strong>Purpose</strong>: shows the underlying equivalence between
LightGCN and APPNP, thus our LightGCN enjoys the sames benefits in
propagating long-range with controllable overs-moothing.</p>
<p>APPNP: a recent GCN variant that addresses over-smoothing. APPNP
complements each propagation layer with the starting features.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213212642134.png"
alt="image-20230213212642134" />
<figcaption aria-hidden="true">image-20230213212642134</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213212845456.png"
alt="image-20230213212845456" />
<figcaption aria-hidden="true">image-20230213212845456</figcaption>
</figure>
<p>also equivalent to a weighted sum of the embeddings propagated at
each LGC layer.</p>
<h2 id="second-order-embedding-smoothness">Second-Order Embedding
Smoothness</h2>
<p><strong>Purpose</strong>: providing more insights into the working
mechanism of LightGCN.</p>
<p>below is influence from2-order neighbor to target node.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213213500081.png"
alt="image-20230213213500081" />
<figcaption aria-hidden="true">image-20230213213500081</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213213521206.png"
alt="image-20230213213521206" />
<figcaption aria-hidden="true">image-20230213213521206</figcaption>
</figure>
<p><strong>conclusion</strong>: the influence of a second-order neighbor
v on u is determined by</p>
<ol type="1">
<li>the number of co-interacted items, the more the larger.</li>
<li>the popularity of the co-interacted items, the less popularity
(i.e., more indicative of user personalized preference) the larger</li>
<li>the activity of v, the less active the larger.</li>
</ol>
<h1 id="model-train">Model Train</h1>
<h2 id="loss-function">Loss function</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213213949666.png"
alt="image-20230213213949666" />
<figcaption aria-hidden="true">image-20230213213949666</figcaption>
</figure>
<h2 id="optimizer-adam">Optimizer: Adam</h2>
<h2 id="no-dropout-strategy">No dropout strategy</h2>
<p>The reason is that we do not have feature transformation weight
matrices in LightGCN, thus enforcing L2 regularization on the embedding
layer is sufficient to prevent overfitting.</p>
<h1 id="experiment">Experiment</h1>
<h2 id="compared-with-ngcf">compared with NGCF</h2>
<ol type="1">
<li>LightGCN performs better than NGCF and NGCF-fn, as NGCF-fn still
contains more useless operations than LightGCN.</li>
<li>Increasing the number of layers can improve performance, but the
benefits diminish. Increasing the layer number from 0 to 1 leads to the
largest performance gain, and using a layer number of 3 leads to
satisfactory performance in most cases.</li>
<li>LightGCN consistently obtains lower training loss, which indicates
that LightGCN fits the training data better than NGCF. Moreover, the
lower training loss successfully transfers to better testing accuracy,
indicating the strong generalization power of LightGCN. In contrast, the
higher training loss and lower testing accuracy of NGCF reflect the
practical difficulty to train such a heavy model it well.</li>
</ol>
<h2 id="ablation-and-effectiveness-analyses">Ablation and Effectiveness
Analyses</h2>
<h3 id="impact-of-layer-combination">Impact of Layer Combination</h3>
<h4 id="using-models">Using models:</h4>
<ol type="1">
<li>LightGCN</li>
<li>LightGCN-single: does not use layer combination</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215151920378.png"
alt="image-20230215151920378" />
<figcaption aria-hidden="true">image-20230215151920378</figcaption>
</figure>
<h4 id="conclusion-1">Conclusion</h4>
<ol type="1">
<li>Focusing on LightGCN-single, we find that its performance first
improves and then drops when the layer number increases from 1 to 4.
This indicates that smoothing a node’s embedding with its first-order
and secondorder neighbors is very useful for CF, but will suffer from
oversmoothing issues when higher-order neighbors are used.</li>
<li>Focusing on LightGCN, we find that its performance gradually
improves with the increasing of layers even using 4 layers. This
justifies the effectiveness of layer combination for addressing
over-smoothing.</li>
<li>we find that LightGCN consistently outperforms LightGCN-single on
Gowalla, but not on AmazonBook and Yelp2018. There are two reason:
<ol type="1">
<li>LightGCN-single is special case of LightGCN that sets αK to 1 and
other αk to 0;</li>
<li>we do not tune the <span class="math inline">\(αk\)</span> and
simply set it as <span class="math inline">\(\frac{1}{K+1}\)</span>
uniformly for LightGCN.</li>
</ol></li>
</ol>
<h3 id="impact-of-symmetric-sqrt-normalization">Impact of Symmetric Sqrt
Normalization</h3>
<h4 id="setting">Setting:</h4>
<ol type="1">
<li>LightGCN-L: normalization only at the left side (i.e., the target
node’s coefficient).</li>
<li>LightGCN-R: the right side (i.e., the neighbor node’s
coefficient).</li>
<li>LightGCN-L1: use L1 normalization( i.e., removing the square
root).</li>
<li>LightGCN-L1-L: use L1 normalization only on the left side.</li>
<li>LightGCN-L1-R: use L1 normalization only on the right side.</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215153734701.png"
alt="image-20230215153734701" />
<figcaption aria-hidden="true">image-20230215153734701</figcaption>
</figure>
<h4 id="conclusion-2">Conclusion</h4>
<ol type="1">
<li>The best setting in general is using sqrt normalization at both
sides (i.e., the current design of LightGCN). Removing either side will
drop the performance largely.</li>
<li>The second best setting is using L1 normalization at the left side
only (i.e., LightGCN-L1-L). This is equivalent to normalize the
adjacency matrix as a stochastic matrix by the
in-degree(norm后矩阵无对称性).</li>
<li>Normalizing symmetrically on two sides is helpful for the sqrt
normalization, but will degrade the performance of L1
normalization.</li>
</ol>
<h3 id="analysis-of-embedding-smoothness">Analysis of Embedding
Smoothness</h3>
<p><strong>Object</strong>: Making sure such
smoothing（有点像聚类的感觉） of embeddings is the key reason of
LightGCN’s effectiveness.</p>
<p><strong>Method</strong>: we first define the smoothness of user
embeddings as(用于衡量2-order
neighbor的embedding差别大小，是否合理聚类的感觉):</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215160123446.png"
alt="image-20230215160123446" />
<figcaption aria-hidden="true">image-20230215160123446</figcaption>
</figure>
<p>where the L2 norm on embeddings is used to eliminate the impact of
the embedding’s scale.</p>
<p><strong>result</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215160328103.png"
alt="image-20230215160328103" />
<figcaption aria-hidden="true">image-20230215160328103</figcaption>
</figure>
<p><strong>Conclusion</strong>: the smoothness loss of LightGCN-single
is much lower than that of MF.</p>
<p>This indicates that by conducting light graph convolution, the
embeddings become smoother and more suitable for recommendation.</p>
<h2 id="hyper-parameter-studies">Hyper-parameter Studies</h2>
<p><strong>object</strong>: Ensure the L2 regularization coefficient
<span class="math inline">\(λ\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215161009450.png"
alt="image-20230215161009450" />
<figcaption aria-hidden="true">image-20230215161009450</figcaption>
</figure>
<p><strong>Conclusion</strong>:</p>
<ol type="1">
<li>LightGCN is relatively insensitive to λ.</li>
<li>Even when λ sets to 0, LightGCN is better than NGCF, which
additionally uses dropout to prevent overfitting. This shows that
LightGCN is less prone to overfitting</li>
<li>When λ is larger than 1e−3, the performance drops quickly, which
indicates that too strong regularization will negatively affect model
normal training and is not encouraged.</li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title>Math in DNN</title>
    <url>/2023/06/24/MATHinDNN/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="相似度">相似度</h1>
<h2 id="余弦相似度">余弦相似度</h2>
<h2 id="皮尔逊相关系数">皮尔逊相关系数</h2>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
  </entry>
  <entry>
    <title>NGCF</title>
    <url>/2023/02/24/NGCF/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>question: example(the Laplacian)</p>
<h2 id="some-definition">Some Definition</h2>
<ol type="1">
<li><p>Recommendation system: Estimate how likely a user will adopt an
item based on the historical interaction like purchase and
click.</p></li>
<li><p>Collaborative filtering(CF): behaviorally similar users would
exhibit similar preference on items.</p>
<p>CF consists of</p>
<ol type="1">
<li><p>embedding: transforms users and items into vectorized
representations. e.g. matrix factorization(MF),deep learning
function...</p></li>
<li><p>interaction modeling: reconstructs historical interactions based
on the embeddings. e.g. inner product, neural function...</p></li>
</ol></li>
<li><p>collaborative signal: signal latent in user-item
interactions</p></li>
</ol>
<h2 id="existing-problem">Existing Problem</h2>
<p>The current embedding process of CF doesn't encode a collaborative
signal. Most of them focus on the descriptive feature(e.g. user id,
attributes). When the embeddings are insufficient in capturing CF, the
methods have to rely on the interaction function to make up for the
deficiency of suboptimal embeddings</p>
<h2 id="main-contribute">Main contribute</h2>
<ol type="1">
<li><p>Highlight the critical importance of explicitly exploiting the
collaborative signal in the embedding function of model-based CF
methods.</p></li>
<li><p>Propose NGCF, a new recommendation framework based on a graph
neural network, which explicitly encodes the collaborative signal in the
form of high-order connectivities by performing embedding
propagation.</p></li>
</ol>
<h1 id="model">Model</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211111222966.png"
alt="image-20230211111222966" />
<figcaption aria-hidden="true">image-20230211111222966</figcaption>
</figure>
<p>There are three components in the framework:</p>
<ol type="1">
<li>Embedding layer: offers and initialization of user embeddings and
item embeddings;</li>
<li>Multiple embedding propagation layers: refine the embeddings by
injecting high-order connectivity relations;</li>
<li>Prediction layer: aggregates the refined embeddings from different
propagation layers and outputs the affinity score of a user-item
pair.</li>
</ol>
<h2 id="embedding-layer">Embedding layer</h2>
<p>Just initializing user embeddings and item embeddings by using ID or
other features.</p>
<p>Get user embedding <span class="math inline">\(e_i\)</span> and item
embedding <span class="math inline">\(e_u\)</span>.</p>
<h2 id="multiple-embedding-propagation-layers">Multiple Embedding
Propagation Layers</h2>
<h3 id="one-layer-propagation">One layer propagation</h3>
<p>It consists of two parts: Message Construction and Message
aggregation.</p>
<h4 id="message-construction">Message Construction</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211112521161.png"
alt="image-20230211112521161" />
<figcaption aria-hidden="true">image-20230211112521161</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211111736136.png"
alt="image-20230211111736136" />
<figcaption aria-hidden="true">image-20230211111736136</figcaption>
</figure>
<p><span class="math inline">\(m_{u&lt;-i}\)</span>: the result of the
message construction module. It is a message embedding that will be used
to update the target node.</p>
<p><span class="math inline">\(e_i\)</span>: Embedding of neighbor
item.</p>
<p><strong>meaning</strong> : encode neighbor item's feature.</p>
<p><span class="math inline">\(e_i⊙e_u\)</span> : element-wise product
of <span class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_u\)</span>.</p>
<p><strong>meaning</strong>: encodes the interaction between <span
class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_u\)</span> into the message and makes the
message dependent on the affinity between <span
class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_j\)</span>.</p>
<p><span class="math inline">\(W_1\)</span>, <span
class="math inline">\(W_2\)</span>: trainable weight matrices， the
shape is (<span class="math inline">\(d&#39;\)</span>, <span
class="math inline">\(d\)</span>), while <span
class="math inline">\(d\)</span> is the size of the initial embedding,
<span class="math inline">\(d&#39;\)</span> is the size of
transformation size.</p>
<p><span class="math inline">\(P_{ui}\)</span>: to control the decay
factor on each propagation on edge (u, i). Here, we set <span
class="math inline">\(P_{ui}\)</span> as <strong>Laplacian norm</strong>
$ $, $ N_u$, $ N_i$ is the first-hot neighbors of user u and item i.
(就是拉普拉斯矩阵归一化！！<span
class="math inline">\(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\)</span>)</p>
<p><strong>meaning</strong> -From the viewpoint of representation
learning: <span class="math inline">\(P_{ui}\)</span> reflects how much
the historical item contributes to the user preference.</p>
<p>From the viewpoint of the message passing: <span
class="math inline">\(P_{ui}\)</span> can be interpreted as a discount
factor, considering the messages being propagated should decay with the
path length.</p>
<h4 id="message-aggregation">Message Aggregation</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211151741633.png"
alt="image-20230211151741633" />
<figcaption aria-hidden="true">image-20230211151741633</figcaption>
</figure>
<p><span class="math inline">\(e_u^{(1)}\)</span>: the representation of
user u after 1 propagation layer.</p>
<p><span class="math inline">\(m_{u&lt;-u}\)</span>: self-connection of
u. Here is <span class="math inline">\(W1e_u\)</span>.</p>
<p><strong>meaning</strong>: retain information of original feature.</p>
<p><span class="math inline">\(m_{u&lt;-i}\)</span>： neighbor node
propagation.</p>
<h3 id="high-order-propagation">High-order propagation</h3>
<h4 id="formulate-form">Formulate Form</h4>
<p>By stacking l-embedding propagation layers, a user (and an item) is
capable of receiving the messages propagated from its l-hop neighbors.
The formulates are similar to one-layer propagation.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212105956664.png"
alt="image-20230212105956664" />
<figcaption aria-hidden="true">image-20230212105956664</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212110019741.png"
alt="image-20230212110019741" />
<figcaption aria-hidden="true">image-20230212110019741</figcaption>
</figure>
<h4 id="matrix-form">Matrix Form</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212110725475.png"
alt="image-20230212110725475" />
<figcaption aria-hidden="true">image-20230212110725475</figcaption>
</figure>
<p><span class="math inline">\(E^{(l)}\)</span> : the representations
for users and items obtained after l-layers propagation. Shape is
(N+M,d)</p>
<p>L: Laplacian matrix for the user-item graph.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212111719667.png"
alt="image-20230212111719667" />
<figcaption aria-hidden="true">image-20230212111719667</figcaption>
</figure>
<p>D is the diagonal degree matrix. where <span
class="math inline">\(D_{tt}=\vert N_t\vert\)</span> meaning the
<code>D[t][t]</code> is the number of neighbors' node. The shape is
(N+M, N+M), because there are totally n+m node(including user and
item)</p>
<p>A is the adjacency matrix. The shape of R is (N, M), while the shape
of A is (N+M, N+M).</p>
<p>some extra knowledge: <a
href="https://zhuanlan.zhihu.com/p/362416124/">理解拉普拉斯矩阵</a></p>
<p>I: identity matrix</p>
<h5 id="a-simple-example-for-matrix-form">A simple example for matrix
form:</h5>
<p>Suppose we have 2 users (A, B), 3 items(C, D, E), N=2 and M=3.</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/D9B00E7DDF74FF18B83E42668335328A.png" alt="D9B00E7DDF74FF18B83E42668335328A" style="zoom: 25%;" /></p>
<p>Let consider this part: <span
class="math inline">\((L+I)E^{(l-1)}W^{(l)}\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/81DBE0096BF060771E3355F2E6A34151.png"
alt="81DBE0096BF060771E3355F2E6A34151" />
<figcaption
aria-hidden="true">81DBE0096BF060771E3355F2E6A34151</figcaption>
</figure>
<p>After calculating <span
class="math inline">\((L+I)E^{(l-1)}\)</span>, we get information on
self-connection and neighbor-propagation (after the Laplacian norm), and
then we can multiply the trainable parameter W1(MLP).</p>
<p>拉普拉斯矩阵归一化的不成熟小理解：</p>
<p>①target node由n个邻居点做贡献，为了避免邻居越多，target
node的value越大的情况，首先除<span
class="math inline">\(\frac{1}{\sqrt{N_n}}\)</span>,
大概也可以理解为邻居越多，每个邻居对其造成的影响越小</p>
<p>②只做一次norm影响对称性，所以为了保持对称性在做一次<span
class="math inline">\(\frac{1}{\sqrt{N_t}}\)</span>,可以理解为neighbor
node有多少邻居对他给到每个邻居的权重有影响，是否能理解为邻居越多说明这个node能提供的信息更普通没价值（例如所有用户购买了水，对推荐系统来说，水能提供的信息就没那么有用）</p>
<p>xxxxxxxxxx class UV_Aggregator(nn.Module):    """   item and user
aggregator: for aggregating embeddings of neighbors (item/user
aggreagator).   """​    def <strong>init</strong>(self, v2e, r2e, u2e,
embed_dim, cuda="cpu", uv=True):        ...​    def forward(self, nodes,
history_uv, history_r):        # create a container for result, shpe of
embed_matrix is (batchsize,embed_dim)        embed_matrix =
torch.empty(len(history_uv), self.embed_dim,
dtype=torch.float).to(self.device)​        # deal with each single item
nodes' neighbors        for i in range(len(history_uv)):          
 history = history_uv[i]            num_histroy_item = len(history)    
       tmp_label = history_r[i]​            # e_uv : turn neighbors(user
node) id to embedding            # uv_rep : turn single node(item node)
to embedding            if self.uv == True:                # user
component                e_uv = self.v2e.weight[history]              
 uv_rep = self.u2e.weight[nodes[i]]            else:                #
item component                e_uv = self.u2e.weight[history]          
     uv_rep = self.v2e.weight[nodes[i]]​            # get rating score
embedding            e_r = self.r2e.weight[tmp_label]            #
concatenated rating and neighbor, and than through two layers mlp to get
fjt            x = torch.cat((e_uv, e_r), 1)            x =
F.relu(self.w_r1(x))​            o_history = F.relu(self.w_r2(x))        
   # calculate neighbor attention and fjt*weight to finish aggregation  
         att_w = self.att(o_history, uv_rep, num_histroy_item)          
 att_history = torch.mm(o_history.t(), att_w)            att_history =
att_history.t()​            embed_matrix[i] = att_history        # result
(batchsize, embed_dim)        to_feats = embed_matrix        return
to_featspython</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/8EE43A6D961CA0F0145CD44C62B9F9BE.png"
alt="8EE43A6D961CA0F0145CD44C62B9F9BE" />
<figcaption
aria-hidden="true">8EE43A6D961CA0F0145CD44C62B9F9BE</figcaption>
</figure>
<p>We get information on the interaction between <span
class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_u\)</span> (after the Laplacian norm), and then
we can multiply the trainable parameter W2(MLP).</p>
<p>Add two parts and through LeakyRelu, we get user or item embedding
after l-layers propagation.</p>
<h2 id="model-prediction">Model Prediction</h2>
<p>Just concatenate all propagation layers' output embedding, and use
inner product to estimate the user's preference towards the target
item.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212173756733.png"
alt="image-20230212173756733" />
<figcaption aria-hidden="true">image-20230212173756733</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212173813003.png"
alt="image-20230212173813003" />
<figcaption aria-hidden="true">image-20230212173813003</figcaption>
</figure>
<h1 id="optimization">Optimization</h1>
<h2 id="loss">Loss</h2>
<p>BPR Loss: assumes that the observed interactions, which are more
reflective of a user’s preferences, should be assigned higher prediction
values than unobserved ones.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212212248890.png"
alt="image-20230212212248890" />
<figcaption aria-hidden="true">image-20230212212248890</figcaption>
</figure>
<h2 id="optimizer-adam">Optimizer: Adam</h2>
<h2 id="model-size">Model Size</h2>
<p>In NGCF, only W1 and W2 in the propagation layer need to be trained,
so has <span class="math inline">\(2Ld_ld_{l-1}\)</span> more
parameters, while L is always smaller than 5 and <span
class="math inline">\(d\)</span> is set as the embedding size(e.g. 64)
which is also small.</p>
<h2 id="message-and-node-dropout">Message and Node Dropout</h2>
<ol type="1">
<li><p><strong>Message dropout</strong>: randomly drops out the outgoing
messages (equal to dropout edge).</p>
<p><strong>meaning</strong>: endows the representations more robustness
against the presence or absence of single connections between users and
items.</p>
<p><strong>example</strong>: For the <span
class="math inline">\(l-th\)</span> propagation layer, we drop out the
messages being propagated, with a probability <span
class="math inline">\(p1\)</span>.</p></li>
<li><p><strong>Node dropout</strong>: randomly blocks a particular node
and discards all its outgoing messages.</p>
<p><strong>meaning</strong>: focuses on reducing the influences of
particular users or items.</p>
<p><strong>example</strong>: For the <span
class="math inline">\(l-th\)</span> propagation layer, we randomly drop
<span class="math inline">\((M + N)p2\)</span> nodes of the Laplacian
matrix, where <span class="math inline">\(p2\)</span> is the dropout
ratio.</p></li>
</ol>
<p>区别：对于message
dropout，计算时node的邻居数、拉普拉斯norm都是正常的，就是更新embedding的时候遗漏了信息，作用是提高一下鲁棒性和容错性；对于Node
dropout，直接在拉普拉斯矩阵中屏蔽若干个node，可能影响临界点数、归一化数值等，在矩阵运算时候就有影响，作用是希望模型不要过于依赖某些特定邻接点，没了部分点依然能正常运行。</p>
<h1 id="experiment">Experiment</h1>
<h2 id="conclusions-from-comparing-with-other-models">Conclusions from
comparing with other models</h2>
<ol type="1">
<li>The inner product is insufficient to capture the complex relations
between users and items.</li>
<li>Nonlinear feature interactions between users and items are
important</li>
<li>Neighbor information can improve embedding learning, and using the
attention mechanism is better than using equal and heuristic
weight.</li>
<li>Considering high-order connectivity or neighbor is better than only
considering first-order neighbor.</li>
<li>that exploiting high-order connectivity greatly facilitates
representation learning for inactive users, as the collaborative signal
can be effectively captured. And the embedding propagation is beneficial
to relatively inactive users.</li>
</ol>
<h2 id="study-for-ngcf">Study for NGCF</h2>
<p>....</p>
<h2 id="effect-of-high-order-connectivity">Effect of High-order
Connectivity</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212225247958.png"
alt="image-20230212225247958" />
<figcaption aria-hidden="true">image-20230212225247958</figcaption>
</figure>
<ol type="1">
<li>the representations of NGCF-3 exhibit discernible clustering,
meaning that the points with the same colors (<em>i.e.,</em> the items
consumed by the same users) tend to form the clusters.</li>
<li>when stacking three embedding propagation layers, the embeddings of
their historical items tend to be closer. It qualitatively verifies that
the proposed embedding propagation layer is capable of injecting the
explicit collaborative signal (via NGCF-3) into the
representations.</li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title>GraphRec</title>
    <url>/2023/02/24/Note_for_GraphRec/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="graphrec">GraphRec</h1>
<h1 id="graphrec-feature">GraphRec feature</h1>
<ol type="1">
<li><p>Can capture both interactions and opinions in user-item
graph.</p></li>
<li><p>Consider different strengths of social relations.</p></li>
<li><p>Use attention mechanism.</p></li>
</ol>
<h1 id="overall-architecture">Overall architecture</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/Snipaste_2023-02-02_15-10-34.png"
alt="Snipaste_2023-02-02_15-10-34" />
<figcaption aria-hidden="true">Snipaste_2023-02-02_15-10-34</figcaption>
</figure>
<h2 id="three-import-module">Three import module:</h2>
<ol type="1">
<li><p>User Modeling: used to compute User Latent Factor(vector
containing many useful information)</p></li>
<li><p>Item Modeling: used to compute Item Latent Factor.</p></li>
<li><p>Rating Prediction: used to predict the item which user would like
to interact with.</p></li>
</ol>
<h1 id="source-code-analyses">Source code analyses</h1>
<h2 id="data">Data</h2>
<h3 id="what-kind-of-datas-we-use"><strong>What kind of datas we
use?</strong></h3>
<ol type="1">
<li><p>User-Item graph: record interation(e.g. purchase) and
opinion(e.g. five star rating) between user and item</p></li>
<li><p>User-User social graph: relationship between user and
user</p></li>
</ol>
<h3 id="how-to-represent-these-datas-in-code"><strong>How to represent
these datas in code?</strong></h3>
<h4 id="user-item-graph">User-Item graph:</h4>
<ol type="1">
<li>history_u_lists, history_ur_lists: user's purchased history (item
set in training set), and his/her rating score (dict)</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history_u_list = &#123;</span><br><span class="line">    user_id1:[item_id1, item_id2, item_id3...],</span><br><span class="line">    user_id2:[item_id4...],</span><br><span class="line">&#125;</span><br><span class="line">history_ur_list = &#123;</span><br><span class="line">    user_id1:[rating_score_u1i1, rating_score_u1i2, rating_score_u1i3...],</span><br><span class="line">    user_id2:[rating_score_u2i4...],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">e.g.</span><br><span class="line">history_u_list = &#123;</span><br><span class="line">    <span class="number">681</span>: [<span class="number">0</span>, <span class="number">156</span>], </span><br><span class="line">    <span class="number">81</span>: [<span class="number">1</span>, <span class="number">41</span>, <span class="number">90</span>]&#125;</span><br><span class="line">history_ur_list = &#123;</span><br><span class="line">    <span class="number">681</span>: [<span class="number">5</span>,<span class="number">4</span>],</span><br><span class="line">    <span class="number">81</span>: [<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>]&#125;</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>history_v_lists, history_vr_lists: user set (in training set) who
have interacted with the item, and rating score (dict). Similar with
history_u_lists, history_ur_lists but key is item id and value is user
id.</li>
</ol>
<h4 id="user-user-socal-graph">User-User socal graph</h4>
<ol type="1">
<li>social_adj_lists: user's connected neighborhoods</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">social_adj_lists = &#123;</span><br><span class="line">    user_id1:[user_id2, user_id3, user_id4...],</span><br><span class="line">    user_id2:[user_id1...],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="other">other</h4>
<ol type="1">
<li>train_u, train_v, train_r: used for model training, one by one based
on index (user, item, rating)</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_u = [user_id1, user_id2,....]</span><br><span class="line">train_v = [item_id34, item_id1,...]</span><br><span class="line">train_r = [rating_socre_u1i34, rating_socre_u2i1]</span><br><span class="line"><span class="built_in">len</span>(train_u) = <span class="built_in">len</span>(train_v) = <span class="built_in">len</span>(train_r)</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li><p>test_u, test_v, test_r: similar with training datas</p></li>
<li><p>ratings_list: rating value from 0.5 to 4.0 (8 opinion embeddings)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;2.0: 0, 1.0: 1, 3.0: 2, 4.0: 3, 2.5: 4, 3.5: 5, 1.5: 6, 0.5: 7&#125;</span><br></pre></td></tr></table></figure></p></li>
</ol>
<h3 id="how-to-pre-process-data"><strong>How to pre-process
data?</strong></h3>
<p>use <code>torch.utils.data.TensorDataset</code> and
<code>torch.utils.data.DataLoader</code> generate
<code>training_dataset</code> and <code>testing_dataset</code> (user,
item, rating)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">support batchsize = <span class="number">5</span></span><br><span class="line">[tensor([<span class="number">409</span>,  <span class="number">88</span>, <span class="number">134</span>, <span class="number">298</span>, <span class="number">340</span>]),                             <span class="comment">#user id</span></span><br><span class="line">tensor([<span class="number">1221</span>,  <span class="number">761</span>,   <span class="number">39</span>,  <span class="number">145</span>,    <span class="number">0</span>]),                         <span class="comment">#item id</span></span><br><span class="line">tensor([<span class="number">1.0000</span>, <span class="number">2.0000</span>, <span class="number">3.5000</span>, <span class="number">0.5000</span>, <span class="number">1.5000</span>, <span class="number">3.5000</span>])        <span class="comment">#rating score</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h2 id="model">Model</h2>
<h3 id="init">Init</h3>
<p>Translate user_id, item_id and rating_id to low-dimension vector,
just random initize, the weight of embedding layers will be trained.</p>
<p>After translate we get</p>
<pre><code>qj-embedding of item vj, 
pi-embedding of user ui, 
er-embedding of rating.</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">u2e = nn.Embedding(num_users, embed_dim).to(device)</span><br><span class="line">v2e = nn.Embedding(num_items, embed_dim).to(device)</span><br><span class="line">r2e = nn.Embedding(num_ratings, embed_dim).to(device)</span><br><span class="line"><span class="built_in">print</span>(u2e, v2e, r2e)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;Output</span></span><br><span class="line"><span class="string">Embedding(705, 64) </span></span><br><span class="line"><span class="string">Embedding(1941, 64) </span></span><br><span class="line"><span class="string">Embedding(8, 64)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>So that, we can easily get embedding through U2e, V2e and r2e.</p>
<h3 id="overall-architecture-1">Overall architecture</h3>
<figure>
<img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/GraphRec.jpg"
alt="GraphRec" />
<figcaption aria-hidden="true">GraphRec</figcaption>
</figure>
<p>GraphRec consist of User Modeling, Item Modeling and Rating
Prediction. The forward code of GraphRec is as follow:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GraphRec</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_u, enc_v_history, r2e</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes_u, nodes_v</span>):</span><br><span class="line">        <span class="comment"># nodes_u : [128] 128(batchsize) user id</span></span><br><span class="line">        <span class="comment"># nodes_v : [128] 128(batchsize) item id</span></span><br><span class="line">        <span class="comment"># self.enc_u is the User Modeling part(including Item Aggregation and Social Aggregation )</span></span><br><span class="line">        <span class="comment"># self.enc_v_history is the Item Modeling part(User Aggregation)</span></span><br><span class="line">        embeds_u = self.enc_u(nodes_u)</span><br><span class="line">        embeds_v = self.enc_v_history(nodes_v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># After aggregation information, forward two layer MLP， and get the Latent vector of user and item</span></span><br><span class="line">        x_u = F.relu(self.bn1(self.w_ur1(embeds_u)))</span><br><span class="line">        x_u = F.dropout(x_u, training=self.training)</span><br><span class="line">        x_u = self.w_ur2(x_u)</span><br><span class="line">        x_v = F.relu(self.bn2(self.w_vr1(embeds_v)))</span><br><span class="line">        x_v = F.dropout(x_v, training=self.training)</span><br><span class="line">        x_v = self.w_vr2(x_v)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># concatenated user vector and item vector, use three layer MLP to predict</span></span><br><span class="line">        x_uv = torch.cat((x_u, x_v), <span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.bn3(self.w_uv1(x_uv)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = F.relu(self.bn4(self.w_uv2(x)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        scores = self.w_uv3(x)</span><br><span class="line">        <span class="keyword">return</span> scores.squeeze()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, nodes_u, nodes_v, labels_list</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>full code of GraphRec class</p>
<details>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GraphRec</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_u, enc_v_history, r2e</span>):</span><br><span class="line">        <span class="built_in">super</span>(GraphRec, self).__init__()</span><br><span class="line">        self.enc_u = enc_u</span><br><span class="line">        self.enc_v_history = enc_v_history</span><br><span class="line">        self.embed_dim = enc_u.embed_dim</span><br><span class="line"></span><br><span class="line">        self.w_ur1 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_ur2 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_vr1 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_vr2 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_uv1 = nn.Linear(self.embed_dim * <span class="number">2</span>, self.embed_dim)</span><br><span class="line">        self.w_uv2 = nn.Linear(self.embed_dim, <span class="number">16</span>)</span><br><span class="line">        self.w_uv3 = nn.Linear(<span class="number">16</span>, <span class="number">1</span>)</span><br><span class="line">        self.r2e = r2e</span><br><span class="line">        self.bn1 = nn.BatchNorm1d(self.embed_dim, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(self.embed_dim, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(self.embed_dim, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.bn4 = nn.BatchNorm1d(<span class="number">16</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes_u, nodes_v</span>):</span><br><span class="line">        embeds_u = self.enc_u(nodes_u)</span><br><span class="line">        embeds_v = self.enc_v_history(nodes_v)</span><br><span class="line"></span><br><span class="line">        x_u = F.relu(self.bn1(self.w_ur1(embeds_u)))</span><br><span class="line">        x_u = F.dropout(x_u, training=self.training)</span><br><span class="line">        x_u = self.w_ur2(x_u)</span><br><span class="line">        x_v = F.relu(self.bn2(self.w_vr1(embeds_v)))</span><br><span class="line">        x_v = F.dropout(x_v, training=self.training)</span><br><span class="line">        x_v = self.w_vr2(x_v)</span><br><span class="line"></span><br><span class="line">        x_uv = torch.cat((x_u, x_v), <span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.bn3(self.w_uv1(x_uv)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = F.relu(self.bn4(self.w_uv2(x)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        scores = self.w_uv3(x)</span><br><span class="line">        <span class="keyword">return</span> scores.squeeze()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, nodes_u, nodes_v, labels_list</span>):</span><br><span class="line">        scores = self.forward(nodes_u, nodes_v)</span><br><span class="line">        <span class="keyword">return</span> self.criterion(scores, labels_list)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</details>
<h3 id="user-modeling">User Modeling</h3>
<p>It contain Item Aggregation and Social Aggregation</p>
<p>在这里本质上是先做了一层Item
Aggregation之后，用得到的结果再做一层Social Aggregation 所以这里的Item
Aggregation，本质上是Social Aggregation中的self-connection</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Social_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, social_adj_lists, aggregator, base_model=<span class="literal">None</span>, cuda=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># to_neighs is a list which element is list recording social neighbor node, and len(list) is batchsize,</span></span><br><span class="line">        to_neighs = []</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            to_neighs.append(self.social_adj_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Social aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, to_neighs)  <span class="comment"># user-user network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Item aggregation</span></span><br><span class="line">        self_feats = self.features(torch.LongTensor(nodes.cpu().numpy())).to(self.device)</span><br><span class="line">        self_feats = self_feats.t()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        <span class="comment"># Concatenate Item Aggregation and Social Aggregation, and through one layer MLP</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<p>full code of User Modeling</p>
<details>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Social_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, social_adj_lists, aggregator, base_model=<span class="literal">None</span>, cuda=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Social_Encoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.features = features</span><br><span class="line">        self.social_adj_lists = social_adj_lists</span><br><span class="line">        self.aggregator = aggregator</span><br><span class="line">        <span class="keyword">if</span> base_model != <span class="literal">None</span>:</span><br><span class="line">            self.base_model = base_model</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.device = cuda</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">2</span> * self.embed_dim, self.embed_dim)  <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># to_neighs is a list which element is list recording social neighbor node, and len(list) is batchsize,</span></span><br><span class="line">        to_neighs = []</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            to_neighs.append(self.social_adj_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Item aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, to_neighs)  <span class="comment"># user-user network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Social aggregation</span></span><br><span class="line">        self_feats = self.features(torch.LongTensor(nodes.cpu().numpy())).to(self.device)</span><br><span class="line">        self_feats = self_feats.t()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        <span class="comment"># Concatenate Item Aggregation and Social Aggregation, and through one layer MLP</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
</details>
<h4 id="item-aggregation">Item Aggregation</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, history_uv_lists, history_r_lists, aggregator, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        tmp_history_uv = []</span><br><span class="line">        tmp_history_r = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">#get nodes(batch) neighbors</span></span><br><span class="line">        <span class="comment">#tmp_history_uv is a list which len is 128,while it&#x27;s element is also a list meaning that the each node&#x27;s(in batch) neighbor item id list</span></span><br><span class="line">        <span class="comment">#tmp_history_r is similar with tmp_history_uv, but record the rating score instead of item id</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            tmp_history_uv.append(self.history_uv_lists[<span class="built_in">int</span>(node)])</span><br><span class="line">            tmp_history_r.append(self.history_r_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># after neigh aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  <span class="comment"># user-item network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># id to embedding (features : u2e)</span></span><br><span class="line">        self_feats = self.features.weight[nodes]</span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<p>And the <code>self.aggregator</code> in neigh aggregation is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Aggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    item and user aggregator: for aggregating embeddings of neighbors (item/user aggreagator).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, v2e, r2e, u2e, embed_dim, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, history_uv, history_r</span>):</span><br><span class="line">        <span class="comment"># create a container for result, shpe of embed_matrix is (batchsize,embed_dim)</span></span><br><span class="line">        embed_matrix = torch.empty(<span class="built_in">len</span>(history_uv), self.embed_dim, dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># deal with each single nodes&#x27; neighbors</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(history_uv)):</span><br><span class="line">            history = history_uv[i]</span><br><span class="line">            num_histroy_item = <span class="built_in">len</span>(history)</span><br><span class="line">            tmp_label = history_r[i]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># e_uv : turn neighbors id to embedding</span></span><br><span class="line">            <span class="comment"># uv_rep : turn single node to embedding</span></span><br><span class="line">            <span class="keyword">if</span> self.uv == <span class="literal">True</span>:</span><br><span class="line">                <span class="comment"># user component</span></span><br><span class="line">                e_uv = self.v2e.weight[history]</span><br><span class="line">                uv_rep = self.u2e.weight[nodes[i]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># item component</span></span><br><span class="line">                e_uv = self.u2e.weight[history]</span><br><span class="line">                uv_rep = self.v2e.weight[nodes[i]]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># get rating score embedding</span></span><br><span class="line">            e_r = self.r2e.weight[tmp_label]</span><br><span class="line">            <span class="comment"># concatenated rating and neighbor, and than through two layers mlp to get xia</span></span><br><span class="line">            x = torch.cat((e_uv, e_r), <span class="number">1</span>)</span><br><span class="line">            x = F.relu(self.w_r1(x))</span><br><span class="line"></span><br><span class="line">            o_history = F.relu(self.w_r2(x))</span><br><span class="line">            <span class="comment"># calculate neighbor attention and xia*weight to finish aggregation</span></span><br><span class="line">            att_w = self.att(o_history, uv_rep, num_histroy_item)</span><br><span class="line">            att_history = torch.mm(o_history.t(), att_w)</span><br><span class="line">            att_history = att_history.t()</span><br><span class="line"></span><br><span class="line">            embed_matrix[i] = att_history</span><br><span class="line">        <span class="comment"># result (batchsize, embed_dim)</span></span><br><span class="line">        to_feats = embed_matrix</span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
<p>While <code>self.att</code> is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dims</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, node1, u_rep, num_neighs</span>):</span><br><span class="line">        <span class="comment"># pi</span></span><br><span class="line">        uv_reps = u_rep.repeat(num_neighs, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># concatenated neighbot and pi</span></span><br><span class="line">        x = torch.cat((node1, uv_reps), <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># through 3 layers MLP</span></span><br><span class="line">        x = F.relu(self.att1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = F.relu(self.att2(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.att3(x)</span><br><span class="line">        <span class="comment"># get weights</span></span><br><span class="line">        att = F.softmax(x, dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> att</span><br></pre></td></tr></table></figure>
<h4 id="social-aggregation">Social Aggregation</h4>
<p>use the result of Item Aggregation and pi as input</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Social_Aggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Social Aggregator: for aggregating embeddings of social neighbors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, u2e, embed_dim, cuda=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, to_neighs</span>):</span><br><span class="line">        <span class="comment">#return a uninitialize matrix as result container, which shape is (batchsize, embed_dim)</span></span><br><span class="line">        embed_matrix = torch.empty(<span class="built_in">len</span>(nodes), self.embed_dim, dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nodes)):</span><br><span class="line">            <span class="comment"># get social graph neighbor</span></span><br><span class="line">            tmp_adj = to_neighs[i]</span><br><span class="line">            num_neighs = <span class="built_in">len</span>(tmp_adj)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># fase : can use user embedding instead of result of item aggregation to improve speed</span></span><br><span class="line">            <span class="comment"># e_u = self.u2e.weight[list(tmp_adj)] # fast: user embedding </span></span><br><span class="line">            <span class="comment"># slow: item-space user latent factor (item aggregation)</span></span><br><span class="line">            feature_neigbhors = self.features(torch.LongTensor(<span class="built_in">list</span>(tmp_adj)).to(self.device))</span><br><span class="line">            e_u = torch.t(feature_neigbhors)</span><br><span class="line"></span><br><span class="line">            u_rep = self.u2e.weight[nodes[i]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># concatenated node embedding and neigbor vector (result of item aggregation) </span></span><br><span class="line">            <span class="comment"># and than through MLPs and Softmax to calculate weights</span></span><br><span class="line">            att_w = self.att(e_u, u_rep, num_neighs)</span><br><span class="line">            <span class="comment"># weight*neighbor vector</span></span><br><span class="line">            att_history = torch.mm(e_u.t(), att_w).t()</span><br><span class="line">            embed_matrix[i] = att_history</span><br><span class="line">        to_feats = embed_matrix</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
<h3 id="item-modeling">Item Modeling</h3>
<p>Similar with the Item Aggregation of User Modeling</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, history_uv_lists, history_r_lists, aggregator, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        tmp_history_uv = []</span><br><span class="line">        tmp_history_r = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">#get nodes(batch) neighbors of item</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            tmp_history_uv.append(self.history_uv_lists[<span class="built_in">int</span>(node)])</span><br><span class="line">            tmp_history_r.append(self.history_r_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># after neigh aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  <span class="comment"># user-item network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># id to embedding (features : v2e)</span></span><br><span class="line">        self_feats = self.features.weight[nodes]</span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<p>And the <code>self.aggregator</code> in neigh aggregation is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Aggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    item and user aggregator: for aggregating embeddings of neighbors (item/user aggreagator).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, v2e, r2e, u2e, embed_dim, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, history_uv, history_r</span>):</span><br><span class="line">        <span class="comment"># create a container for result, shpe of embed_matrix is (batchsize,embed_dim)</span></span><br><span class="line">        embed_matrix = torch.empty(<span class="built_in">len</span>(history_uv), self.embed_dim, dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># deal with each single item nodes&#x27; neighbors</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(history_uv)):</span><br><span class="line">            history = history_uv[i]</span><br><span class="line">            num_histroy_item = <span class="built_in">len</span>(history)</span><br><span class="line">            tmp_label = history_r[i]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># e_uv : turn neighbors(user node) id to embedding</span></span><br><span class="line">            <span class="comment"># uv_rep : turn single node(item node) to embedding</span></span><br><span class="line">            <span class="keyword">if</span> self.uv == <span class="literal">True</span>:</span><br><span class="line">                <span class="comment"># user component</span></span><br><span class="line">                e_uv = self.v2e.weight[history]</span><br><span class="line">                uv_rep = self.u2e.weight[nodes[i]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># item component</span></span><br><span class="line">                e_uv = self.u2e.weight[history]</span><br><span class="line">                uv_rep = self.v2e.weight[nodes[i]]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># get rating score embedding</span></span><br><span class="line">            e_r = self.r2e.weight[tmp_label]</span><br><span class="line">            <span class="comment"># concatenated rating and neighbor, and than through two layers mlp to get fjt</span></span><br><span class="line">            x = torch.cat((e_uv, e_r), <span class="number">1</span>)</span><br><span class="line">            x = F.relu(self.w_r1(x))</span><br><span class="line"></span><br><span class="line">            o_history = F.relu(self.w_r2(x))</span><br><span class="line">            <span class="comment"># calculate neighbor attention and fjt*weight to finish aggregation</span></span><br><span class="line">            att_w = self.att(o_history, uv_rep, num_histroy_item)</span><br><span class="line">            att_history = torch.mm(o_history.t(), att_w)</span><br><span class="line">            att_history = att_history.t()</span><br><span class="line"></span><br><span class="line">            embed_matrix[i] = att_history</span><br><span class="line">        <span class="comment"># result (batchsize, embed_dim)</span></span><br><span class="line">        to_feats = embed_matrix</span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>SocialRec</category>
      </categories>
  </entry>
  <entry>
    <title>tutorial of PyG</title>
    <url>/2023/05/23/PyG/</url>
    <content><![CDATA[<p>Some simple knowledge of PyG</p>
<span id="more"></span>
<h1 id="basic">Basic</h1>
<h2 id="data">Data</h2>
<p>A single graph in PyG is described by an instance of
<code>torch_geometric.data.Data</code>, which holds the following
attributes by default:</p>
<ol type="1">
<li><p><code>data.x</code>: Node feature matrix
<code>[num_nodes,num_node_features_dim]</code></p></li>
<li><p><code>data.edge_index</code>: Graph connectivity in <a
href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs">COO
format</a> with shape <code>[2, num_edges]</code> and type
<code>torch.long</code></p></li>
<li><p><code>data.edge_attr</code>: Edge feature matrix with shape
<code>[num_edges, num_edge_features_dim]</code></p></li>
<li><p><code>data.y</code>: Target to train(label). <em>e.g.</em>,
node-level targets of shape <code>[num_nodes, *]</code> or graph-level
targets of shape <code>[1, *]</code></p>
<p>...</p></li>
</ol>
<p>example:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230523172031612.png"
alt="image-20230523172031612" />
<figcaption aria-hidden="true">image-20230523172031612</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> Data</span><br><span class="line"></span><br><span class="line">edge_index = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                           [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                           [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                           [<span class="number">2</span>, <span class="number">1</span>]], dtype=torch.long)</span><br><span class="line">x = torch.tensor([[-<span class="number">1</span>], [<span class="number">0</span>], [<span class="number">1</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##t()会让原变量和新变量直接有依赖（类似浅拷贝），contiguous()断开依赖</span></span><br><span class="line">data = Data(x=x, edge_index=edge_index.t().contiguous())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Data(edge_index=[<span class="number">2</span>, <span class="number">4</span>], x=[<span class="number">3</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p><strong>Note</strong>： Although the graph has only two edges, we
need to define four index tuples to account for both directions of an
edge.</p>
<p>operation</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(data.keys)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;edge_index&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;x&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor([[-<span class="number">1.0</span>],</span><br><span class="line">            [<span class="number">0.0</span>],</span><br><span class="line">            [<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key, item <span class="keyword">in</span> data:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;key&#125;</span> found in data&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x found <span class="keyword">in</span> data</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>edge_index found <span class="keyword">in</span> data</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;edge_attr&#x27;</span> <span class="keyword">in</span> data</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="literal">False</span></span><br><span class="line"></span><br><span class="line">data.num_nodes</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">3</span></span><br><span class="line"></span><br><span class="line">data.num_edges</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">4</span></span><br><span class="line"></span><br><span class="line">data.num_node_features</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1</span></span><br><span class="line"></span><br><span class="line">data.has_isolated_nodes()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="literal">False</span></span><br><span class="line"></span><br><span class="line">data.has_self_loops()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="literal">False</span></span><br><span class="line"></span><br><span class="line">data.is_directed()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Transfer data object to GPU.</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">data = data.to(device)</span><br></pre></td></tr></table></figure>
<h2 id="minibatch">Minibatch</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> TUDataset</span><br><span class="line"><span class="keyword">from</span> torch_geometric.loader <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = TUDataset(root=<span class="string">&#x27;/tmp/ENZYMES&#x27;</span>, name=<span class="string">&#x27;ENZYMES&#x27;</span>, use_node_attr=<span class="literal">True</span>)</span><br><span class="line">loader = DataLoader(dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> loader:</span><br><span class="line">    batch</span><br><span class="line">    &gt;&gt;&gt; DataBatch(batch=[<span class="number">1082</span>], edge_index=[<span class="number">2</span>, <span class="number">4066</span>], x=[<span class="number">1082</span>, <span class="number">21</span>], y=[<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line">    batch.num_graphs</span><br><span class="line">    &gt;&gt;&gt; <span class="number">32</span></span><br></pre></td></tr></table></figure>
<h1 id="message-passing-network">Message Passing Network</h1>
<ul>
<li><a
href="https://blog.csdn.net/weixin_39925939/article/details/121360884">(149条消息)
pytorch geometric教程一: 消息传递源码详解（MESSAGE
PASSING）+实例_每天都想躺平的大喵的博客-CSDN博客</a></li>
</ul>
]]></content>
      <categories>
        <category>GNN</category>
        <category>Frame</category>
      </categories>
  </entry>
  <entry>
    <title>Python Basic</title>
    <url>/2023/07/11/PythonBasic/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="sort">sort</h1>
<ol type="1">
<li>怎么让数组在第一维度正序，第二维度倒序</li>
</ol>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">li.sort(key = <span class="keyword">lambda</span> x:(x[<span class="number">0</span>],-x[<span class="number">1</span>]),reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>王树森推荐系统公开课</title>
    <url>/2023/03/13/Recommendation-WangShusen/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="基本概念">基本概念</h1>
<h2 id="指标">指标</h2>
<h3 id="消费指标">消费指标</h3>
<p>点击率=点击次数/曝光次数</p>
<p>点赞量=点赞次数/点击次数</p>
<p>收藏率=收藏次数/点击次数</p>
<p>转发率=转发次数/点击次数</p>
<p>阅读完成率=滑动到底次数/点击次数<span class="math inline">\(\times
f(笔记长度)\)</span></p>
<h3 id="北极星指标">北极星指标</h3>
<p>用户规模：日活用户数（DAU），月活用户数（MAU）</p>
<p>消费：人均使用推荐时长、人均阅读笔记数量</p>
<p>发布： 发布渗透率、人均发布量</p>
<h2 id="推荐系统链路">推荐系统链路</h2>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230313220835272.png"
alt="image-20230313220835272" />
<figcaption aria-hidden="true">image-20230313220835272</figcaption>
</figure>
<ol type="1">
<li>召回：快速从海量数据中取回几千个用户可能感兴趣的物品。</li>
<li>粗排：用小规模的模型的神经网络给召回的物品打分，然后做截断，选出分数最高的几百个物品。</li>
<li>精排：
用大规模神经网络给粗排选中的几百个物品打分，可以做截断，也可以不做截断。</li>
<li>重排：
对精排结果做多样性抽样，得到几十个物品，然后用规则调整物品的排序。</li>
</ol>
<h2 id="ab测试">AB测试</h2>
<p>完成离线测试后，使用线上小流量AB测试考察指标，或者用AB测试调参（GNN深度）</p>
<h3 id="随机分桶">随机分桶</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20231119172737630.png"
alt="image-20231119172737630" />
<figcaption aria-hidden="true">image-20231119172737630</figcaption>
</figure>
<p>在不同的桶上使用不同的策略或参数实验。</p>
<h3 id="分层实验">分层实验</h3>
<p>不同的部门都需要做AB测试，每个部门对应一个层，分层实验满足：</p>
<ol type="1">
<li><strong>同层互斥</strong>：同一个部门做实验不能使用同一个桶；例：GNN实验占了召回层4个桶，其它召回实验只能用剩下的6个桶。</li>
<li><strong>不同层正交</strong>：每一层独立随机对用户做分桶。每一层都可以独立用100%的用户做实验。</li>
</ol>
<h1 id="召回">召回</h1>
<h2 id="协同过滤">协同过滤</h2>
<h3 id="基于物品的协同过滤-itemcf">基于物品的协同过滤 ItemCF</h3>
<p>基本思想：如果用户喜欢item1,而item1与item2相似，那么用户很可能喜欢item2.</p>
<h4 id="基本结构">基本结构：</h4>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230313223949470.png"
alt="image-20230313223949470" />
<figcaption aria-hidden="true">image-20230313223949470</figcaption>
</figure>
<p>我们从用户历史互动知道用户对<span
class="math inline">\(item_j\)</span>，感兴趣利用下面公式计算对候选物品的兴趣分数
<span class="math display">\[
\sum_jlike(user,item_j)\times sim(item_j,item)
\]</span> 在这个例子中，用户对候选item的兴趣是：<span
class="math inline">\(2\times
0.1+1\times0.4+4\times0.2+3\times0.6=3.2\)</span>,我们计算所有item的分数，然后返回分数最高的若干个item</p>
<h5 id="计算item相似度">计算item相似度</h5>
<p>可以通过与item交互过的用户重合度计算item相似度（其中一种方法，也可以用KG）</p>
<ol type="1">
<li>方法1：不考虑用户对物品的喜欢程度</li>
</ol>
<p><span class="math display">\[
sim(i_1,i_2) = \frac{|W1 \cap W2|}{\sqrt[2]{|W1|\cdot |W2|}}
\]</span></p>
<p>其中，喜欢物品<span class="math inline">\(i_1\)</span>的用户记作<span
class="math inline">\(W_1\)</span>,喜欢物品<span
class="math inline">\(i_2\)</span>的用户记作<span
class="math inline">\(W_2\)</span>.</p>
<ol start="2" type="1">
<li><p>方法2： 考虑用户对物品的喜欢程度,使用余弦相似度！</p>
<p>把每个item用向量表示 <span class="math display">\[
i_1=[like(u_1,i_1),like(u_2,i_1),\cdots ,like(u_n,i_1)] \space u_n\in W
\]</span></p>
<p><span class="math display">\[
i_2=[like(u_1,i_2),like(u_2,i_2),\cdots ,like(u_n,i_2)] \space u_n\in W
\]</span></p>
<p><span class="math display">\[
W=W_1\cup W_2
\]</span></p>
<p>我们使用余弦相似度计算： <span class="math display">\[
similarity=cos(\theta) = \frac{A\cdot B}{||A||\space ||B||}
\]</span> 如果有用户k只喜欢其中一个物品:只喜欢<span
class="math inline">\(i_1\)</span>不喜欢<span
class="math inline">\(i_2\)</span>,那么<span
class="math inline">\(i_2[k]=0\)</span>，所以点乘后第k项为0，所以点乘只与同时喜欢<span
class="math inline">\(i_1,i_2\)</span>的用户有关系，如下面公式 <span
class="math display">\[
sim(i_1,i_2) = \frac{\sum_{v\in V}like(v,i_i)\cdot
like(v,i_2)}{\sqrt[2]{\sum_{u_1\in
W_1}like^2(u_1,i_1)}\sqrt[2]{\sum_{u_2\in W_2}like^2(u_2,i_2)}}
\]</span></p></li>
<li></li>
<li><p>皮尔逊系数 <span class="math display">\[
sim(i,j)=\frac{\sum_{p\in P}(R_{i,p}-\bar R_i)(R_{j,p}-\bar
R_j)}{\sqrt{\sum_{p\in P}(R_{i,p}-\bar R_i)^2}\sqrt{\sum_{p\in
P}(R_{j,p}-\bar R_j)^2}}
\]</span></p></li>
</ol>
<h4 id="运作基本流程">运作基本流程</h4>
<ol type="1">
<li><p>实现做离线计算，预先计算两个索引：</p>
<ol type="1">
<li><p>“user2item”：记录每个用户最近点击交互过的n个物品ID（lastN）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># example 不一定是公司真实的保存方式</span></span><br><span class="line">user2item=&#123;</span><br><span class="line">    <span class="string">&#x27;u1&#x27;</span>:[[i1,like(u1,i1)],[i2,like(u1,i2)],...,[<span class="keyword">in</span>,like(u1,<span class="keyword">in</span>)]]</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>"item2item":计算物品之间两两相似度，记录每个物品最相似的k个物品。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">item2item=&#123;</span><br><span class="line">	#target item:[[similar item, similarity score]...]</span><br><span class="line">	&#x27;i1&#x27;:[[i2,0.9],[i6,0.88]...]</span><br><span class="line">	&#x27;i2&#x27;:...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol></li>
<li><p>线上做召回</p>
<ol type="1">
<li>给定用户ID，通过“user2item”找到用户近期感兴趣的物品列表(last-n)</li>
<li>对于last-n列表中每个物品，通过“item2item"找到top-k相似物品。现在有1个user，n个互动物品，nxk个候选物品。</li>
<li>计算候选物品兴趣分数</li>
<li>返回分数最高的100个物品作为推荐结果</li>
</ol></li>
</ol>
<h3 id="swing召回通道">Swing召回通道</h3>
<p>如果两个Item的重合用户来源于一个小圈子（微信群），一个小圈子用户同时与两个Item交互，不能说明两个Item相似，如果很多不相关的用户交互两个Item，说明Item相似。</p>
<h4 id="基本结构-1">基本结构</h4>
<ol type="1">
<li>计算用户重合度</li>
</ol>
<p>用户<span class="math display">\[u_1\]</span>喜欢的物品记作集合<span
class="math display">\[J_1\]</span></p>
<p>用户<span class="math display">\[u_2\]</span>喜欢的物品记作集合<span
class="math display">\[J_2\]</span></p>
<p>定义两个用户的重合度： <span class="math display">\[
overlap(u_1,u_2)=|J_1\cap J_2|
\]</span> 用户<span class="math display">\[u_1\]</span>和<span
class="math display">\[u_2\]</span>的重合度高，则他们可能来自一个小圈子，要降低他们的权重。</p>
<ol start="2" type="1">
<li>计算物品相似度</li>
</ol>
<p>喜欢物品<span class="math display">\[i_1\]</span>的用户记作集合<span
class="math display">\[W_1\]</span></p>
<p>喜欢物品<span class="math display">\[i_2\]</span>的用户记作集合<span
class="math display">\[W_2\]</span> <span class="math display">\[
V=W_1\cap W_2
\]</span></p>
<p><span class="math display">\[
sim(i_1,i_2) = \sum_{u_1\in V}\sum_{u_2\in
V}\frac{1}{\alpha+overlap(u_1,u_2)}
\]</span></p>
<p>u1u2都对物品i1i2感兴趣，这样的用户越多，说明物品越相似</p>
<p><span class="math display">\[\alpha\]</span>是超参数</p>
<h3 id="基于用户的协同过滤usercf">基于用户的协同过滤（UserCF）</h3>
<p>假设：u1与u2兴趣十分相似，u1可能会对u2交互的item感兴趣</p>
<p>何为兴趣相似：</p>
<ol type="1">
<li>点击、点赞、收藏、转发的笔记有很大重合</li>
<li>关注的作者有很大的重合</li>
</ol>
<h4 id="基本结构-2">基本结构</h4>
<p><img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230525170605162.png"
alt="image-20230525170605162" /> <span class="math display">\[
\sum_jsim(user,user_j)\times like(user_j,item)
\]</span></p>
<h5 id="计算user相似度">计算User相似度</h5>
<ol type="1">
<li><p>计算User相似度</p>
<p>把每个用户表示为一个稀疏向量，向量每个元素对应一个物品。相似度sim就是两个向量夹角的余弦。<span
class="math display">\[u_1\cdot u_2\]</span>结果就是<span
class="math display">\[|I|\]</span></p></li>
</ol>
<p><span class="math display">\[
sim(u_1,u_2) = \frac{|I|}{\sqrt{|J_1|\cdot|J_2|}}
\]</span></p>
<p><span class="math display">\[J_1\]</span>: 用户<span
class="math display">\[u_1\]</span>喜欢的物品集合</p>
<p><span class="math display">\[J_2\]</span>: 用户<span
class="math display">\[u_2\]</span>喜欢的物品集合</p>
<p><span class="math display">\[I\]</span>：<span
class="math display">\[J_1\cap J_2\]</span></p>
<p>|*|:集合的大小</p>
<p><span
class="math display">\[sim(u_1,u_2)\in[0,1]\]</span>,越大代表用户越相似</p>
<ol start="2" type="1">
<li>降低热门物品权重</li>
</ol>
<p>大家都喜欢哈利波特，哈利波特对用户相似度计算意义小,所以我们降低热门物品权重
<span class="math display">\[
sim(u_1,u_2) = \frac{\sum_{l\in I}weight(l)}{\sqrt{|J_1|\cdot|J_2|}}
\]</span></p>
<p><span class="math display">\[
weight(l) = \frac{1}{log(1+n_l)}
\]</span></p>
<p><span class="math display">\[n_l\]</span>:
喜欢物品l的用户数量，反应物品的热门程度。<span
class="math display">\[n_l\]</span>越大，<span
class="math display">\[log(1+n_l)\]</span>越大，权重越小</p>
<h4 id="运作基本流程-1">运作基本流程</h4>
<ol type="1">
<li><p>实现做离线计算，预先计算两个索引：</p>
<ol type="1">
<li><p>“user2item”：记录每个用户最近点击交互过的n个物品ID（lastN）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># example 不一定是公司真实的保存方式</span></span><br><span class="line">user2item=&#123;</span><br><span class="line">    <span class="string">&#x27;u1&#x27;</span>:[[i1,like(u1,i1)],[i2,like(u1,i2)],...,[<span class="keyword">in</span>,like(u1,<span class="keyword">in</span>)]]</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>"user2user":计算用户之间两两相似度，记录每个用户最相似的k个用户。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">user2user=&#123;</span><br><span class="line">	#target user:[[similar user, similarity score]...]</span><br><span class="line">	&#x27;u1&#x27;:[[u2,0.9],[u6,0.88]...]</span><br><span class="line">	&#x27;u2&#x27;:...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol></li>
<li><p>线上做召回</p>
<ol type="1">
<li>给定用户ID，通过“user2user”找到top-k相似用户</li>
<li>对于top-k列表中每个用户，通过“user2item"找到用户近期感兴趣物品列表(last-n)。</li>
<li>对于召回的nk个相似物品，用公式预估用户对每个物品的兴趣分数</li>
<li>返回分数最高的100个物品，作为召回结果</li>
</ol></li>
</ol>
<h3 id="协同过滤缺点">协同过滤缺点</h3>
<p>。。。</p>
<h2 id="向量召回">向量召回</h2>
<h3 id="矩阵补充-matrix-completion">矩阵补充 Matrix Completion</h3>
<p>用于填充评分矩阵中无评分的部分，通过求user与item embedding的内积</p>
<p><img src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230626164840701.png" alt="image-20230626164840701" style="zoom:33%;" /></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626163645000.png"
alt="image-20230626163645000" />
<figcaption aria-hidden="true">image-20230626163645000</figcaption>
</figure>
<h4 id="数据集">数据集</h4>
<ol type="1">
<li><p>（用户ID,物品ID，兴趣分数）————》<span
class="math inline">\(dataset={(u,i,y)}\)</span></p></li>
<li><p>正负例子（0-4分）：</p>
<ol type="1">
<li>负例子：曝光没有点击-0分</li>
<li>正例子：点击、点赞、收藏、转发-各1分</li>
</ol></li>
</ol>
<h4 id="训练">训练</h4>
<p><span class="math display">\[
min_{A,B}\sum _{(u,i,y)\in dataset}(y-&lt;a_u,b_i&gt;)^2
\]</span></p>
<h4 id="缺点">缺点</h4>
<ol type="1">
<li>仅用ID embedding，没利用物品、用户的属性。</li>
<li>负样本选取方法不对。</li>
<li>做训练方法不好
<ol type="1">
<li>内积效果不如余弦相似度</li>
<li>用回归方法不如用分类方法。</li>
</ol></li>
</ol>
<h4 id="运作基本流程-2">运作基本流程</h4>
<ol type="1">
<li>离线计算
<ol type="1">
<li>训练矩阵A、B（embedding层的参数，A for user, B for item）</li>
<li>由于矩阵很大，为了快速读取使用hash方法：
<ol type="1">
<li>把矩阵A存储到key-value表{user_id: user_embedding}。</li>
<li>（加速最近邻查找）将item分区保存至key-value表</li>
</ol></li>
</ol></li>
<li>线上服务
<ol type="1">
<li>通过用户ID查询用户向量，记作A。</li>
<li>最近邻查找：查找用户最优可能感兴趣的k个物品作为召回结果。
<ol type="1">
<li>第i号物品的embedding向量记作<span
class="math inline">\(b_i\)</span></li>
<li>求<span class="math inline">\(&lt;a,b_i&gt;\)</span></li>
<li>返回内积最大的k个物品</li>
</ol></li>
</ol></li>
</ol>
<p><strong>加速最近邻查找方法</strong>：</p>
<p>一般item有几亿个，暴力计算内积并排序过慢</p>
<p>方法：</p>
<ol type="1">
<li><p>确定衡量最近邻标注：欧氏距离最小（L2距离），向量内积最大（内积相似度），向量夹角余弦最大（cosine相似度）</p></li>
<li><p>根据衡量标准将所有item
embedding分块，下面为根据余弦相似度分块的例子，每一个区域用一个向量E表示，通过key-value表保存区域向量E与区域中所有向量的embedding。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626171141459.png"
alt="image-20230626171141459" />
<figcaption aria-hidden="true">image-20230626171141459</figcaption>
</figure></li>
<li><p>求区域向量与user的余弦相似度，获取结构最大区域。再将区域中所有的item暴力枚举算相似度。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626171541263.png"
alt="image-20230626171541263" />
<figcaption aria-hidden="true">image-20230626171541263</figcaption>
</figure></li>
</ol>
<h3 id="双塔模型">双塔模型</h3>
<p>融合除了ID以为的别的特征</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626173816258.png"
alt="image-20230626173816258" />
<figcaption aria-hidden="true">image-20230626173816258</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626173835292.png"
alt="image-20230626173835292" />
<figcaption aria-hidden="true">image-20230626173835292</figcaption>
</figure>
<h4 id="数据集-1">数据集</h4>
<ol type="1">
<li><p>正样本</p>
<p>曝光且有点击的（user，item）组</p>
<p>问题：少部分物品占据大部分点击，导致正样品大多是热门物品，对冷门物品不公平。</p>
<p>解决：过采样冷门物品，或降采样热门物品</p>
<p>​ 过采样：一个样品出现多次</p>
<p>​ 降采样：一些样本被抛弃</p></li>
<li><p>负样本</p>
<p>混合几种负样本：50%的简单负样本，50%的困难负样本</p>
<p>我们分别讨论下面三种可以作为负样本的数据。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626202449765.png"
alt="image-20230626202449765" />
<figcaption aria-hidden="true">image-20230626202449765</figcaption>
</figure>
<ol type="1">
<li><p>简单负样本</p>
<p>没有被召回的数据: <strong>全体物品</strong></p>
<p>没被召回的数据，大概率是用户不感兴趣的，未被召回的样本约等于全体物品，所以在全体物品中做抽样作为负样本。</p>
<p><strong>均匀抽样</strong>：正样本大多是热门物品，负样本大多是冷门物品。（因为热门物品比例小），所以我们需要利用非均匀抽样打压热门物品。</p>
<p><strong>非均抽采样</strong>：负样本抽样概率与热门程度（点击次数）正相关，<span
class="math inline">\(抽样概率\propto (点击次数)^{0.75}\)</span></p>
<p><strong>Batch内负采样</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626203300364.png"
alt="image-20230626203300364" />
<figcaption aria-hidden="true">image-20230626203300364</figcaption>
</figure>
<p>一个batch有n个正样本对，一个用户和n-1个物品组成负样本，batch中一共有n（n-1）个负样本对。</p>
<p>此时，热门物品成为负样本的概率过大（热门物品成为正样本概率大）：<span
class="math inline">\(抽样概率\propto (点击次数)\)</span></p>
<p>所以做训练时，兴趣分数调整为：<span
class="math inline">\(cos(a,b_i)-logp_i\)</span>降低热门物品作为负样本的惩罚</p></li>
<li><p><strong>困难负样本</strong>：用户有一点兴趣，但兴趣不够，特别容易分错</p>
<p>被粗排淘汰的物品（比较困难）</p>
<p>精排分数靠后的物品（非常困难）</p></li>
</ol>
<p><strong>注意</strong>：不能用曝光但没有点击的样本，因为能通过精排（更复杂的模型）的样本已经是用户比较感兴趣的样本，可能只是机缘巧合没有点击，训练召回不能用这一类样本，但是训练排序可以</p></li>
</ol>
<h4 id="训练-1">训练</h4>
<h5 id="pointwise">Pointwise</h5>
<p>当做二分类任务，对于正样本，鼓励cos(a,b)接近+1；对于负样本，鼓励cos(a,b)接近-1</p>
<h5 id="pairwise">Pairwise</h5>
<p>鼓励<span class="math inline">\(cos(a,b^+)\)</span>大于<span
class="math inline">\(cos(a,b^-)\)</span></p>
<p>Triplet hinge loss: <span class="math display">\[
L(a,b^+,b^-) = max\{0,cos(a,b^-)+m-cos(a,b^+)\}
\]</span> m为超参数</p>
<p>Triplet logistic loss: <span class="math display">\[
L(a,b^+,b^-) = log(1+exp[\sigma(cos(a,b^-)-cos(a,b^+))])
\]</span></p>
<h5 id="listwise">Listwise</h5>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626175637609.png"
alt="image-20230626175637609" />
<figcaption aria-hidden="true">image-20230626175637609</figcaption>
</figure>
<h4 id="运作基本流程-3">运作基本流程</h4>
<ol type="1">
<li>离线存储：把物品向量b存入向量数据库。</li>
<li>线上召回：查找用户最感兴趣的k个物品。
<ol type="1">
<li>给定用户ID和画像，线上用升级网络算用户向量A。</li>
<li>最近邻查找</li>
</ol></li>
</ol>
<p>为什么用户向量要在线计算：</p>
<ol type="1">
<li>没做一次召回只用到一个用户向量A，计算成本较小。</li>
<li>用户兴趣动态变化，物品较稳定。</li>
</ol>
<h4 id="模型更新">模型更新</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626205649864.png"
alt="image-20230626205649864" />
<figcaption aria-hidden="true">image-20230626205649864</figcaption>
</figure>
<p><strong>全量更新</strong>：</p>
<p>​
在昨天模型参数基础上做训练（不是随机初始化），用昨天的数据，shuffle后训练一个epoch后发布新的用户塔神经网络和物品向量，供线上召回使用。</p>
<p><strong>增量更新</strong>：</p>
<p>​ 用户兴趣随时发生变化，实时收集线上数据，对模型做online
learning，增量更新ID
Embedding参数(不更新神经网络其他部分参数)，发布用户ID
Embedding，供用户塔线上计算用户向量。</p>
<p><strong>不能只做增量更新，不做全量更新</strong></p>
<ol type="1">
<li>小时级数据有偏差，分钟级偏差更大。</li>
<li>全量更新：random shuffle一天数据，做
1epoch训练；增量更新按照数据从早到晚顺序做1epoch训练，全量更新效果更好。</li>
</ol>
<h4 id="自监督学习">自监督学习</h4>
<h5 id="背景">背景</h5>
<p>推荐系统头部效应严重：少部分物品占据大部分点击，大部分物品曝光、点击次数不高，导致高点击物品的表征学习的好，长尾物品的表征学的不好，用自监督学习做data
augmentation，更好的学习长尾物品的向量表征。</p>
<h5 id="method">Method</h5>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626212328043.png"
alt="image-20230626212328043" />
<figcaption aria-hidden="true">image-20230626212328043</figcaption>
</figure>
<h6 id="特征变换方法">特征变换方法</h6>
<ol type="1">
<li><p>Random Mask</p>
<p>随机选一些离散特征(例如类目特征)，把它们遮住</p>
<p>例子：<span class="math inline">\(U=\{数码，摄影\}\)</span>-&gt;<span
class="math inline">\(U&#39;-\{default\}\)</span></p></li>
<li><p>Dropout</p>
<p>一个物品可以有多个类目，那么类目是一个多值离散特征。Dropout会随机丢弃特征中50%的值。</p>
<p>例子：<span class="math inline">\(U=\{数码，摄影\}\)</span>-&gt;<span
class="math inline">\(U&#39;-\{数码\}\)</span></p></li>
<li><p>complementary互补特征</p>
<p>假设物品一共有四种特征：ID,类目，关键词，城市</p>
<p>随机分成两组：{ID,关键词}，{类目，城市}</p>
<p>{ID,default，关键词，default}作为表征i‘</p>
<p>{default，类目，default，城市}作为表征i‘’</p></li>
<li><p>Mask一组关联的特征</p>
<p>p(u): 某特征取值为u的概率</p>
<p>p(u,v):某特征取值为u，另一个特征取值为v同时发生的概率</p>
<p>离线计算特征的两两关系，用户信息(mutual information): <span
class="math display">\[
MI(U,V)=\sum_{u\in U}\sum_{v\in V}p(u,v)\cdot log\frac{p(u,v)}{p(u)\cdot
p(v)}
\]</span>
假设一共有k种特征。离线计算两两MI，得到kxk的矩阵，随机选一个特征为种子，找到种子最相关的k/2中特征Mask掉，保留其余的k/2中特征。</p>
<p>比random
mask、dropout、互补特征等方法效果更好，但方法复杂实现难度大不容易维护。</p></li>
</ol>
<h6 id="自监督训练">自监督训练</h6>
<p>从全体物品中均匀抽样得到m个物品，作为一个batch。</p>
<p>做两类特征变换，物品他输出两组向量。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626214137364.png"
alt="image-20230626214137364" />
<figcaption aria-hidden="true">image-20230626214137364</figcaption>
</figure>
<h6 id="自监督训练正常训练">自监督训练+正常训练</h6>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626214246267.png"
alt="image-20230626214246267" />
<figcaption aria-hidden="true">image-20230626214246267</figcaption>
</figure>
<h2 id="不适合召回的模型">不适合召回的模型</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626175824199.png"
alt="image-20230626175824199" />
<figcaption aria-hidden="true">image-20230626175824199</figcaption>
</figure>
<p>召回需要计算的item量很大，所以我们一般只做后期融合（计算相似度的时候再融合user和item的embedding），因为融合步骤一般要在线计算不能离线计算完保存（需求内存量太大）。如果我们在召回阶段就要让user
embedding与上亿个item
embedding过神经网络模型，这样时间复杂度太高了。</p>
<h2 id="其他方式召回">其他方式召回</h2>
<p>地理召回</p>
<p>作者召回</p>
<p>缓存召回：复用前n次推荐精排的结果</p>
<h2 id="曝光过滤与链路">曝光过滤与链路</h2>
<p>。。</p>
<h1 id="排序">排序</h1>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>BasicTurtorial</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习推荐系统-王喆</title>
    <url>/2023/07/01/Recommendation-Wangzhe/</url>
    <content><![CDATA[<h1 id="传统模型">传统模型</h1>
<h2 id="协同过滤">协同过滤</h2>
<p>User-CF适用于发现热点以及跟踪热点趋势。（user的爱好经常变动，相似度也会随之变动，而且容易受社交关系影响）</p>
<p>Item-CF适用于兴趣变化较为稳定的应用。（item相似度比较稳定）</p>
<p>缺点：泛化能力弱：无法将两个物品相似这一信息推广到其他物品的相似性计算上。导致热门的物品有很强的头部效应，容易根大量物品产生相似性，尾部物品则因为特征向量稀疏很少与其他物品产生相似性。</p>
<h3 id="user-cf">User-CF</h3>
<p>缺点：
用户数往往大于物品书，用户数的增长会导致用户相似度矩阵的存储空间以<span
class="math inline">\(n^2\)</span>的速度快速增长。</p>
<h4 id="计算用户相似度">计算用户相似度</h4>
<ol type="1">
<li>余弦相似度</li>
</ol>
<p><span class="math display">\[
similarity=cos(\theta) = \frac{A\cdot B}{||A||\space ||B||}
\]</span></p>
<ol start="2" type="1">
<li>皮尔逊系数-user <span class="math display">\[
sim(i,j)=\frac{\sum_{p\in P}(R_{i,p}-\bar R_i)(R_{j,p}-\bar
R_j)}{\sqrt{\sum_{p\in P}(R_{i,p}-\bar R_i)^2}\sqrt{\sum_{p\in
P}(R_{j,p}-\bar R_j)^2}}
\]</span></li>
</ol>
<p><span class="math inline">\(R_{i,p}\)</span>:
用户i对物品p的评分。</p>
<p><span class="math inline">\(\bar{R_i}\)</span>:
用户对所有物品的平均评分。</p>
<ol start="3" type="1">
<li>皮尔逊系数-item</li>
</ol>
<p><span class="math display">\[
sim(i,j)=\frac{\sum_{p\in P}(R_{i,p}-\bar R_p)(R_{j,p}-\bar
R_p)}{\sqrt{\sum_{p\in P}(R_{i,p}-\bar R_p)^2}\sqrt{\sum_{p\in
P}(R_{j,p}-\bar R_p)^2}}
\]</span></p>
<p><span class="math inline">\(R_{i,p}\)</span>:
用户i对物品p的评分。</p>
<p><span class="math inline">\(\bar{R_p}\)</span>: 物品p的平均分。</p>
<h4
id="根据top-n相似用户生成最终推荐结果">根据top-n相似用户生成最终推荐结果</h4>
<p><span class="math display">\[
R_{u,s}=\frac{\sum_{s\in S}(w_{u,s}\cdot R_{s,p})}{\sum_{s\in S}w_(u,s)}
\]</span></p>
<p><span class="math inline">\(w_{u,s}\)</span>：
是用户u和用户s的相似度</p>
<p><span class="math inline">\(R_{s,p}\)</span>：
是用户s对物品p的评分。</p>
<h3 id="item-cf">Item-CF</h3>
<p>计算相似度后（计算方法与user相同），用下面式子计算： <span
class="math display">\[
R_{u,p}=\sum_{u\in H}(w_{p,h},\cdot R_{u,h})
\]</span> <span class="math inline">\(w_{p,h}\)</span>：
是物品p与物品h的相似程度。</p>
<p><span class="math inline">\(R_{u,h}\)</span>：
是用户u对物品h的已有评分。</p>
<h2 id="矩阵分解">矩阵分解</h2>
<p><strong>协同过滤的进化</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230706214331238.png"
alt="image-20230706214331238" />
<figcaption aria-hidden="true">image-20230706214331238</figcaption>
</figure>
<p>优点：</p>
<ol type="1">
<li>泛化能力强</li>
<li>空间复杂度低：不用保存用户或物品相似度矩阵，<span
class="math inline">\((m,m)or(n,n)-&gt;(n+m)\cdot k\)</span></li>
<li>更好的扩展性和灵活性。易于与其他特征拼接，易于与深度学习无缝联合。</li>
</ol>
<p>缺点：没有考虑用户与物品的其他特征。</p>
<h3 id="definition">Definition</h3>
<p>通过分解共现矩阵学习用户和物品的表示。</p>
<p>将（m,n)维的共现矩阵M分解为(m,k)维的用户矩阵和(k,n)维的物品矩阵，其中相应的行和列为特定用户与物品的表示，k是用户和物品表示的维度。</p>
<p>预测方法：用户表示与物品表示的内积。 <span class="math display">\[
r_{ui}=q_i^Tp_u
\]</span></p>
<h3 id="如何分解矩阵">如何分解矩阵</h3>
<h4 id="奇异值分解">奇异值分解</h4>
<p>特征值分解只能用于方阵，所以用特征值分解</p>
<p><a
href="https://zhuanlan.zhihu.com/p/613284889">矩阵分解—特征值分解与奇异值分解
- 知乎 (zhihu.com)</a></p>
<p>通过奇异值分解求得<span class="math inline">\(M=U\sum
V^T\)</span>,其中<span class="math inline">\(U\in (m,m),\sum \in
(m,n),V^T\in (n,n)\)</span>,中间的为对角阵</p>
<p>取<span
class="math inline">\(\sum\)</span>中较大的k个元素为隐含特征，删除其他维度（U与V中的也删掉）</p>
<p>得到<span class="math inline">\(M=U_{m\times k}\sum_{k\times k}
V^T_{k\times n}\)</span></p>
<p><strong>缺点</strong>：</p>
<ol type="1">
<li><p>奇异值分解要求原始的共现矩阵是稠密的，如果要使用奇异值分解，就必须对确实的元素值进行填充。</p></li>
<li><p>复杂度高O(mn^2)</p></li>
</ol>
<h4 id="梯度下降">梯度下降</h4>
<p>主要方法</p>
<p>Loss： <span class="math display">\[
L=min_{q^*,p^*}\sum (r_{ui}-q_i^Tp_u)^2+\lambda(||q_i||^2+||p_u||^2)
\]</span> 由于不同用户打分标准不同，加入偏差 <span
class="math display">\[
r_{ui}=\mu +b_i+b_u+q^T_ip_u
\]</span> <span class="math inline">\(\mu\)</span>:
全局偏差常数,超参数，提前设置好</p>
<p><span class="math inline">\(b_i\)</span>:
物品偏差系数，可以使用物品i收到的所有评分的均值</p>
<p><span class="math inline">\(b_u\)</span>:
用户偏差系数，可以使用用户u给出的所有评分的均值 <span
class="math display">\[
L=min_{q^*,p^*}\sum
(r_{ui}-q_i^Tp_u-\mu-b_u-b_i)^2+\lambda(||q_i||^2+||p_u||^2+b_u^2+b_i^2)
\]</span></p>
<h2 id="逻辑回归">逻辑回归</h2>
<p><strong>独立于协同过滤的推荐模型方向</strong></p>
<p>将用户年龄、性别、物品属性等特征转为数值型向量输入回归或逻辑回归模型</p>
<p>优点：融合了特征</p>
<p>缺点：逻辑回归模型简单，表达能力不强</p>
<h2 id="poly2">POLY2</h2>
<p>逻辑回归只对单一特征做简单加权，不具备特征交叉生成高维组合特征的能力
<span class="math display">\[
POLY2(W,X)=\sum_{j_1=1}^{n-1}
\sum_{j_2=j_1+1}^nw_h(j_1,j_2)x_{j_1}x_{j_2}
\]</span> POLY2就是直接暴力组合特征,<span
class="math inline">\(x\)</span>是未经embedding处理的特征（one-hot 或
数值特征）</p>
<p>缺点：</p>
<ol type="1">
<li>常常用one-hot编码方式处理类别数据（就是大量<span
class="math inline">\(x_？\)</span>会为0），POLY2不进行特征选择，会让本来就稀疏的向量更稀疏</li>
<li>权重参数<span
class="math inline">\(n-&gt;n^2\)</span>，极大提高了训练复杂度。</li>
</ol>
<h2 id="fm-factorization-machines因式分解">FM-Factorization
Machines因式分解</h2>
<p>FM为给个特征学习了一个隐权重向量，在特征交叉时，使用两个特征隐向量的内积作为交叉特征的权重。下面是二阶的数学部分：
<span class="math display">\[
FM(w,x)=\sum_{j_i=1}^{n-1} \sum_{j_2=j_1+1}^n (w_{j_1}\cdot
w_{j_2})x_{j_1}x_{j_2}
\]</span> 优点：</p>
<ol type="1">
<li>计算复杂度<span
class="math inline">\(n^2-&gt;nk\)</span>,k是隐向量维度</li>
<li>泛化强，更好的解决数据稀疏性问题：POLY2只有在出现<span
class="math inline">\(x_{j_1},x_{j_2}\)</span>组合同时出现时才能学习到weight（不如梯度下降梯度为0），FM只要在组合中其中一个是<span
class="math inline">\(x_{j_1}\)</span>就能学到隐向量，能反推出没出现过组合的权重。</li>
</ol>
<p>缺点：</p>
<ol type="1">
<li>丢失了某些具体特征组合的精确记忆能力。</li>
</ol>
<h2 id="ffm-field-aware-factorization-machines">FFM-Field-aware
Factorization Machines</h2>
<p>FFM每个特征对应的不是唯一一个隐向量，而是一组隐向量。特征作用于不同的特征域有不同的隐向量：特征1与特征2交叉，则是特征1作用于特征域2:<span
class="math inline">\(w_{j_1,f_2}\)</span>乘特征2作用于特征域1:<span
class="math inline">\(w_{j_2,f_1}\)</span> <span class="math display">\[
FFM(w,x)=\sum_{j_i=1}^{n-1} \sum_{j_2=j_1+1}^n (w_{j_1,f_2}\cdot
w_{j_2,f_1})x_{j_1}x_{j_2}
\]</span> <strong>与FM区别</strong>：</p>
<p>下图中，P，下面是特征值</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230706212714465.png"
alt="image-20230706212714465" />
<figcaption aria-hidden="true">image-20230706212714465</figcaption>
</figure>
<table>
<thead>
<tr class="header">
<th></th>
<th>ESPN特征与NIKE特征交叉</th>
<th>ESPN特征与MALE特征交叉</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FM</td>
<td><span class="math inline">\(w_{ESPN}\cdot w_{NIKE}\)</span></td>
<td><span class="math inline">\(w_{ESPN}\cdot w_{MALE}\)</span></td>
</tr>
<tr class="even">
<td>FFM</td>
<td><span class="math inline">\(w_{ESPN,A}\cdot w_{NIKE,P}\)</span></td>
<td><span class="math inline">\(w_{ESPN,G}\cdot w_{MALE,P}\)</span></td>
</tr>
</tbody>
</table>
<p>计算复杂度：需要学习n个特征在f个特征域上的k维隐向量，参数数量：<span
class="math inline">\(n\cdot k\cdot
f\)</span>,二次项不能像FM一样简化，复杂度是<span
class="math inline">\(kn^2\)</span>。</p>
<p><strong>注意，n&gt;f,一个特征域有可能有多个特征，例如性别特征域有两种特征：男和女。我们只需要学习NIKE特征在性别特征域的1个隐向量，不需要具体学习NIKE对男性的隐向量或NIKE对女性的。这样参数量还是比POLY2少很多。</strong></p>
<h2 id="gbdtlr">GBDT+LR</h2>
<p>POLY2，再提高交叉维度会产生组合爆炸。</p>
<p>GBDT+LR：就是利用GBDT自动进行特征筛选和组合，生成新的离散特征向量，再把特征向量当做LR模型输入。</p>
<p>以前特征组合要么人工筛选，要么通过改造目标函数筛选，GBDT+LR实现了end2end用模型筛选。</p>
<h3 id="gbdt进行特征筛选组合">GBDT进行特征筛选组合</h3>
<p>决策树的每一层都在划分重要特征（划分后label纯度提高），如果决策树深度为2层则意味着抉择树挑选了两个重要特征进行特征交叉。</p>
<p>训练sample在输入GBDT的某一子树后会根据每个节点的规则落入叶子节点，把所有叶子节点组成的向量为该棵树的特征。</p>
<p>e.g.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230720173011264.png"
alt="image-20230720173011264" />
<figcaption aria-hidden="true">image-20230720173011264</figcaption>
</figure>
<h2 id="ls-plmctr">LS-PLM（CTR）</h2>
<p>参考：<a
href="https://zhuanlan.zhihu.com/p/406615820">经典推荐算法学习（四）|
阿里LS-PLM（MLR）模型原理解析 - 知乎 (zhihu.com)</a></p>
<p>在逻辑回归的基础上加入分片的思想，其灵感来自对广告推荐领域样本特点的观察。</p>
<p>举例来说，如果CTR模型要预估的是女性受众点击女装广告的CTR,那么显然，我们不希望把男性用户点击数码类产品的样本数据也考虑进来，因为这样的样本不仅与女性购买女装的广告场景毫无相关性，甚至会在模型训练过程中扰乱相关特征的权重。</p>
<h3 id="method">METHOD</h3>
<p>其实就是一个类似attention结构去捕捉用户的兴趣。</p>
<p>公式如下： <span class="math display">\[
p(y=1|x) = g(\sum_{j=1}^m\sigma(u_j^Tx)\eta(w_j^Tx))
\]</span></p>
<p><span class="math display">\[
p(y=1|x)=\sum_{i=1}^m\frac{exp(u_i^Tx)}{\sum_{j=1}^mexp(u_i^Tx)}\cdot
\frac{1}{1+exp(-w_i^Tx)}
\]</span></p>
<p>如上述公式所示，LS-PLM在表达上非常朴实，拆开来看就是常见的softmax和LR
。<span class="math inline">\(u^T,w^T\)</span>是可训练参数</p>
<p><span class="math inline">\(sigma(u_j^Tx)\)</span>
:SoftMax部分，负责将特征切分到m个不同的空间。</p>
<p><span class="math inline">\(\eta(w_j^Tx)\)</span>
:LR部分则负责对m个空间的特征分片的进行预测</p>
<p><span class="math inline">\(g(\cdot )\)</span>
:sigma函数，作用则是使得模型符合概率函数定义。</p>
<h3 id="特点">特点</h3>
<ol type="1">
<li><strong>Nonlinearity.</strong> 具备任意强非线性拟合能力；</li>
<li><strong>Sparsity.</strong>具备特征选择能力，使得模型具备稀疏性。</li>
<li><strong>Scalability.</strong>
具备从大规模稀疏数据中挖掘出具有推广性的非线性模式</li>
</ol>
<h4 id="non-linear">Non-linear</h4>
<p>通过控制分片数m，使得LS-PLM便具备拟合任意强度高维空间的非线性分类面能力。</p>
<p>如图1，假设训练数据是一个菱形分类面，基于LR的模型能做到的效果如图1.B)，LS-PLM则可以做到用4个分片完美的拟合训练集合，如图1.C)。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230720180543134.png"
alt="image-20230720180543134" />
<figcaption aria-hidden="true">image-20230720180543134</figcaption>
</figure>
<p>可以简单理解为通过前面的softmax部分把sample分到不同的LR
function去进行计算，就可以拟合出上图结果</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230720180730815.png"
alt="image-20230720180730815" />
<figcaption aria-hidden="true">image-20230720180730815</figcaption>
</figure>
<p>但m增加则容易过拟合，一般阿里选m=12.</p>
<h4 id="sparsityscalability">Sparsity+Scalability</h4>
<p>引入L1正则化进行得到稀疏解，引入L2,1正则化提高泛化能力避免过拟合</p>
<h1 id="深度学习">深度学习</h1>
<h2
id="autorec-利用自编码器对共现矩阵泛化">AutoRec-利用自编码器对共现矩阵泛化</h2>
<p>利用协同过滤中的共现矩阵，完成物品向量或者用户向量的<strong>自编码</strong>。</p>
<p>假设有m个用户n个物品，我们能得到一个（m,n)的评分矩阵。</p>
<h3 id="i-autorec">I-AutoRec</h3>
<h4 id="训练">训练</h4>
<p>对于物品i，所有m个用户对它的评分可以行程一个m维向量r，构建一个三层网络：</p>
<p><img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230720215757791.png"
alt="image-20230720215757791" /> <span class="math display">\[
h(r,\theta) = f(W\cdot g(Vr+\mu)+b)
\]</span> g,f为激活函数</p>
<p>Loss function： <span class="math display">\[
Loss=min_\theta^n||r^{(i)}-h(r^{(i)},\theta)||^2+\frac{\lambda}{2}(||W||_F^2+||V||_F^2)
\]</span></p>
<h4 id="预测">预测</h4>
<p>当输入物品i评分向量<span
class="math inline">\(r^{(i)}\)</span>时，模型输出<span
class="math inline">\(h(r^{(i)},\theta)\)</span>就是所有用户对物品i的评分，那么其中的第u维就是用户u对物品i的预测.
<span class="math display">\[
R_{ui} = (h(r,\theta))_u
\]</span> 其实就是一个泛化过程，重建函数<span
class="math inline">\(h(r,\theta)\)</span>中存储了所有数据向量的精华，经过自编码器生成的输出向量不会完全等同于输入向量，所以会具备了一定的缺失维度的预测能力。</p>
<h3 id="u-autorec">U-AutoRec</h3>
<p>把用户评分向量作为输入向量，但是用户向量稀疏性可能会影响模型效果。</p>
<h2
id="neuralcf-在cf思想上使用深度学习方法">NeuralCF-在CF思想上使用深度学习方法</h2>
<p>传统矩阵分解欠拟合，因为score层太简单</p>
<p>将矩阵分解（CF的扩展）中的scoring层用多层神经网络取代</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230725215426788.png"
alt="image-20230725215426788" />
<figcaption aria-hidden="true">image-20230725215426788</figcaption>
</figure>
<p>NeuralCF还提出了将传统矩阵分解方法和深度学习方法融合的模型：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230725220107980.png"
alt="image-20230725220107980" />
<figcaption aria-hidden="true">image-20230725220107980</figcaption>
</figure>
<p>左边MF为传统矩阵分解学习的向量，通过元素积让向量在各个维度上成分交叉（取代原来的直接内积）；</p>
<p>右边则是深度学习矩阵分解，最后将两个处理后向量连接再进行评分。</p>
<h2 id="deep-crossing模型-利用dnn自动学习特征交叉">Deep
Crossing模型-利用DNN自动学习特征交叉</h2>
<p>输入特征：</p>
<ol type="1">
<li><p>可以被处理为one-hot或者multi-hop的类别特征</p></li>
<li><p>数组特征</p></li>
<li><p>需要进一步处理的特征</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230725112028369.png"
alt="image-20230725112028369" />
<figcaption aria-hidden="true">image-20230725112028369</figcaption>
</figure></li>
</ol>
<p>模型结构：</p>
<ol type="1">
<li>Embedding层：稀疏特征向量转稠密特征向量（有的特征需要，有的特征不需要e.g.数值特征）</li>
<li>Stacking层：把特征embedding拼接在一起（concatenate）</li>
<li>Multiple Residual Units层： 多层MLP＋残差网络</li>
<li>Scoring层：根据具体任务的评分层。CTR问题二分类用逻辑回归模型，多分类用Softmax。</li>
</ol>
<p>意义：</p>
<ol type="1">
<li>无人工参加特征筛选</li>
<li>模型能自动学习特征交叉，模型越深，交叉越深</li>
</ol>
<h2 id="pnn模型-加强特征交叉能力">PNN模型-加强特征交叉能力</h2>
<p>利用乘积层（Product Layer）取代了DeepCrossing中的Stacking层。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230725221009996.png"
alt="image-20230725221009996" />
<figcaption aria-hidden="true">image-20230725221009996</figcaption>
</figure>
<p>不同特征的Embedding不再是简单的拼接，而是在Product层进行两两交互，更有针对性的获取特征之间的交互信息。</p>
<p>乘积特征交叉部分又分为内积操作IPNN和外积OPNN操作，其中OPNN在进行外积后得到的是一个矩阵，PNN把所有两两特征Embedding向量外积互操作的结果叠加进行降维。IPNN则是内积得到一个值后concatenate后变成一个向量。</p>
<p>优点：</p>
<p>加强不同特征之间交叉交互。</p>
<p>缺点：</p>
<p>OPNN中粗暴的简化操作可能会丢失信息。</p>
<p>无差别价差特征一定程度上忽略原始特征向量中包含的有价值的信息。</p>
<h2
id="widedeep-记忆能力与泛化能力的综合">Wide&amp;Deep-记忆能力与泛化能力的综合</h2>
<p>wide-让模型更有记忆能力-模型结构简单，原始数据王位可以直接影响推荐结果。</p>
<p>记忆能力可以理解为模型直接学习并利用历史数据中物品或者特征的“共现频率”能力（哪些关键特征会直接导致什么必然结果，e.g.如果点击过A则大概率会点击B）</p>
<p>deep-让模型更有泛化能力</p>
<p>泛化能力可以理解为模型传递特征的相关性以及挖掘稀疏甚至从未出现过的稀有特征与最终标签相关性的能力。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230726214649249.png"
alt="image-20230726214649249" />
<figcaption aria-hidden="true">image-20230726214649249</figcaption>
</figure>
<p>什么特征输入到Deep什么输入到Wide需要深刻理解应用场景后人工设计，例子：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230726214719704.png"
alt="image-20230726214719704" />
<figcaption aria-hidden="true">image-20230726214719704</figcaption>
</figure>
<p>所有特征都被输入到Deep去挖掘深层次关系，只有已安装应用和曝光应用这种直接粗暴对结论有重要直接影响的特征。</p>
<p>在Wide层，Geogle用的交叉积变换处理：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230726215115342.png"
alt="image-20230726215115342" />
<figcaption aria-hidden="true">image-20230726215115342</figcaption>
</figure>
<p><span
class="math inline">\(c^{ki}\)</span>是一个布尔变量，当第i个特征属于第k
个组合特征时，c的值为1，否则为0;</p>
<p>x是第i个特征的值。</p>
<p>例如，对于“AND(user_installed app=netflix.impression
app=pandora)”这个组合特征来说只有当“user installed
app=netflix和“impression_app=pandora”这两个特征同时为1时，其对应的交叉积变换层的结果才为1，否则为0。</p>
<h2 id="widecross模型">Wide&amp;Cross模型</h2>
<p>用Cross网络取代Wide&amp;Deep中原来Wide部分，Deep部分没变。</p>
<p>Cross网络能增加特征之间的交互力度</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230727160456625.png"
alt="image-20230727160456625" />
<figcaption aria-hidden="true">image-20230727160456625</figcaption>
</figure>
<p>Cross网络使用多层交叉层，假设第l层输出为<span
class="math inline">\(x_l\)</span>，那么第l+1层输出为： <span
class="math display">\[
x_{l+1}=x_0x_l^TW_l+b_l+x_l
\]</span> 这里的<span
class="math inline">\(x_0\)</span>是将所有输入特征处理好之后concatenate起来的一个向量，不同的维度代表不同的特征。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230727160827480.png"
alt="image-20230727160827480" />
<figcaption aria-hidden="true">image-20230727160827480</figcaption>
</figure>
<p>cross layer的每一层其实是将两个向量A、B矩阵相乘为一个矩阵M，<span
class="math inline">\(M_{ij}\)</span>代表A向量的第i维度与B向量第j维度相乘，这样就将两个向量的每个维度两两相乘，而向量<span
class="math inline">\(x_0\)</span>中的不同维度代表不同特征，也就是让不同特征充分交互。</p>
<p>交叉后过一个MLP层，最后再加上交叉前的l层输入，其实就是为了学习残差。</p>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>BasicTurtorial</category>
      </categories>
  </entry>
  <entry>
    <title>Relation-enhance Rec</title>
    <url>/2023/06/17/Relation-enhance_Rec/</url>
    <content><![CDATA[<p>relation-enhance KG</p>
<span id="more"></span>
<h1 id="re-kgr">RE-KGR</h1>
<p>Paper: RE-KGR: Relation-Enhanced Knowledge Graph Reasoning for
Recommendation</p>
<p>总结：把relation当做向量空间，同时考虑relation的方向性，最后基于路径概率预测</p>
<h2 id="methodology">Methodology</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617151926949.png"
alt="image-20230617151926949" />
<figcaption aria-hidden="true">image-20230617151926949</figcaption>
</figure>
<p>given a CKG</p>
<h3 id="embedding-layer">Embedding Layer</h3>
<p>for every entity and relation, one-hot to dense vector</p>
<h3 id="rgc-layer">RGC Layer</h3>
<p><strong>First-order Aggregation</strong>:</p>
<p>project each entity t to a different semantic space conditioned to
the relation r:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153022663.png"
alt="image-20230617153022663" />
<figcaption aria-hidden="true">image-20230617153022663</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153033948.png"
alt="image-20230617153033948" />
<figcaption aria-hidden="true">image-20230617153033948</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153045142.png"
alt="image-20230617153045142" />
<figcaption aria-hidden="true">image-20230617153045142</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153143790.png"
alt="image-20230617153143790" />
<figcaption aria-hidden="true">image-20230617153143790</figcaption>
</figure>
<p>Here, <span class="math inline">\(M_{r−1}\)</span>, <span
class="math inline">\(M_r\)</span> are mapping matrices, and r and r−1
are a pair of inverse relations,such as AuthorOf and WrittenBy.</p>
<p><strong>High-order Aggregation</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153235776.png"
alt="image-20230617153235776" />
<figcaption aria-hidden="true">image-20230617153235776</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153301643.png"
alt="image-20230617153301643" />
<figcaption aria-hidden="true">image-20230617153301643</figcaption>
</figure>
<p>Here, is the concatenation operator, and e(0) denotes initial
embeddings.(dense connectivity)</p>
<h3 id="local-similarity-layer">Local Similarity Layer</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153403760.png"
alt="image-20230617153403760" />
<figcaption aria-hidden="true">image-20230617153403760</figcaption>
</figure>
<h3 id="prediction-layer">Prediction Layer</h3>
<p>Predict Based on path:</p>
<p>use <span class="math inline">\(P_{UIIP}={(h,r,t)|(h,r,t)\in
G}\)</span> to describe an acyclic UIIP, the probability of the UIIP
is:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153641218.png"
alt="image-20230617153641218" />
<figcaption aria-hidden="true">image-20230617153641218</figcaption>
</figure>
<p>we use Pui to denote all acyclic UIIPs that start and end with user u
and item i.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153723353.png"
alt="image-20230617153723353" />
<figcaption aria-hidden="true">image-20230617153723353</figcaption>
</figure>
<h1 id="pern">PeRN</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230627183244905.png"
alt="image-20230627183244905" />
<figcaption aria-hidden="true">image-20230627183244905</figcaption>
</figure>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>RippleNet</title>
    <url>/2023/03/02/RippleNet/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p><strong>CF</strong>: sparsity, cold start</p>
<p><strong>KG-benefit</strong>:</p>
<ol type="1">
<li>KG introduces semantic relatedness among items, which can help find
their latent connections and improve the <em>precision</em> of
recommended items;</li>
<li>KG consists of relations with various types, which is helpful for
extending a user’s interests reasonably and increasing the
<em>diversity</em> of recommended items;</li>
<li>KG connects a user’s historical records and the recommended ones,
thereby bringing <em>explainability</em> to recommender systems.</li>
</ol>
<p><strong>Existing KG model</strong>:</p>
<ol type="1">
<li><strong>embedding-based method</strong>: DKN, CKE, SHINE, but more
suitable for in-graph applications</li>
<li><strong>path-based method</strong>: rely heavily on manually
designed meta-paths</li>
</ol>
<p>so the author proposes RippleNet:</p>
<ol type="1">
<li>combine embedding-based and path-based() methods
<ol type="1">
<li>RippleNet incorporates the KGE methods into recommendation naturally
by preference propagation;<br />
</li>
<li>RippleNet can automatically discover possible paths from an item in
a user’s history to a candidate item.</li>
</ol></li>
</ol>
<h1 id="method">Method</h1>
<p>专注于挖掘KG中用户感兴趣的实体！！</p>
<h2 id="input">Input</h2>
<p>interaction matrix <strong>Y</strong> <em>and knowledge graph</em>
<strong>G</strong></p>
<h2 id="some-definition">Some definition</h2>
<h3 id="relevant-entity">Relevant entity</h3>
<p>the set of <strong>k</strong>-hop relevant entities for user
<strong>u</strong> is defined as</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302155703683.png"
alt="image-20230302155703683" />
<figcaption aria-hidden="true">image-20230302155703683</figcaption>
</figure>
<p><span class="math inline">\(\varepsilon_u^0=V_u =
\{v|y_{uv}=1\}\)</span> is the items which the user interacts with, and
they can link with entities in knowledge graph</p>
<p>can be seen as the seed set of user u in
KG(就是user如何参与到KG中)</p>
<h3 id="ripple-set">Ripple set</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302155653732.png"
alt="image-20230302155653732" />
<figcaption aria-hidden="true">image-20230302155653732</figcaption>
</figure>
<h2 id="model">Model</h2>
<h3 id="first-layer-propagation">First layer propagation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302163505799.png"
alt="image-20230302163505799" />
<figcaption aria-hidden="true">image-20230302163505799</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302165453023.png"
alt="image-20230302165453023" />
<figcaption aria-hidden="true">image-20230302165453023</figcaption>
</figure>
<p>v: embedding of item. Item embedding can incorporate one-hot ID ,
attributes of an item, based on the application scenario.</p>
<p>r: embedding of relation between head entity and tail entity.</p>
<p>h: embedding of head entity.</p>
<p>t: embedding of tail entity.</p>
<p>attention weight <span class="math inline">\(p_i\)</span> can be
regarded as the similarity of item <strong>v</strong> and the entity
<span class="math inline">\(h_i\)</span> measured in the space of
relation <span class="math inline">\(r_i\)</span>.</p>
<p><span class="math inline">\(r_i\)</span> is important, since an
item-entity pair may have different similarities when measured by
different relations</p>
<h3 id="multi-layer">Multi-layer</h3>
<p>the second layer just replace v with <span
class="math inline">\(o_u^1\)</span></p>
<p><span class="math display">\[
p_i = softmax(o_u^{1T}R_ih_i) =
\frac{exp(o_u^{1T}T_ih_i)}{\sum_{(h,r,t)\in S_u^2}exp(o_u^{1T}Rh)}
\]</span></p>
<p><span class="math display">\[
o_u^2 = \sum_{(h_i,r_i,t_i)\in S_u^2}p_it_i
\]</span></p>
<p>and third layer replace <span class="math inline">\(o_u^1\)</span>
with <span class="math inline">\(o_u^2\)</span></p>
<p>while</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302165932080.png"
alt="image-20230302165932080" />
<figcaption aria-hidden="true">image-20230302165932080</figcaption>
</figure>
<h3 id="predict">predict</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302170052985.png"
alt="image-20230302170052985" />
<figcaption aria-hidden="true">image-20230302170052985</figcaption>
</figure>
<h3 id="whole-process">Whole process</h3>
<p><strong>Propagation only used in KG-graph</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/6C506EFAADC22D9AC38B07273F102601.png"
alt="6C506EFAADC22D9AC38B07273F102601" />
<figcaption
aria-hidden="true">6C506EFAADC22D9AC38B07273F102601</figcaption>
</figure>
<p>模型不断扩散，不断获取更高层数neighbor的信息，最后通过加在一起汇总</p>
<p>所以与曾经互动过的item有关系的实体信息（KG信息）汇总为user
embedding，最后再与没互动过的item计算估计互动概率，</p>
<p>所以是否能理解为user汇总的KG信息</p>
<h3 id="loss-function还没想明白">Loss Function（还没想明白）</h3>
<p>别人的笔记：：</p>
<p>这里的分成三个部分：分别是预测分数的交叉熵损失，知识图谱特征表示的损失，参数正则化的损失：</p>
<p>预测部分的损失很好理解，就是用户和该item之间的预测值和真实值的loss</p>
<p>知识图谱特征表示的损失：我们在计算每个阶段的加权求和时上面说了，假设前提是hR=t，这是假设，所以我们需要设一个loss让模型学习，学习的内容就是hR和t之间计算相似度后，预测0,1是否相似</p>
<p>l2正则化损失：每一个hop中h，r，t分别和自己相乘后，求和再求均值得到一个值，即为该loss（这里我理解的不是很深，有了解的可以评论区说说）</p>
<h1 id="experiment">Experiment</h1>
<h1 id="other">Other</h1>
<ol type="1">
<li><p>ripple set 可能太大，</p>
<p>在RippleNet中，我们可以对固定大小的邻居集进行采样，而不是使用完整的纹波集来进一步减少计算开销。</p></li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>Traditional Machine Learning</title>
    <url>/2023/07/10/TraditionalMachineLearning/</url>
    <content><![CDATA[<p>传统机器学习方法</p>
<span id="more"></span>
<h1 id="tree">Tree</h1>
<h2 id="single-tree">Single Tree</h2>
<h3 id="decision-tree">Decision Tree</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230712165541790.png"
alt="image-20230712165541790" />
<figcaption aria-hidden="true">image-20230712165541790</figcaption>
</figure>
<p>决策树重点：挑选重要的特征，越重要的特征放的越接近根节点。</p>
<h4 id="id3">ID3</h4>
<p>ID3算法使用Information Gain信息增益作为挑选特征指标。</p>
<h5 id="entropy-熵">Entropy-熵</h5>
<p>在数学与物理上，熵指混乱程度，或者说值得是变量X的自由度和不确定性。</p>
<p>在计算机理论中，熵度量给定数据中的杂质。 <span
class="math display">\[
Entropy(p)=\sum_{i=1}^C-p_i*log_2(p_i)
\]</span> c : 可能出现的种类数量</p>
<p><span class="math inline">\(p_i\)</span>：第i类的概率</p>
<p>熵越小，数据越纯</p>
<p>example:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230712174626803.png"
alt="image-20230712174626803" />
<figcaption aria-hidden="true">image-20230712174626803</figcaption>
</figure>
<h5 id="information-gain">Information Gain</h5>
<p>Information
Gain计算拆分前后熵的变化，即用拆分特征前的熵减去拆分特征后的平均熵。
<span class="math display">\[
Information Gain =  Entropy(entire\_dataset)-\frac{1}{n}\sum_{i=1}^n
Entropy(child\_dataset_i)
\]</span>
上面式子表示将整个数据集依照某个特征值拆分为了n个子数据集。</p>
<p>example:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230712175543982.png"
alt="也可以" />
<figcaption aria-hidden="true">也可以</figcaption>
</figure>
<p>有的教程会写作：<strong>熵</strong>减去<strong>条件熵</strong></p>
<p>简单认识一下条件熵就好，起码知道词是个啥子意思嘛</p>
<p>熵是对事件结果不确定性的度量，但在知道有些条件时，不确定性会变小。这里的条件就是数据集中的特征。</p>
<p><strong>条件熵</strong>就是已知某个特征X的情况下，事件Y的不确定性
<span class="math display">\[
Entropy(Y|X)=\sum_{i=1}^np_i Entropy(Y|X=x_i)
\]</span> 就是特征X每个可能性的结果的熵乘以发生概率的求和。 <span
class="math display">\[
Information Gain =  Entropy(Y)- Entropy(Y|X)
\]</span></p>
<h5 id="id3缺点">ID3缺点</h5>
<ol type="1">
<li>没有剪枝策略，容易过拟合。</li>
<li>只能用于处理离散分布的特征。</li>
<li>没有考虑缺失值。</li>
<li>InformationGain对可取数值数目较多的特征有所偏好，类似‘ID’的特征其信息增益接近于1。</li>
</ol>
<h4 id="c4.5">C4.5</h4>
<p>使用Information Gain
Ratio信息增益率作为挑选指标，减少因特征值类别多导致信息增益大的问题。同时引入了剪枝策略。</p>
<h5 id="information-gain-ratio">Information Gain Ratio</h5>
<p><span class="math display">\[
IGR = \frac{IG(feature,dataset)}{Entropy(feature)}
\]</span></p>
<p>Entropy(feature):
特征的固有值，即是特征的纯度，（之前求得都是label的熵）公式如下： <span
class="math display">\[
Entropy_A(dataset) = -\sum_{i=1}^np_i*log_2(p_i)
\]</span> c : A中可能出现的种类数量</p>
<p><span class="math inline">\(p_i\)</span>：A为第i类的概率</p>
<p>这能解决ID3的第四个缺点，特征可能类别越多（越细致），纯度低，熵越高，分母越大</p>
<h5 id="剪枝策略">剪枝策略</h5>
<p>剪枝缓解决策树过拟合</p>
<h6 id="预剪枝">预剪枝</h6>
<p>生成决策树过程中剪枝</p>
<ol type="1">
<li>提前设定决策树的高度，当达到这个高度时，就停止构建决策树；</li>
<li>当达到某节点的实例具有相同的label，也可以停止树的生长；</li>
<li>提前设定某个阈值，当达到某个节点的样例个数小于该阈值的时候便可以停止树的生长，但这种方法的缺点是对数据量的要求较大，无法处理数据量较小的训练样例；</li>
<li>同样是设定某个阈值，每次扩展决策树后都计算其对系统性能的增益，若小于该阈值，则让它停止生长。</li>
</ol>
<p>预剪枝的显著缺点是视野效果。因为剪枝是伴随着构建决策树同时进行的，构建者无法预知下一步可能会发生的事，会出现某种情况，在相同标准下，当前决策树不满足要求最开始的构建要求、构建者进行了剪枝，但实际上若进行进一步构建后、决策树又满足了要求。这种情况下，预剪枝会过早停止决策树的生长。</p>
<p>C4.5算法常选择后剪枝的方法消除决策树的过度拟合</p>
<h6 id="后剪枝">后剪枝</h6>
<p>在已经生成的决策树上剪枝</p>
<p>用递归的方式从低往上针对每一个非叶子节点，评估用一个叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。C4.5
通过训练数据集上的错误分类数量来估算未知样本上的错误率。</p>
<p>后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间会大的多。</p>
<h5 id="优点">优点</h5>
<ol type="1">
<li><p>解决特征值类别多导致信息增益大的问题</p></li>
<li><p>引入剪枝</p></li>
<li><p>增加对连续特征值的处理：将连续特征离散化，假设 n 个样本的连续特征
A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1
个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点；</p></li>
<li><p>增加对缺失值处理：</p>
<ol type="1">
<li><p>在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）</p>
<p>C4.5
的做法是：对于具有缺失值特征，用没有缺失的样本子集占整个数据集的比重来折算；</p></li>
<li><p>选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里）</p>
<p>C4.5
的做法是：将样本同时划分到所有子节点。不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。</p></li>
</ol></li>
</ol>
<h5 id="缺点">缺点</h5>
<ol type="1">
<li>依然不能处理回归问题</li>
<li>使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；</li>
<li>用的是多叉树，用二叉树效率更高；</li>
<li>在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。</li>
</ol>
<h3 id="cart--classification-and-regression-tree">CART- classification
and regression tree</h3>
<p>CART是决策树的延伸，也可以看作特殊的决策树。他建立起来的是二叉树，而不是多叉树。</p>
<p>可以不仅可以解决分类问题，还可以觉得回归问题</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230717124658992.png"
alt="image-20230717124658992" />
<figcaption aria-hidden="true">image-20230717124658992</figcaption>
</figure>
<h4 id="classification">Classification</h4>
<p>特征挑选指标：Gini index</p>
<p>基尼系数越小，纯度越高，特征越好。</p>
<h5 id="gini值">Gini值</h5>
<p>Gini值反应了从数据集D中随机抽取两个样本，其类别不一样的概率。 <span
class="math display">\[
Gini(D) = \sum_{i=1}^np(x_i)*(1-p(x_i))=1-\sum_{i=1}^np(x_i)^2
\]</span> <span class="math inline">\(p(x_i)\)</span>: <span
class="math inline">\(x_i\)</span>出现的概率</p>
<p>n：类别数量</p>
<h5 id="分类树的构造">分类树的构造</h5>
<p>递归生成二叉树，以Gini值最小化作为准则。</p>
<ol type="1">
<li><p>遍历所有特征<span class="math inline">\(A\)</span>,遍历特征<span
class="math inline">\(A\)</span>所有的可能取值<span
class="math inline">\(a\)</span>，根据特征A
是否取某一可能值a，把样本D分成两部分<span
class="math inline">\(D_1\)</span>和<span
class="math inline">\(D_2\)</span>。计算Gini值，将Gini值最小的<span
class="math inline">\(A,a\)</span>作为最优特征与最优切分点。 <span
class="math display">\[
D_1 = (x,y)\in D |A(x)=a,D_2 = D-D_1
\]</span></p>
<p><span class="math display">\[
Gini(D|A=a) = \frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
\]</span></p></li>
<li><p>循环1步骤</p></li>
</ol>
<h4 id="regression">Regression</h4>
<p>递归生成二叉树，使用平方误差最小化作为准则。</p>
<p>假设有n个样本<span
class="math inline">\(D={(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)}\)</span>,其中x是d维向量，代表d个特征，y是连续值。</p>
<p>CART回归树生成算法：</p>
<ol type="1">
<li><p>遍历所有特征<span
class="math inline">\(d_i\)</span>,遍历n个样本在<span
class="math inline">\(d_i\)</span>特征的取值x，取相邻两样本值x(排序后)的平均数s作为划分点，利用s作为划分两个区域的阈值：
<span class="math display">\[
R_1 = \{y_i|x_{i,d_i}&lt;=s\},R_2 = \{y_i|x_{i,d_i}&gt;s\}
\]</span> 每个区域内样本<span
class="math inline">\(y_i\)</span>的均值作为该区域的预测值<span
class="math inline">\(c_1,
c_2\)</span>，计算样本真实值和预测值的平方误差之和,并找到能使平方误差最小的<span
class="math inline">\(d_i\)</span>和<span
class="math inline">\(s\)</span>: <span class="math display">\[
c_1 = \frac{1}{n}\sum_{x_i\in R_1}y_i,c_2 = \frac{1}{n}\sum_{x_i\in
R_2}y_i,
\]</span></p>
<p><span class="math display">\[
\min_{d_i,s}\{\sum_{x_i\in R_1(d_i,s)}(y_i-c_1)^2+\sum_{x_i\in
R_2(d_i,s)}(y_i-c_2)^2\}
\]</span></p></li>
<li><p>重复1直到满足停止条件</p></li>
<li><p>最后划分为M区域<span
class="math inline">\(R_1,R_2,\cdots,R_M\)</span>,生成决策树,预测值为：
<span class="math display">\[
f(x)=\sum_{i=1}^Mc_iI(x\in R_i)
\]</span> 这里的<span class="math inline">\(I(x\in
R_i)\)</span>值，满足条件返回值为1，否则为0</p></li>
</ol>
<h2 id="several-tree">Several tree</h2>
<h3 id="bagged-decision-tree">Bagged Decision Tree</h3>
<p>单棵的决策树高方差（不同训练集训练的预测结果差很多，不稳定），Bagged
Decisoin Tree要解决这个问题。</p>
<p>使用Bootstrap生成多个训练集。它基于有放回地从原始训练数据集中抽取样本，构成一个新的训练集。通过对原始数据集的多次重采样，可以生成多个不同的训练集，每个训练集都是原始数据集的一部分。接着将每轮未抽取的数据合并形成<strong>袋外数据集</strong>（Out
of Bag, OOB），用于模型中的测试集。</p>
<p>使用Bagging进行训练和预测。通过在 Bootstrap
生成的多个训练集上训练多个独立的基础模型，并将它们的预测结果进行平均或投票来进行最终预测。每个基础模型都是独立训练的，它们之间没有依赖关系。Bagging
可以减少模型的方差，提高模型的稳定性和泛化能力。</p>
<h3 id="random-forest">Random Forest</h3>
<p>随机森林使用的方法与Bagged Decision
Tree一样使用Bootstrap和Bagging方法，但做出了改进。</p>
<p>决策树使用的是贪心策略，每一步都是局部最优解，但每次的局部最优解不一定能得到全局最优解。而且Bagging后，我们可能会得到几个非常相似的局部最优解树。</p>
<p>所以随机森林加入随机策略，鲁棒性会更强。</p>
<h4 id="构建随机森林">构建随机森林</h4>
<ol type="1">
<li>使用Bootstrap策略，有放回的抽取sample生成N个训练集，每个训练集可以训练出一个子模型。</li>
<li>训练单棵决策树
<ol type="1">
<li>每个sample有M个特征，在决策树的每个节点需要分裂的时候，随机从M个特种中选取m个特征，满足条件m
&lt;&lt;
M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。（这样让决策树不要过分关心一个重要特征，鲁棒性更强）</li>
<li>循环1步骤</li>
</ol></li>
</ol>
<h3 id="gbdt">GBDT</h3>
<p>使用Boosting思想，以决策树为基函数的提升方法称为提升决策树。保证每棵树都是低方差，高偏差。</p>
<p><a
href="https://zhuanlan.zhihu.com/p/280222403">GBDT的原理、公式推导、Python实现、可视化和应用
- 知乎 (zhihu.com)</a></p>
<h4 id="算法">算法</h4>
<p>!<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230720171728041.png"
alt="image-20230720171728041" />]</p>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>常见激活函数</title>
    <url>/2023/02/28/activate_function/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="常见激活函数">常见激活函数</h1>
<p>激活函数作用：加入非线性因素</p>
<h2 id="sigmoid">Sigmoid</h2>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1+exp(-x)}
\]</span></p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228224935533.png" alt="image-20230228224935533" style="zoom:40%;" /></p>
<p>输出的值范围在[0,1]之间。但是<code>sigmoid</code>型函数的输出存在<strong>均值不为0</strong>的情况，并且存在<strong>梯度消失的问题</strong>，在深层网络中被其他激活函数替代。在<strong>逻辑回归</strong>中使用的该激活函数用于输出<strong>分类</strong>。</p>
<h3 id="求导公式">求导公式</h3>
<p>链式法则</p>
<h3 id="梯度消失原因">梯度消失原因：</h3>
<p><span class="math display">\[
\sigma&#39;(x) = \sigma\space \cdot (1-\sigma)
\]</span></p>
<ol type="1">
<li>sigmoid函数两边的斜率趋向0，很难继续学习</li>
<li>sigmoid导数两个部分都小于1，在深层神经网络中，靠前layer参数会因为后面多层sigmoid导数叠加（链式法则）导致更新的特别慢。</li>
</ol>
<h3 id="缺点解决办法">缺点解决办法</h3>
<ol type="1">
<li>在深层网络中被其他激活函数替代。如<code>ReLU(x)</code>、<code>Leaky ReLU(x)</code>等</li>
<li>在分类问题中，sigmoid做激活函数时，使用交叉熵损失函数替代均方误差损失函数。</li>
<li>采用正确的权重初始化方法（让初始化的数据尽量不要落在梯度消失区域）</li>
<li>加入BN层（同上，避免数据落入梯度消失区）</li>
<li>分层训练权重</li>
</ol>
<h2 id="tanh">tanh</h2>
<p><span class="math display">\[
tanh(x) = \frac{e^x-e^{(-x)}}{e^x+e^{(-x)}} =\frac{e^{2x}-1}{e^{2x}+1}=
2 \cdot sigmoid(2x)-1
\]</span></p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194241514.png" alt="image-20230307194241514" style="zoom:67%;" /></p>
<p><code>tanh(x)</code>型函数可以解决<code>sigmoid</code>型函数的<strong>期望（均值）不为0</strong>的情况。函数输出范围为(-1,+1)。但<code>tanh(x)</code>型函数依然存在<strong>梯度消失的问题</strong>。</p>
<p>在LSTM中使用了<code>tanh(x)</code>型函数。</p>
<h2 id="relu">Relu</h2>
<p><code>ReLU(x)</code>型函数可以有效避免<strong>梯度消失的问题</strong>，公式如下：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228222815687.png"
alt="image-20230228222815687" />
<figcaption aria-hidden="true">image-20230228222815687</figcaption>
</figure>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194352199.png" alt="image-20230307194352199" style="zoom:67%;" /></p>
<p><code>ReLU(x)</code>型函数的缺点是<strong>负值成为“死区”</strong>，神经网络无法再对其进行响应。Alex-Net使用了<code>ReLU(x)</code>型函数。当我们训练深层神经网络时，最好使用<code>ReLU(x)</code>型函数而不是<code>sigmoid(x)</code>型函数。</p>
<p>ReLU梯度稳定，值还比sigmoid大，所以<strong>可以加快网络训练</strong>。</p>
<p>但是要注意，我们在输入图像时就要注意，应该使用Min-Max归一化，而不能使用Z-score归一化。（避免进入死区）</p>
<h3 id="在0点不可导">在0点不可导</h3>
<p>人为将梯度规定为0（源码就是这么写的）</p>
<h2 id="relu6">Relu6</h2>
<p>Relu的正值输出是[0，无穷大]，但计算机内存优先，所以限定relu最大值为6</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194457525.png" alt="image-20230307194457525" style="zoom:67%;" /></p>
<h2 id="leakyrelu">LeakyRelu</h2>
<p>为<strong>负值增加了一个斜率</strong>，缓解了“死区”现象，公式如下：</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194659126.png" alt="image-20230307194659126" style="zoom:67%;" /></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228222900735.png"
alt="image-20230228222900735" />
<figcaption aria-hidden="true">image-20230228222900735</figcaption>
</figure>
<p><code>Leaky ReLU(x)</code>型函数缺点是，<strong>超参数a（阿尔法）合适的值不好设定</strong>。当我们想让神经网络能够学到负值信息，那么使用该激活函数。</p>
<h2 id="p-relu-参数化relu">P-Relu 参数化Relu</h2>
<p>数化ReLU（P-ReLU）。参数化ReLU为了解决超参数a（阿尔法）合适的值不好设定的问题，干脆将这个参数也融入模型的整体训练过程中。也使用误差反向传播和随机梯度下降的方法更新参数。</p>
<h2 id="r-relu-随机化relu">R-Relu 随机化Relu</h2>
<p>就是超参数a（阿尔法）随机化，<strong>让不同的层自己学习不同的超参数</strong>，但随机化的超参数的分布符合均值分布或高斯分布。</p>
<h2 id="mish激活函数">Mish激活函数</h2>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194824945.png" alt="image-20230307194824945" style="zoom:67%;" />
<span class="math display">\[
Mish(x) = x\cdot tanh(log(1+e^x))
\]</span></p>
<p>在负值中，允许有一定的梯度流入。</p>
<h2 id="elu指数化线性单元">ELU指数化线性单元</h2>
<p>也是为了解决死区问题，公式如下：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194918301.png"
alt="image-20230307194918301" />
<figcaption aria-hidden="true">image-20230307194918301</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228224119801.png"
alt="image-20230228224119801" />
<figcaption aria-hidden="true">image-20230228224119801</figcaption>
</figure>
<p>缺点是<strong>指数计算量大</strong>。</p>
<h2 id="maxout">Maxout</h2>
<p>就是用一个MLP层作为激活函数。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307195003327.png"
alt="image-20230307195003327" />
<figcaption aria-hidden="true">image-20230307195003327</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307195013184.png"
alt="image-20230307195013184" />
<figcaption aria-hidden="true">image-20230307195013184</figcaption>
</figure>
<p>与常规的激活函数不同，<strong>Maxout</strong>是一个可以学习的<strong>分段线性函数</strong>。其原理是，任何ReLU及其变体等激活函数都可以看成分段的线性函数，而Maxout加入的一层神经元正是一个可以学习参数的分段线性函数。</p>
<p>优点是其拟合能力很强，理论上可以拟合任意的凸函数。缺点是参数量激增！在Network-in-Network中使用的该激活函数。</p>
<h1 id="softmax求导">Softmax求导</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307195822249.png"
alt="image-20230307195822249" />
<figcaption aria-hidden="true">image-20230307195822249</figcaption>
</figure>
<p>要结合交叉熵loss函数考虑</p>
<p><span class="math inline">\(\frac{dL}{dz}=\frac{dL}{da}\cdot
\frac{da}{dz}\)</span></p>
<p>假设第j个类别是正确的，<span
class="math inline">\(y_j=1\)</span>,其它为0</p>
<p><span class="math inline">\(L = -\sum_{i=1}^ny_iln(a_i)\)</span></p>
<p><span class="math inline">\(\frac{dL}{da} =
-y_iln(a_j)=-ln(a_j)\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307200559541.png"
alt="image-20230307200559541" />
<figcaption aria-hidden="true">image-20230307200559541</figcaption>
</figure>
<p>所以最终Loss只跟label类别有关</p>
<p>所以当i=j：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307201705230.png"
alt="image-20230307201705230" />
<figcaption aria-hidden="true">image-20230307201705230</figcaption>
</figure>
<p>当i!=j:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307201737792.png"
alt="image-20230307201737792" />
<figcaption aria-hidden="true">image-20230307201737792</figcaption>
</figure>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>算法</title>
    <url>/2023/02/13/algorithm/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="二叉树">二叉树</h1>
<h2 id="完全二叉树">完全二叉树</h2>
<ol type="1">
<li>树的深度=一直遍历最左节点的长度</li>
<li>左子树深度==右子树深度，左子树是全满的完全二叉树，如果左子树深度大于右子树深度，右子树是全满的完全二叉树</li>
</ol>
<h2 id="平衡二叉树">平衡二叉树</h2>
<p>所有节点左右子树高度不大于1</p>
<h1 id="字典序">字典序</h1>
<p>就是按照字典排列顺序，英文字母按下面方式排列：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ABCDEFG HIJKLMN OPQRST UVWXYZ</span><br><span class="line">abcdefg hijklmn opqrst uvwxyz</span><br></pre></td></tr></table></figure>
<h1 id="回溯">回溯</h1>
<h2 id="显回溯">显回溯</h2>
<p>pre是公共变量，遍历完这种情况要记得pop（），加入结果时记得copy（）</p>
<h2 id="隐回溯">隐回溯</h2>
<p>pre是函数内传递的变量，直接传就ok</p>
<h2 id="去重">去重</h2>
<h3 id="去res中重复元素">去res中重复元素</h3>
<ol type="1">
<li><p>排序数组</p>
<p>可以用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">num[i]=num[i-1]跳过同层重复元素的选取（注意：子层不跳过）</span><br></pre></td></tr></table></figure></li>
<li><p>set去重</p></li>
</ol>
<h3 id="排序问题去重">排序问题去重</h3>
<p>使用used数组</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">used=[True, ..., False, False]</span><br></pre></td></tr></table></figure>
<h1 id="位运算">位运算</h1>
<h2 id="基础操作">基础操作</h2>
<h3 id="取反">取反</h3>
<p>异或操作，要取反的区域为1，不取反区域为0</p>
<h1 id="优先队列">优先队列</h1>
<ol type="1">
<li><p>使用heapq实现</p>
<p><a
href="https://docs.python.org/2/library/heapq.html#basic-examples">8.4.
heapq — Heap queue algorithm — Python 2.7.18 documentation</a></p></li>
</ol>
<p>​ 只能实现最小堆，通过再所有元素前加-号这个trick可实现最大堆</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pq=[]</span><br><span class="line">heapq.heappush(pq,a)</span><br><span class="line">heapq.heappop(pq)</span><br></pre></td></tr></table></figure>
<p>​ 优先队列的元素可以是tuple</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; h = []</span><br><span class="line">&gt;&gt;&gt; heappush(h, (5, &#x27;write code&#x27;))</span><br><span class="line">&gt;&gt;&gt; heappush(h, (7, &#x27;release product&#x27;))</span><br><span class="line">&gt;&gt;&gt; heappush(h, (1, &#x27;write spec&#x27;))</span><br><span class="line">&gt;&gt;&gt; heappush(h, (3, &#x27;create tests&#x27;))</span><br><span class="line">&gt;&gt;&gt; heappop(h)</span><br><span class="line">(1, &#x27;write spec&#x27;)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>Hyper Knowledge Graph</title>
    <url>/2023/06/17/hyper-knowledgegraph/</url>
    <content><![CDATA[<p>Hypergraph knowledge graph</p>
<span id="more"></span>
<h1 id="khnn">KHNN</h1>
<p>Paper: Knowledge-Aware Hypergraph Neural Network for Recommender
Systems</p>
<p>总结：用CKAN方法表示user和item，用hyperedge将l-hop的node全部连在一起，用（l-1）hop和l-hop
concate卷积计算出l-hop节点的权重与l-hop节点相乘，再做conv最后得到该层的一维embedding，然后再aggregation</p>
<h2 id="methodology">Methodology</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617154902916.png"
alt="image-20230617154902916" />
<figcaption aria-hidden="true">image-20230617154902916</figcaption>
</figure>
<h3 id="knowledge-aware-hypergraph-construction">Knowledge-Aware
Hypergraph Construction</h3>
<p><strong>Initial Hyperedge Construction</strong></p>
<p>use user’s interacted items to represent user u</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617155650437.png"
alt="image-20230617155650437" />
<figcaption aria-hidden="true">image-20230617155650437</figcaption>
</figure>
<p>use items, which have been watched by the same user, to construct the
initial item set of item v</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617155746509.png"
alt="image-20230617155746509" />
<figcaption aria-hidden="true">image-20230617155746509</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617155754397.png"
alt="image-20230617155754397" />
<figcaption aria-hidden="true">image-20230617155754397</figcaption>
</figure>
<p><strong>Knowledge Hyperedge Construction</strong></p>
<p>让l-hop neighbor 与(l-1)-hop
neighbor在相连，即所有节点被一条hyper-edge相连，主要服务于下面的neighborhood
convolution</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617161110960.png"
alt="image-20230617161110960" />
<figcaption aria-hidden="true">image-20230617161110960</figcaption>
</figure>
<h3 id="knowledge-aware-hypergraph-convolution"><strong>Knowledge-Aware
Hypergraph Convolution</strong></h3>
<p><strong>Neighborhood Convolution</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617162200596.png"
alt="image-20230617162200596" />
<figcaption aria-hidden="true">image-20230617162200596</figcaption>
</figure>
<ol type="1">
<li><p>learn the transform matrix T from the entity vectors in both
l-order and l-1-order hyperedges for vector permutation and
weighting(entity vectors in l-order) . use 1-d conv to generate T , use
another 1-d conv to aggregate the transformed vectors.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617162545551.png"
alt="image-20230617162545551" />
<figcaption aria-hidden="true">image-20230617162545551</figcaption>
</figure>
<p><em>conv</em>1 and <em>conv</em>2 are 1-dimension convolution but
withdifffferent out channels.</p></li>
<li><p>for the initial hyper-edge</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617162625945.png"
alt="image-20230617162625945" />
<figcaption aria-hidden="true">image-20230617162625945</figcaption>
</figure></li>
<li><p>add item v information</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617162651714.png"
alt="image-20230617162651714" />
<figcaption aria-hidden="true">image-20230617162651714</figcaption>
</figure></li>
<li><p>combine</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617162708343.png"
alt="image-20230617162708343" />
<figcaption aria-hidden="true">image-20230617162708343</figcaption>
</figure></li>
<li><p>aggregation</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617162731699.png"
alt="image-20230617162731699" />
<figcaption aria-hidden="true">image-20230617162731699</figcaption>
</figure></li>
</ol>
<h1 id="lgcl">LGCL</h1>
<p>Paper：Line Graph Contrastive Learning for Link Prediction</p>
<h2 id="methodology-1">Methodology</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617174642449.png"
alt="image-20230617174642449" />
<figcaption aria-hidden="true">image-20230617174642449</figcaption>
</figure>
<h1 id="hpr">HPR</h1>
<p>Paper: Empowering Knowledge Graph Construction with Hyper-graph for
Personalized Recommendation</p>
<p>basic idea: You might like something that someone with similar
preferences likes you.</p>
<p>总结：将相似度高的user作为hyper-edge（本质上是扩展了target
user的1-hop neighbor），通过计算user的l-hop neighbor与target
item的相似度来计算出user的embedding</p>
<h2 id="methodology-2">Methodology</h2>
<p>we would like to calculate the probability of <span
class="math inline">\(u_1\)</span> will interact with <span
class="math inline">\(i\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617144000884.png"
alt="image-20230617144000884" />
<figcaption aria-hidden="true">image-20230617144000884</figcaption>
</figure>
<h3 id="hyper-graph-learning">Hyper-graph learning</h3>
<ol type="1">
<li>adopt the cosine similarity to estimate the relevance between
users.</li>
<li>select some users with the highest similarity as the
hyper-edge.</li>
</ol>
<p>这里假设u1与u2最为相似，所以将u1、u2成为一条hypergraph</p>
<h3 id="knowledge-graph-construction">Knowledge Graph Construction</h3>
<p>given:</p>
<p><span class="math inline">\(\vec{i}\)</span>: the embedding of item
i</p>
<p><span class="math inline">\(S_{u_l}^1\)</span>​ : the l-hop neighbor
of user1, which takes the entities which with implicit interaction
behaviour with users as head entities.</p>
<p><span class="math inline">\(S_{u_2}^l\)</span> : the l-hop neighbor
of user2, because there is a hyperedge consist of u1 and u2</p>
<p>1-hop cal:</p>
<p>gain information between item-i and the 1-hop neighbor of user</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617151011714.png"
alt="image-20230617151011714" />
<figcaption aria-hidden="true">image-20230617151011714</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617150959919.png"
alt="image-20230617150959919" />
<figcaption aria-hidden="true">image-20230617150959919</figcaption>
</figure>
<p><span class="math inline">\(N_u^1:S^1_{u_1} \or S^1_{u_2}\)</span>
user one-hop neighbor (include hyperedge)</p>
<p>multi-hop cal: get <span class="math inline">\(q_u^{l}\)</span></p>
<p>the user embedding:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617151325200.png"
alt="image-20230617151325200" />
<figcaption aria-hidden="true">image-20230617151325200</figcaption>
</figure>
<p>predict:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617151352536.png"
alt="image-20230617151352536" />
<figcaption aria-hidden="true">image-20230617151352536</figcaption>
</figure>
<h1 id="hype">HypE</h1>
<p>Paper:Knowledge Hypergraphs: Extending Knowledge Graphs Beyond Binary
Relations</p>
<p>score： convolution-based embedding method for knowledge
hypergraph</p>
<p>总结：做KG图的连接预测，无GNN方法，主要是embedding计算方法。考虑到entity在triple中的i个位置，在这个位置有训练出来的filter，对embbeding进行转换，最后计算概率score，</p>
<p>计算成本较低。</p>
]]></content>
      <categories>
        <category>GNN</category>
        <category>hypergraph</category>
      </categories>
  </entry>
  <entry>
    <title>常见损失函数</title>
    <url>/2023/02/28/loss_function/</url>
    <content><![CDATA[<p>常见损失函数及常见问题</p>
<span id="more"></span>
<h1 id="常见损失函数">常见损失函数</h1>
<p><strong>损失函数</strong>用来评价模型的<strong>预测值</strong>和<strong>真实值</strong>不一样的程度，在模型正常拟合的情况下，损失函数值越低，模型的性能越好。不同的模型用的损失函数一般也不一样。</p>
<p><strong>损失函数</strong>分为<strong>经验风险损失函数</strong>和<strong>结构风险损失函数</strong>。经验风险损失函数指<strong>预测结果</strong>和<strong>实际结果</strong>的差值，结构风险损失函数是指<strong>经验风险损失函数</strong>加上<strong>正则项</strong>。</p>
<h2 id="常用">常用</h2>
<h3 id="用于回归">用于<strong>回归</strong>：</h3>
<h4 id="绝对值损失函数">绝对值损失函数</h4>
<p><span class="math display">\[
L(Y,f(x)) = |Y-f(x)|
\]</span></p>
<h4 id="平方损失函数">平方损失函数</h4>
<p><span class="math display">\[
L(Y,f(x)) = (Y-f(x))^2
\]</span></p>
<p>对n个数据求平方损失后加和求平均叫<strong>均方误差MSE</strong>，常在<strong>线性回归</strong>使用
<span class="math display">\[
\frac{1}{N}\sum_n(Y-f(x))^2
\]</span></p>
<h3 id="用于分类">用于分类</h3>
<h4 id="损失函数zero-one-loss">0-1损失函数（zero-one loss）</h4>
<p><span class="math display">\[
L(Y,f(x)) = \left\{
\begin{array}{rcl}
1   &amp;   &amp;{Y!=f(x)}\\
0   &amp;   &amp;{Y=f(x)}
\end{array} \right.
\]</span></p>
<p>非黑即白，过于严格，用的很少，比如<strong>感知机</strong>用。</p>
<p>可通过设置阈值放宽条件 <span class="math display">\[
L(Y,f(x)) = \left\{
\begin{array}{rcl}
1   &amp;   &amp;{|Y-f(x)&gt;=T}\\
0   &amp;   &amp;{|Y-f(x)&lt;T}
\end{array} \right.
\]</span></p>
<h4 id="对数损失函数log-loss">对数损失函数（log loss）</h4>
<p><span class="math display">\[
L(Y,P(Y|X)) = -logP(Y|X)
\]</span></p>
<p>Y为真实分类，<span
class="math inline">\(P(Y|X)\)</span>为X条件下分类为Y的概率。用于最大似然估计，等价于交叉熵损失函数</p>
<p>加负号原因：习惯在模型更准确的情况下，loss函数越小</p>
<p>加log原因：这和最大（极大）似然估计有关，对数损失是用于最大似然估计的。</p>
<p><strong>最大似然估计</strong>：<strong>利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值</strong>。</p>
<p>我们假定一组参数（<span
class="math inline">\(\Theta\)</span>）在一堆数据（样本结果<span
class="math inline">\(x_1,x_2...\)</span>）下的<strong>似然值</strong>为<code>P(θ|x1,x2,...,xn)=P(x1|θ)*P(x2|θ)*...*P(xn|θ)</code>，可以看出来，似然值等于每一条数据在这组参数下的条件概率<strong>之积</strong>。求概率是<strong>乘性</strong>，而求损失是<strong>加性</strong>，所以才需要借助log（对数）来<strong>转积为和</strong>，另一方面也是为了简化运算。</p>
<p>对数损失在<strong>逻辑回归</strong>和<strong>多分类任务</strong>上广泛使用。交叉熵损失函数的标准型就是对数损失函数，本质没有区别。</p>
<h4 id="交叉熵损失函数">交叉熵损失函数</h4>
<p>双分类： <span class="math display">\[
L(Y,f(x)) = -[Ylnf(x)+(1-y)ln(1-f(x))]
\]</span> 多分类： <span class="math display">\[
L(Y,f(x)) = -Ylnf(x)
\]</span></p>
<h4 id="合页损失函数hinge-loss">合页损失函数(hinge loss)</h4>
<p><span class="math display">\[
L(Y,f(x)) = max(0, 1-Y\cdot f(x))
\]</span></p>
<p>SVM就是使用的合页损失，还加上了正则项。公式意义是，当样本被正确分类且函数间隔大于1时，合页损失是0，否则损失是<span
class="math inline">\(1-Y\cdot f(x)\)</span>.</p>
<p>SVM中<span class="math inline">\(Y\cdot
f(x)\)</span>为函数间隔，对于函数间隔：</p>
<ol type="1">
<li><p>正负</p>
<p>当样本被正确分类时，<span class="math inline">\(Y\cdot
f(x)&gt;0\)</span>；当样本被错误分类时，<span
class="math inline">\(Y\cdot f(x)&lt;0\)</span>。</p></li>
<li><p>大小</p>
<p><span class="math inline">\(Y\cdot
f(x)\)</span>的绝对值代表样本距离决策边界的远近程度。<span
class="math inline">\(Y\cdot
f(x)\)</span>的绝对值越大，表示样本距离决策边界越远。因此，我们可以知道：</p></li>
</ol>
<p>​ 当<span class="math inline">\(Y\cdot f(x)&gt;0\)</span>时，<span
class="math inline">\(Y\cdot
f(x)\)</span>的绝对值越大表示决策边界对样本的区分度越好</p>
<p>​ 当<span class="math inline">\(Y\cdot f(x)&lt;0\)</span>时，<span
class="math inline">\(Y\cdot
f(x)\)</span>的绝对值越大表示决策边界对样本的区分度越差</p>
<h4 id="指数损失函数exponential-loss">指数损失函数(exponential
loss)</h4>
<p><span class="math display">\[
L(Y,f(x)) = exp(-Y\cdot f(x)) = \frac{exp(f(x))}{exp(Y)}
\]</span></p>
<p>常用于AdaBoost算法，</p>
<p><strong>那么为什么AdaBoost算法使用指数损失函数，而不使用其他损失函数呢？</strong></p>
<p>这是因为，当<strong>前向分步算法的损失函数是指数损失函数</strong>时，其学习的具体操作等价于AdaBoost算法的学习过程。</p>
<h3 id="用于分割">用于分割</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230221201538734.png"
alt="image-20230221201538734" />
<figcaption aria-hidden="true">image-20230221201538734</figcaption>
</figure>
<h3 id="用于检测">用于检测</h3>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230228205041367.png"
alt="image-20230228205041367" />
<figcaption aria-hidden="true">image-20230228205041367</figcaption>
</figure>
<h1 id="常见损失函数问题">常见<strong>损失函数问题</strong></h1>
<h2 id="交叉熵相关">交叉熵相关</h2>
<h3
id="交叉熵函数与最大似然函数的联系和区别">交叉熵函数与最大似然函数的联系和区别？</h3>
<p><strong>区别</strong>：</p>
<p><strong>交叉熵函数</strong>使用来描述模型预测值和真实值的差距大小，越大代表越不相近；</p>
<p><strong>极大似然</strong>就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！即“模型已定，参数未知”</p>
<p><strong>联系</strong>：</p>
<p><strong>交叉熵函数</strong>可以由<strong>最大似然函数</strong>在<strong>伯努利分布</strong>的条件下推导出来，或者说<strong>最小化交叉熵函数</strong>的本质就是<strong>对数似然函数的最大化</strong>。</p>
<p><img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/IMG_0115(20230224-203706).PNG" /></p>
<h3
id="在用sigmoid作为激活函数的时候为什么要用交叉熵损失函数而不用均方误差损失函数">在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？</h3>
<p>另一个问法其实是在分类问题中为什么不用均方误差做损失函数。</p>
<ol type="1">
<li><p><strong>sigmoid</strong>作为激活函数的时候，如果采用<strong>均方误差损失函数</strong>，那么这是一个<strong>非凸优化</strong>问题，不宜求解。而采用<strong>交叉熵损失函数</strong>依然是一个<strong>凸优化</strong>问题，更容易优化求解。（凸优化问题中局部最优解同时也是全局最优解）。而且<span
class="math inline">\(\frac{dL}{dW}\)</span>中，有地方为0，如果参数刚好导致<span
class="math inline">\(\frac{dL}{dW}\)</span>为0，参数就不会更新。</p>
<p><img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224210154928.png" /></p></li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224211215148.png"
alt="image-20230224211215148" />
<figcaption aria-hidden="true">image-20230224211215148</figcaption>
</figure>
<ol start="2" type="1">
<li>因为<strong>交叉熵损失函数</strong>可以<strong>完美解决平方损失函数权重更新过慢</strong>的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。</li>
</ol>
<p>​ 方损失函数权重更新过慢原因：</p>
<p>​ 梯度更新公式为：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224211408952.png"
alt="image-20230224211408952" />
<figcaption aria-hidden="true">image-20230224211408952</figcaption>
</figure>
<p>这里a是预测值，y是实际值</p>
<p>有<span
class="math inline">\(\sigma&#39;(z)\)</span>这一项而sigmoid函数两端梯度很小，导致参数更新缓慢。</p>
<p>而交叉熵函数不会有这个问题虽然有<span
class="math inline">\(\sigma(z)\)</span>但没有<span
class="math inline">\(\sigma&#39;(z)\)</span>,求导detail如下：</p>
<details>
<img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224211928602.png" >
</details>
<h3 id="交叉熵和均分函数区别">交叉熵和均分函数区别</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224212504307.png"
alt="image-20230224212504307" />
<figcaption aria-hidden="true">image-20230224212504307</figcaption>
</figure>
<h3 id="如何推导出交叉熵函数">如何推导出交叉熵函数</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224215208050.png"
alt="image-20230224215208050" />
<figcaption aria-hidden="true">image-20230224215208050</figcaption>
</figure>
<h3 id="为什么交叉熵函数有log项">为什么交叉熵函数有log项</h3>
<p>第一种：因为是公式推导出来的，比如第六题的推导，推导出来的有log项。</p>
<p>第二种：通过最大似然估计的方式求得交叉熵公式，这个时候引入log项。这是因为似然函数（概率）是乘性的，而loss函数是加性的，所以需要引入log项“<strong>转积为和</strong>”。而且也是为了<strong>简化运算</strong>。</p>
<h3 id="交叉熵的设计思想">交叉熵的设计思想</h3>
<p><strong>交叉熵函数</strong>的本质是对数函数。</p>
<p><strong>交叉熵函数</strong>使用来描述模型预测值和真实值的差距大小，越大代表越不相近。</p>
<p><strong>交叉熵损失函数</strong>可以<strong>完美解决平方损失函数权重更新过慢</strong>的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。</p>
<p>对数损失在<strong>逻辑回归</strong>和<strong>多分类任务</strong>上广泛使用。交叉熵损失函数的标准型就是对数损失函数，本质没有区别。</p>
<h2 id="cv相关">CV相关</h2>
<h3 id="yolo损失函数">Yolo损失函数</h3>
<p>Yolo是用于模板检测的模型</p>
<p>Yolo的损失函数由四部分组成：</p>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823639975/8B2446F6E2BC3932829E4B801BDBDF05"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测的中心坐标做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823762782/488A1D20613F3E03B97A925F2C63D9AF"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测边界框的宽高做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823867484/DE22034D2077B5200B2C5440D47249FC"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测的类别做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823980181/9CE8A218F55F619B9EAEBCDFCFBF6446"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测的置信度做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618824075778/79B60A7E11ACBF428FD0510200949CFC"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>我们发现每一项loss的计算都是L2
loss（平方差），即使是分类问题也是。所以说yolo是把<strong>分类</strong>问题转为了<strong>回归</strong>问题。</p>
<h3 id="iou与miou计算">IOU与MIOU计算</h3>
<p>IOU（Intersection over Union），交集占并集的大小。</p>
<figure>
<img
src="https://www.nowcoder.com/equation?tex=%0A%20%20IOU%3DJaccard%20%3D%5Cfrac%7B%7CA%5Ccap%20B%7C%7D%20%7B%7CA%5Ccup%20B%7C%7D%3D%5Cfrac%7B%7CA%5Ccap%20B%7C%7D%20%7B%7CA%7C%2B%7CB%7C-%7CA%5Ccap%20B%7C%7D%20%5C%5C%0A%20%20%5Ctag%7B.%7D%0A%20%20&amp;preview=true"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>mIOU一般都是基于类进行计算的，将每一类的IOU计算之后累加，再进行平均，得到的就是mIOU。</p>
<h2 id="其它">其它</h2>
<h3 id="kl散度">KL散度</h3>
<p>相对熵（relative
entropy），又被称为Kullback-Leibler散度（Kullback-Leibler
divergence）或信息散度（information
divergence），是<strong>两个概率分布（probability
distribution）间差异的非对称性度量</strong>
。在信息理论中，<strong>相对熵等价于两个概率分布的信息熵（Shannon
entropy）的差值</strong>。</p>
<p>设<img
src="https://www.nowcoder.com/equation?tex=P(x)&amp;preview=true"
alt="img" />，<img
src="https://www.nowcoder.com/equation?tex=Q(x)&amp;preview=true"
alt="img" />是随机变量<img
src="https://www.nowcoder.com/equation?tex=X&amp;preview=true"
alt="img" />上的两个概率分布，则在离散和连续随机变量的情形下，相对熵的定义分别为：</p>
<figure>
<img
src="https://www.nowcoder.com/equation?tex=%0AKL(P%7C%7CQ)%3D%5Csum%7BP(x)log%20%5Cfrac%7BP(x)%7D%7BQ(x)%7D%7D%20%5C%5C%0AKL(P%7C%7CQ)%3D%5Cint%7BP(x)log%20%5Cfrac%7BP(x)%7D%7BQ(x)%7Ddx%7D%20%5C%5C%0A%5Ctag%7B.%7D%0A&amp;preview=true"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><img
src="https://www.nowcoder.com/equation?tex=Q(x)&amp;preview=true"
alt="img" />为<strong>理论概率分布</strong>，<img
src="https://www.nowcoder.com/equation?tex=P(x)&amp;preview=true"
alt="img" />为模型<strong>预测概率分布</strong>，而KL就是度量这两个分布的差异性，当然差异越小越好，所以KL也可以用作损失函数。</p>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>常见优化函数</title>
    <url>/2023/03/07/optimizer/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="常见优化函数">常见优化函数</h1>
<h2 id="梯度下降gd决定优化方向">梯度下降GD(决定优化方向)</h2>
<p><strong>梯度下降的核心思想：负梯度方向是使函数值下降最快的方向</strong></p>
<h3 id="批次梯度下降bgd">批次梯度下降BGD</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307202943629.png"
alt="image-20230307202943629" />
<figcaption aria-hidden="true">image-20230307202943629</figcaption>
</figure>
<p><strong>优点</strong>：在梯度下降法中，因为每次都遍历了完整的训练集，<strong>其能保证结果为全局最优</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307204257470.png"
alt="image-20230307204257470" />
<figcaption aria-hidden="true">image-20230307204257470</figcaption>
</figure>
<p><strong>缺点</strong>：我们需要对于每个参数求偏导，且在对每个参数求偏导的过程中还需要对训练集遍历一次，当训练集（m）很大时，计算费时</p>
<p><strong>解决方法</strong>：使用minibatch去更新</p>
<h3 id="随机梯度下降">随机梯度下降</h3>
<p>为了解决BGD耗时过长，它是利用单个样本的损失函数对θ求偏导得到对应的梯度，来更新θ，更新过程如下：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307204324179.png"
alt="image-20230307204324179" />
<figcaption aria-hidden="true">image-20230307204324179</figcaption>
</figure>
<p>速度快，但受抽样影响大，<strong>噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307204553630.png"
alt="image-20230307204553630" />
<figcaption aria-hidden="true">image-20230307204553630</figcaption>
</figure>
<p>因为每一次迭代的梯度受抽样的影响比较大，学习率需要逐渐减少，否则模型很难收敛。在实际操作中，一般采用线性衰减：
<span class="math display">\[
\eta_k=(1-\alpha)\eta_0+\alpha\eta_{\tau}
\]</span></p>
<p><span class="math display">\[
\alpha=\frac{k}{\tau}
\]</span></p>
<p><span class="math inline">\(\eta_0\)</span>:初始学习率</p>
<p><span class="math inline">\(\eta_{\tau}\)</span>：
最后一次迭代的学习率</p>
<p><span class="math inline">\(\tau\)</span>：自然迭代次数</p>
<p><span class="math inline">\(\eta_{\tau}\)</span>设为<span
class="math inline">\(\eta_0\)</span>的1%，k一般设为100的倍数。</p>
<p><strong>优点</strong>：收敛速度快</p>
<p><strong>缺点</strong>：</p>
<ol type="1">
<li><p>训练不稳定：噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p></li>
<li><p>选择适当的学习率可能很困难。
太小的学习率会导致收敛性缓慢，而学习速度太大可能会妨碍收敛，并导致损失函数在最小点波动。</p></li>
<li><p>无法逃脱鞍点</p></li>
</ol>
<details>
在数学中，鞍点或极小值点是函数图形表面上的一个点，其正交方向上的斜率(导数)均为零(临界点)，但不是函数的局
部极值。一句话概括就是：一个不是局部极值点的驻点称为鞍点。
*驻点：函数在一点处的一阶导数为零。
<img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307205942585.png">
<details>
<h3 id="min-batch-小批量梯度下降mbgd">min-batch 小批量梯度下降MBGD</h3>
<p><strong>算法的训练过程比较快，而且也要保证最终参数训练的准确率</strong></p>
<p>m表示一个批次的数据个数</p>
<h2 id="动量方法">动量方法</h2>
<h3 id="momentum随机梯度下降">Momentum随机梯度下降</h3>
<p>核心思想：Momentum借用了物理中的<strong>动量</strong>概念,即前一次的梯度也会参与运算。为了表示动量，引入了<strong>一阶动量</strong>m。<img
src="https://www.nowcoder.com/equation?tex=m&amp;preview=true"
alt="img" />是之前的梯度的累加,但是每回合都有一定的衰减。公式如下：
<span class="math display">\[
m_t=\beta m_{t-1}+(1-\beta)\cdot g_t
\]</span></p>
<p><span class="math display">\[
w_{t+1}=w_t-\eta \cdot m_t
\]</span></p>
<p><span class="math inline">\(g_t\)</span>：
为第t次计算的梯度（就是现在要算这次）</p>
<p><span class="math inline">\(m_{t-1}\)</span>: 为之前梯度的累加</p>
<p><span class="math inline">\(\beta\)</span>: 动量因子</p>
<p>所以当前权值的改变受上一次改变的影响，类似加上了<strong>惯性</strong>。</p>
<p>优点：momentum能够加速SGD收敛，抑制震荡。并且动量有机会逃脱局部极小值(鞍点)。</p>
<ol type="1">
<li>在梯度方向改变时，momentum能够降低参数更新速度，从而减少震荡；</li>
<li>在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛。</li>
</ol>
<h3 id="nesterov动量随机梯度下降法">Nesterov动量随机梯度下降法</h3>
<p>Nesterov是Momentum的变种。与Momentum唯一区别就是，计算梯度的不同。Nesterov动量中，先用当前的速度临时更新一遍参数，在用更新的临时参数计算梯度。</p>
<p>在momentum更新梯度时加入对当前梯度的校正，让梯度“多走一步”，可能跳出局部最优解：
<span class="math display">\[
w_t^*=\beta m_{t-1}+w_t
\]</span></p>
<p><span class="math display">\[
m_t=\beta m_{t-1}+(1-\beta)\cdot g_t
\]</span></p>
<p><span class="math display">\[
w_{t+1}=w_t-\eta \cdot m_t
\]</span></p>
<p>这里的<span class="math inline">\(g_t\)</span>用临时点<span
class="math inline">\(w_t^*\)</span>计算的</p>
<h2 id="更新学习率方法">更新学习率方法</h2>
<h3 id="adagrad">Adagrad</h3>
<p>引入<strong>二阶动量</strong>，根据训练轮数的不同，对学习率进行了动态调整：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307213914026.png"
alt="image-20230307213914026" />
<figcaption aria-hidden="true">image-20230307213914026</figcaption>
</figure>
<p><strong>缺点</strong>：仍然需要人为指定一个合适的全局学习率，同时网络训练到一定轮次后，分母上梯度累加过大使得学习率为0而导致训练提前结束。</p>
<h3 id="adadelta不是很懂">Adadelta(不是很懂)</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307215135905.png"
alt="image-20230307215135905" />
<figcaption aria-hidden="true">image-20230307215135905</figcaption>
</figure>
<h3 id="rmsprop">RMSProp</h3>
<p>AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSprop算法对Adagrad算法做了一点小小的修改，RMSprop使用指数衰减只保留过去给定窗口大小的梯度，使其能够在找到凸碗状结构后快速收敛。RMSProp法可以视为Adadelta法的一个特例，即依然使用全局学习率替换掉Adadelta法中的<span
class="math inline">\(s_t\)</span>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307215341546.png"
alt="image-20230307215341546" />
<figcaption aria-hidden="true">image-20230307215341546</figcaption>
</figure>
<p>推荐<span
class="math inline">\(\eta_{global}=1,\rho=0.9,\epsilon=10^{-6}\)</span></p>
<p>缺点：依然使用了全局学习率，需要根据实际情况来设定 优点：</p>
<ol type="1">
<li>分母不再是一味的增加，它会重点考虑距离它较近的梯度（指数衰减的效果）</li>
<li>只用了部分梯度加和而不是所有，这样避免了梯度累加过大使得学习率为0而导致训练提前结束。</li>
</ol>
<h3 id="adam">Adam</h3>
<p>https://zhuanlan.zhihu.com/p/377968342</p>
<p>Adam公式如下： <span class="math display">\[
m_t:=beta_1*m_{t-1}+(1-beta_1)*g
\]</span></p>
<p><span class="math display">\[
v_t:=beta_2*v_{t-1}+(1-beta_2)*g*g
\]</span></p>
<p><span class="math display">\[
variable:=variable-lr_t*\frac{m_t}{\sqrt{v_t+\epsilon}}
\]</span></p>
<p><span
class="math inline">\(m_t\)</span>可以理解为求历史梯度加强平均，思想来自动量方法，防止震荡。</p>
<p><span class="math inline">\(v_t\)</span>则是用于调整lr的，即是<span
class="math inline">\(\frac{lr}{\sqrt{v_t+\epsilon}}\)</span>,</p>
<p>在迭代过程中，如果某一维度一直以很小的梯度进行更新，证明此方向梯度变换较为稳定，因此可以加大学习率，以较大的学习率在此维度更新，体现在公式上就是：对历史梯度平方进行一阶指数平滑后，公式2会得到一个很小的值，公式3中的自适应学习率会相对较大</p>
<p>相反，某一维度在迭代过程中一直以很大的梯度进行更新，明此方向梯度变换较为剧烈（不稳定），因此可减小学习率，以较小的学习率在此维度更新
体现在公式上就是：对历史梯度平方进行一阶指数平滑后，公式2则会得到一个很大的值，公式3中的自适应学习率会相对较小</p>
<p><span
class="math inline">\(v_t\)</span>也可以解决<strong>梯度稀疏</strong>的问题；频繁更新的梯度将会被赋予一个较小的学习率，而稀疏的梯度则会被赋予一个较大的学习率，通过上述机制，在数据分布稀疏的场景，能更好利用稀疏梯度的信息，比标准的SGD算法更有效地收敛。</p>
<h1 id="常见优化函数问题">常见优化函数问题</h1>
<h2
id="sgd和adam谁收敛的比较快谁能达到全局最优解">SGD和Adam谁收敛的比较快？谁能达到全局最优解？</h2>
<p>SGD算法没有动量的概念，SGD和Adam相比，缺点是下降速度慢，对学习率要求严格。</p>
<p>而Adam引入了一阶动量和二阶动量，下降速度比SGD快，Adam可以自适应学习率，所以初始学习率可以很大。</p>
<p>SGD相比Adam，更容易达到全局最优解。主要是后期Adam的学习率太低，影响了有效的收敛。</p>
<p>我们可以前期使用Adam，后期使用SGD进一步调优。</p>
<h2 id="adam用到二阶矩的原理是什么">adam用到二阶矩的原理是什么</h2>
<p>引入二阶动量，根据训练轮数不同对学习率进行调整。</p>
<p>可以看出来，公式将前面的训练梯度平方加和，在网络训练的前期，由于分母中梯度的累加（<span
class="math inline">\(v_t\)</span>）较小，所以一开始的学习率<span
class="math inline">\(\eta_t\)</span>比较大；随着训练后期梯度累加较大时，<span
class="math inline">\(\eta_t\)</span>逐渐减小，而且是自适应地减小。</p>
<p>而且如果某个维度频繁震荡梯度大，学习率就降低；如果梯度小而稳定，学习率就大。</p>
<h2
id="batch的大小如何选择过大的batch和过小的batch分别有什么影响">Batch的大小如何选择，过大的batch和过小的batch分别有什么影响</h2>
<p><strong>Batch选择时尽量采用2的幂次，如8、16、32等</strong></p>
<p>在合理范围内，增大Batch_size的<strong>好处</strong>：</p>
<ol type="1">
<li>提高了<strong>内存利用率</strong>以及大矩阵乘法的并行化效率。</li>
<li>减少了跑完一次epoch(全数据集）所需要的迭代次数，加快了对于相同数据量的处理速度。</li>
</ol>
<p>盲目增大Batch_size的<strong>坏处</strong>：</p>
<ol type="1">
<li>提高了内存利用率，但是内存容量可能不足。</li>
<li>跑完一次epoch(全数据集)所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加，从而对参数的修正也就显得更加缓慢。</li>
<li>Batch_size增大到一定程度，其确定的下降方向已经基本不再变化。</li>
</ol>
<p>Batch_size过小的<strong>影响</strong>：</p>
<ol type="1">
<li>训练时不稳定，可能不收敛</li>
<li>精度可能更高。</li>
</ol>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>Regularization</title>
    <url>/2023/03/14/regularization/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="什么是正则化">什么是正则化</h1>
<p>目的：防止模型过拟合</p>
<p>原理：正则化通过在损失函数中<strong>引入惩罚项来限制模型的复杂度</strong>，以防止模型过度拟合训练数据。惩罚项会在优化过程中对模型的参数进行调整，以平衡模型的拟合能力和泛化能力。</p>
<p>https://www.zhihu.com/question/20924039</p>
<p>最直接的防止过拟合的方法就是减少特征数量，就是减少0范数（向量中非零元素的个数），但是0范数很难求，所以就有了1范数，2范数。</p>
<p>作用：</p>
<ol type="1">
<li>防止过拟合</li>
<li>特征选择：l1正则化</li>
<li>改善模型稳定性</li>
</ol>
<h1 id="常见正则化">常见正则化</h1>
<h2 id="l1正则化">l1正则化</h2>
<p><span class="math display">\[
l1=\lambda||\vec{w}||_1=\sum_i|w_i|
\]</span></p>
<p><span class="math inline">\(\lambda\)</span>控制约束程度</p>
<p>l1不仅可以<strong>约束参数量</strong>，还可以使<strong>参数更稀疏</strong>。因为对目标函数经过优化后，一部分参数会变为0，另一部分参数为非零实值。<strong>非零实值说明这部分参数是最重要的特征</strong>。</p>
<p>假设参数分布是Laplace分布。</p>
<h3 id="稀疏原因">稀疏原因</h3>
<p>https://blog.csdn.net/b876144622/article/details/81276818</p>
<p>https://www.zhihu.com/question/37096933/answer/70426653</p>
<p>0处导数突变，如果此时0+导数为正，优化时放负方向跑，0-导数为负数，优化时往正方向跑，就很容易落入0</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230314211808962.png"
alt="image-20230314211808962" />
<figcaption aria-hidden="true">image-20230314211808962</figcaption>
</figure>
<h3 id="缺点">缺点</h3>
<ul>
<li>非光滑性：L1
正则化的正则化项是参数的绝对值之和，这导致目标函数在参数为零时不可导。这使得优化过程变得更加困难，特别是在使用梯度下降等基于梯度的优化算法时。在参数为零附近，梯度不连续，可能导致优化过程出现问题。</li>
<li>多重共线性：当特征之间存在高度相关性（多重共线性）时，L1
正则化倾向于选择其中一个特征，而忽略其他相关特征。这可能导致模型的解释性下降，因为被忽略的相关特征可能包含有用的信息。相比之下，L2
正则化对相关特征的惩罚更均衡，可以保留更多相关特征的权重。</li>
<li>不适用于高维问题：在高维问题中，特征数量远远大于样本数量时，L1
正则化可能不太适用。由于参数空间的维度过高，L1
正则化可能无法准确地选择特征，导致过拟合或选择不稳定的特征子集。</li>
</ul>
<h2 id="l2正则化">l2正则化</h2>
<p><span class="math display">\[
l2=\frac{1}{2}\lambda||\vec{w}||_2^2=\sum_i|w_i|^2
\]</span></p>
<p>l2正则化会使部分特征<strong>趋近于0</strong>，也就达到正则化的目的了。</p>
<p>此外，l1正则化和l2正则化也可以联合使用，这种形式也被称为“<strong>Elastic网络正则化</strong>”。</p>
<p>假设参数分布是正态分布</p>
<h2 id="dropout">Dropout</h2>
<p>在训练的时候让一定量的神经元失活，在该epoch中不参与网络训练</p>
<h3
id="dropout训练出的参数需要乘以keep-prib使用">dropout训练出的参数需要乘以keep-prib使用</h3>
<p>因为神经元预测的时候就不应该随机丢弃，一种”补偿“的方案就是每个dropout训练出的神经元的权重都乘以一个<strong>p</strong>，这样在“总体上”使得<strong>测试数据</strong>和<strong>训练数据</strong>是大致一样的。保证<strong>测试</strong>的时候把这个神经元的权重乘以<strong>p</strong>可以得到<strong>同样的期望</strong>。比如一个神经元的输出是<strong>x</strong>，那么在训练的时候它有<strong>p</strong>的概率参与训练，<strong>(1-p)</strong>的概率丢弃，那么它输出的期望是<img
src="https://www.nowcoder.com/equation?tex=p%20%5Ctimes%20x%2B(1-p)%20%5Ctimes%200%20%3D%20p%20%5Ctimes%20x&amp;preview=true"
alt="img" />。因此<strong>测试</strong>的时候把这个神经元的权重乘以<strong>p</strong>可以得到<strong>同样的期望</strong>。</p>
<p>注：目前主流是采用inverted dropout替代dropout，inverted
dropout不需要乘以keep-prib。它的做法是在训练阶段对执行了dropout操作的层，其输出激活值要<strong>除以keep_prib</strong>，而测试的模型不用再做任何改动。除以（1-p），让期望与不dropout相同。</p>
<h2 id="早停">早停</h2>
<p>每一个epoch训练结束后使用<strong>验证集</strong>验证模型效果，画出训练曲线，这样就可以判断是否过拟合了。当发现网络有点过拟合了，当然就是“<strong>早停</strong>”了，可以直接停止训练了。</p>
<h2 id="扩充数据集">扩充数据集</h2>
<p>Augmentation，增加变化增加多样性</p>
<h3 id="数据增强方法">数据增强方法</h3>
<p>数据集越大，网络泛化性能越好，所以努力扩充数据集，通过平移、翻转、旋转、放缩、随机截取、加噪声、色彩抖动等等方式。</p>
<h2 id="bnbatch-normalization">BN（Batch Normalization）</h2>
<p>目的：用于解决深度网络<strong>梯度消失</strong>和<strong>梯度爆炸</strong>的问题，加速网络收敛速度。</p>
<p>批规范化，即在模型每次随机梯度下降训练时，通过mini-batch来对每一层的输出做<strong>规范化操作</strong>，使得结果（各个维度）的<strong>均值为0</strong>，<strong>方差为1</strong>，然后在进行尺度变换和偏移。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230316191341497.png"
alt="image-20230316191341497" />
<figcaption aria-hidden="true">image-20230316191341497</figcaption>
</figure>
<p>m是mini-batch中的数据个数。前面的散步是对input数据进行白化操作（线性），<strong>最后的“尺度变换和偏移”操作是为了让BN能够在线性和非线性之间做一个权衡</strong>，而这个偏移的参数是神经网络在训练时学出来的。</p>
<p>经过BN操作，网络每一层的输出小值被“拉大”，大值被“缩小”，所以就有效避免了梯度消失和梯度爆炸。<strong>总而言之，BN是一个可学习、有参数（γ、β）的网络层</strong>。</p>
<h3 id="尺度变换和偏移的作用">尺度变换和偏移的作用：</h3>
<p>归一会影响到本层网络A所学习到的特征（比如网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，如果强制把它给归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于这一层网络所学习到的特征分布<strong>被搞坏</strong>了）</p>
<p>于是<strong>BN</strong>最后的“<strong>尺度变换和偏移</strong>”操作，让我们的网络可以学习恢复出原始网络所要学习的特征分布（衡量线性和非线性）</p>
<h3 id="bn训练和测试有什么不同">BN训练和测试有什么不同</h3>
<p>训练时，均值和方差针对一个<strong>Batch</strong>。</p>
<p>测试时，均值和方差针对<strong>整个数据集</strong>而言。因此，在训练过程中除了正常的前向传播和反向求导之外，我们还要记录<strong>每一个Batch的均值和方差</strong>。</p>
<h3 id="bn和ln的差别">BN和LN的差别</h3>
<p>LN：Layer
Normalization，LN是“横”着来的，<strong>对一个样本，经过同一层的所有神经元</strong>做<strong>归一化</strong>。LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；LN不依赖于batch的大小和输入sequence的深度，因此可以用于<strong>batchsize为1</strong>和RNN中对边长的输入sequence的normalize操作。</p>
<p>BN：Batch
Normalization，BN是“竖”着来的，<strong>经过一个神经元的所有样本</strong>做<strong>归一化</strong>，所以与<strong>batch
size</strong>有关系。</p>
<p>二者提出的目的都是为了加快模型收敛，减少训练时间。</p>
<h3 id="如何同时使用bn和dropout">如何同时使用BN和dropout</h3>
<p>同时使用BN和Dropout会出现方差偏移的现象，原因：</p>
<p>使用Dropout：训练的时候以<strong>概率p</strong>
drop了一些节点，比如dropout设置为0.5，隐藏层共有6个节点，那训练的时候有3个节点的值被丢弃，而测试的时候这6个节点都被保留下来，这就导致了<strong>训练</strong>和<strong>测试</strong>的时候以该层节点为输入的下一层的神经网络节点获取的<strong>期望</strong>会有量级上的差异。为了解决这个问题，在训练时对当前dropout层的输出数据<strong>除以（1-p）</strong>，之后再输入到下一层的神经元节点，以作为失活神经元的补偿，以使得在训练时和测试时每一层的输入有大致相同的期望。</p>
<p>但是这样使得神经元输入期望大致相同，但是方差不一样，而BN是通过均值方差计算的，所以会导致输出不正确。</p>
<p>解决方法：</p>
<ol type="1">
<li>只<strong>在所有BN层的后面采用dropout层</strong>。</li>
<li>dropout原文提出了一种高斯dropout，论文再进一步对高斯dropout进行扩展，提出了一个<strong>均匀分布Dropout</strong>，这样做带来了一个好处就是这个形式的Dropout（又称为“Uout”）对方差的偏移的敏感度降低了</li>
</ol>
<h2 id="bagging-和bootstrap">Bagging 和Bootstrap？</h2>
<p><strong>Bootstrap</strong>是一种抽样方法，即随机抽取数据并将其放回。如一次抽取一个样本，然后放回样本集中，下次可能再抽取这个样本。接着将每轮未抽取的数据合并形成<strong>袋外数据集</strong>（Out
of Bag, OOB），用于模型中的测试集。</p>
<p><strong>Bagging算法</strong>使用<strong>Bootstrap方法</strong>从原始样本集中随机抽取样本。共提取K个轮次，得到K个独立的训练集，元素可以重复。用K个训练集训练K个模型。分类问题以结果中的多个值投票作为最终结果，回归问题以平均值作为最终结果。结果采用投票法，避免了决策树的过拟合问题。</p>
<p><strong>Boosting</strong>是为每个训练样本设置一个权重，在下一轮分类中，误分类的样本权重较大，即每轮样本相同，但样本权重不同；对于分类器来说，分类误差小的分类器权重较大，反之则小。</p>
<p><strong>采用模型融合的方式也可以避免过拟合</strong>。</p>
<h2 id="参数共享">参数共享</h2>
<p>参数共享是一种在神经网络中常用的正则化方法，特别适用于卷积神经网络（CNN）。通过在神经网络的不同层之间共享参数，可以减少模型的参数数量，提高模型的效率和泛化能力。</p>
<h2 id="数据标准化">数据标准化</h2>
<p>数据标准化是对数据进行预处理的一种方式，将数据按特征进行缩放，使得每个特征的均值为
0，标准差为
1。这有助于使不同特征之间的尺度一致，提高模型的收敛速度和性能。</p>
<ul>
<li>z-score</li>
<li>min-max</li>
</ul>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
</search>
