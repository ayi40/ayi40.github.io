<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>算法</title>
    <url>/2023/02/14/Algorithm_example/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="动态规划">动态规划</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">有一排深渊法师，他们的护盾值用数组 a 表示；</span><br><span class="line">你可以对深渊法师这样破盾：</span><br><span class="line">1) 用重击破盾，每消耗1MP破1点护盾</span><br><span class="line">2) 如果有两个相邻的深渊法师他们的元素不同，且护盾都不为0，你可以对他们俩使用高天之歌，消耗xMP破所有护盾</span><br><span class="line">问最少消耗的MP？</span><br><span class="line">下面输入的 els 表示深渊法师的元素，I 表示冰，W 表示水，F 表示火</span><br><span class="line"></span><br><span class="line">输入：</span><br><span class="line">a = [4, 8, 10, 2, 15, 2]</span><br><span class="line">x = 9</span><br><span class="line">els = WIFFII</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i]=min(dp[i-1]+a[i],dp[i-2]+x) if 能使用高天之歌 else dp[i-1]+a[i]</span><br></pre></td></tr></table></figure>
<h1 id="找规律题目">找规律题目</h1>
<h2 id="找规律后拼接">找规律后拼接</h2>
<h3 id="构建长度为n的字符串">构建长度为n的字符串</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A希望你构造一个长度为n的数组，满足以下条件：</span><br><span class="line">1. 所有元素绝对值不大于3</span><br><span class="line">2. 相邻两个元素乘积小于0，且和不为0</span><br><span class="line">3. 所有元素之和=0</span><br><span class="line">input: 2 output:no answer</span><br><span class="line">input: 3 output: -1 2 -1</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">根据题目信息：</span><br><span class="line">1. 数组的元素在&#123;1,2,3&#125;里挑选</span><br><span class="line">2. 数组相邻元素一个为正一个为负，且互不相等，正号组和负号组可以互换。</span><br><span class="line">3. 正负号数值和相同，且可拼接！！！因为0+0=0</span><br><span class="line"></span><br><span class="line">考虑拼接条件：只要首尾元素不相同就可以拼接！！</span><br><span class="line"></span><br><span class="line">开始找规律以及能用于拼接的元素：</span><br><span class="line">input: 2 output:no answer</span><br><span class="line">input: 3 output: -1 3 -2（这个可以用于拼接，首位元素不同） -1 2 -1（这个不可以）</span><br><span class="line">input: 4 output: -1 2 -3 2（这个可以用于拼接，首位元素不同）</span><br><span class="line">input: 5 output: no answer</span><br><span class="line">我们发现所以大于5的数都可以由若干个3和若干个4相加得到，且3、4中有答案可以随意拼接，所以我们能利用3、4的答案拼接出n&gt;5的满足条件的数组</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="拼接mhy字符串">拼接mhy字符串</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">如果不能定义：字符串的“权重”是字符串中出现的字符种类数量，比如 mmm 权重为1，mmh 为 2，mhy 为 3</span><br><span class="line">输入：x, y, z 三个变量</span><br><span class="line">输出：一个长度为 x + y + z + 2 的字符串，这个字符串只能由 m h y 三种字符组成，这个字符串一共有 x + y + z 个长度为 3 的子串，其中有 x 个权重为 1 的子串，y 个权重为 2 的子串，z 个权重为 3 的子串。</span><br><span class="line">只用输出一种情况，若无法组成，输出-1</span><br><span class="line"></span><br><span class="line">范例输入：x = 2, y = 1, z = 1</span><br><span class="line">范例输出：mmmmhy</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 找规律，发现权重3字符串后只能接权重2或权重3的字符串，不能跟权重1；所以如果x,z&gt;0,y=0这种情况无解</span><br><span class="line">2. 我们可以先拼接权重3字符串（若有）mhymhy...</span><br><span class="line">3. 然后拼接权重2，mhymhyhyhy...；如果无权重三直接拼接权重2,hyhyh....</span><br><span class="line">4. 只剩下一个权重2没拼时，拼一个可以接权重1的权重2，mhymhyhyhyy</span><br><span class="line">5. 拼接权重1：mhymhyhyhyyyyyyy</span><br></pre></td></tr></table></figure>
<h2 id="找规律后计算">找规律后计算</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A拿到了3元无限长字符串&#123;1,2,3；4,5,6；7,8,9；....&#125;</span><br><span class="line">其中3的倍数后用；分割，其他用逗号</span><br><span class="line">求l个字符到r个字符之间有几个逗号和分号。</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 求l到r字符之间有几个逗号分号——》r前逗号分号-l前逗号分号</span><br><span class="line">2. 难点：数字为1-9时，每个数字只占1个字符，为10-99，每个数字就占两个字符了。</span><br><span class="line">	开始找规律！</span><br><span class="line">	1-9			每6个字符为一组&#123;1,2,3;&#125;			这样的组有3个</span><br><span class="line">	10-99		每9个字符为一组&#123;10,11,12;&#125;			这样的组有30个</span><br><span class="line">	100-999		每12个字符为一组&#123;100,111,112;&#125;		这样的组有300个</span><br><span class="line">	以此类推</span><br><span class="line">	</span><br></pre></td></tr></table></figure>
<h1 id="审题题">审题题</h1>
<h2 id="交换字母使字典序尽可能大">交换字母使字典序尽可能大</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A拿到一个仅有小写字母组成的字符串，她准备进行恰好一次操作：交换连个相邻字母，在操作结束后使字符串的字典序尽可能大。</span><br><span class="line"></span><br><span class="line">input: ba</span><br><span class="line">output: ab</span><br><span class="line">2&lt;=len(input)&lt;=200000</span><br></pre></td></tr></table></figure>
<p>知识点：</p>
<ol type="1">
<li><p>字典序，就是按照字典排列顺序，英文字母按下面方式排列：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ABCDEFG HIJKLMN OPQRST UVWXYZ</span><br><span class="line">abcdefg hijklmn opqrst uvwxyz</span><br></pre></td></tr></table></figure></li>
<li><p>题目说的是<strong>恰好一次</strong>操作！！！！就算交换之后会让原来字符串字典序减小也需要进行操作！</p></li>
</ol>
<h1 id="寻找用例">寻找用例</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。</span><br><span class="line"></span><br><span class="line">计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。</span><br><span class="line"></span><br><span class="line">你可以认为每种硬币的数量是无限的。</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><p>找正例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：coins = [1, 2, 5], amount = 11</span><br><span class="line">输出：3 </span><br></pre></td></tr></table></figure></li>
<li><p>找负例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：coins = [2], amount = 3</span><br><span class="line">输出：-1</span><br></pre></td></tr></table></figure></li>
<li><p>找边界条件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：coins = [1], amount = 0</span><br><span class="line">输出：0</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>DHGCN(2HRDR)</title>
    <url>/2023/06/21/DHGCN/</url>
    <content><![CDATA[<p>propose DHGCN,2HRDR</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>task:
基于知识图谱的问答系统，在知识图谱中检索与问题相关的多个元组</p>
<p>contribution: propose a convolutional network for directed
hypergraph</p>
<h1 id="dhgcn">DHGCN</h1>
<h2 id="hgcn">HGCN</h2>
<p>given a hypergraph <span class="math inline">\(G=(V,E,W)\)</span>, as
well as the incidence matrix <span class="math inline">\(H\in
R^{|V|\times |E|}\)</span></p>
<p>the edge and vertex degrees:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621153922205.png"
alt="image-20230621153922205" />
<figcaption aria-hidden="true">image-20230621153922205</figcaption>
</figure>
<p>while the hypergraph convolutional networks is:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621155931211.png"
alt="image-20230621155931211" />
<figcaption aria-hidden="true">image-20230621155931211</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/IMG_0183(20230621-201655).JPG"
alt="IMG_0183(20230621-201655)" />
<figcaption aria-hidden="true">IMG_0183(20230621-201655)</figcaption>
</figure>
<h2 id="dhgcn-1">DHGCN</h2>
<p>the directed hypergraph can be denoted by two incidence matrices
<span class="math inline">\(H^{head}\)</span> and <span
class="math inline">\(H^{tail}\)</span></p>
<p>the degree:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621160733846.png"
alt="image-20230621160733846" />
<figcaption aria-hidden="true">image-20230621160733846</figcaption>
</figure>
<p>the directed hypergraph convolutional networks is:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621160751671.png"
alt="image-20230621160751671" />
<figcaption aria-hidden="true">image-20230621160751671</figcaption>
</figure>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/IMG_0184(20230621-202928).JPG" alt="IMG_0184(20230621-202928)" style="zoom:50%;" /></p>
<h1 id="hrdr">2HRDR</h1>
<h2 id="task-definition">Task Definition</h2>
<p>given a knowledge graph <span
class="math inline">\(K=(V,E,T)\)</span> and <span
class="math inline">\(q=(w_1,w_2,\cdots ,w_{|q|})\)</span>.</p>
<p>the task aims to pick the answers from <em>V</em>.</p>
<h2 id="method">Method</h2>
<h3 id="directed-hypergraph-retrieval-and-construction"><strong>Directed
Hypergraph Retrieval and</strong> <strong>Construction</strong></h3>
<p>find subgraph</p>
<ol type="1">
<li>obtain seed entities from the question by entity linking</li>
<li>get the entities set within L hops to form a subgraph</li>
<li>get <span class="math inline">\(H^{head}\)</span> and <span
class="math inline">\(H^{tail}\)</span></li>
</ol>
<h3 id="input-encoder">Input Encoder</h3>
<ol type="1">
<li><p>apply a bi-LSTM to encode question and obtain hidden states <span
class="math inline">\(H\in R^{|q|\times h}\)</span>,we assume
h=d</p></li>
<li><p>employ co-attention to learn query-aware entity
representation</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621170223607.png"
alt="image-20230621170223607" />
<figcaption aria-hidden="true">image-20230621170223607</figcaption>
</figure></li>
</ol>
<h3 id="reasoning-over-hypergraph">Reasoning over Hypergraph</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621163315446.png"
alt="image-20230621163315446" />
<figcaption aria-hidden="true">image-20230621163315446</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621172646592.png"
alt="image-20230621172646592" />
<figcaption aria-hidden="true">image-20230621172646592</figcaption>
</figure>
<ol type="1">
<li><p>Learn Relation Representation Explicitly</p>
<ol type="1">
<li><p>combine entity embedding and co-attention</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621173702999.png"
alt="image-20230621173702999" />
<figcaption aria-hidden="true">image-20230621173702999</figcaption>
</figure></li>
<li><p>propagation</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621173728920.png"
alt="image-20230621173728920" />
<figcaption aria-hidden="true">image-20230621173728920</figcaption>
</figure></li>
<li><p>aggregation</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621173815550.png"
alt="image-20230621173815550" />
<figcaption aria-hidden="true">image-20230621173815550</figcaption>
</figure></li>
</ol></li>
<li><p>Allocate Relation Weights Dynamically(dynamically allocated
hop-by-hop)</p>
<ol type="1">
<li><p>use co-attention to cal <span
class="math inline">\(R_{co\_attn}\)</span></p></li>
<li><p>compute the weight of edge</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621174053983.png"
alt="image-20230621174053983" />
<figcaption aria-hidden="true">image-20230621174053983</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621174101797.png"
alt="image-20230621174101797" />
<figcaption aria-hidden="true">image-20230621174101797</figcaption>
</figure></li>
</ol></li>
<li><p>Update Entity Adaptively</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230621174229381.png"
alt="image-20230621174229381" />
<figcaption aria-hidden="true">image-20230621174229381</figcaption>
</figure></li>
</ol>
]]></content>
      <categories>
        <category>GNN</category>
        <category>hypergraph</category>
      </categories>
  </entry>
  <entry>
    <title>DHT-Relation Work</title>
    <url>/2023/05/26/DHT-relation/</url>
    <content><![CDATA[<p>Relation work of DHT</p>
<span id="more"></span>
<h1 id="edgeformers">Edgeformers</h1>
<p>EDGEFORMERS: G RAPH-E MPOWERED TRANSFORMERS FOR REPRESENTATION L
EARNING ON T EXTUALE DGE NETWORKS</p>
<h2 id="background">Background</h2>
<ol type="1">
<li><p>Edge-aware GNNs:</p>
<p>studies assume the information carried by edges can be directly
described as an attribute vector.</p>
<ol type="1">
<li>This assumption holds well when edge features are categorical</li>
<li>cannot fully capture contextualized text semantic</li>
</ol></li>
<li><p>PLM-GNN</p>
<p>text information is first encoded by a PLM and then aggregated by a
GNN</p>
<ol type="1">
<li>such architectures process text and graph signals one after the
other, and fail to simultaneously model the deep interactions</li>
</ol></li>
<li><p>GNN-nested PLM</p>
<p>inject network information into the text encoding process</p>
<ol type="1">
<li>cannot be easily adapted to handle text-rich edges</li>
</ol></li>
</ol>
<h2 id="proposed-method">Proposed method</h2>
<ol type="1">
<li>we conduct edge representation learning by jointly considering text
and network information via a Transformer-based architecture
(Edgeformer-E).</li>
<li>perform node representation learning using the edge representation
learning module as building blocks (Edgeformer-N)</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230602122224210.png"
alt="image-20230602122224210" />
<figcaption aria-hidden="true">image-20230602122224210</figcaption>
</figure>
<h3
id="network-aware-edge-text-encoding-with-virtual-node-tokens">Network-aware
Edge Text Encoding with Virtual Node Tokens</h3>
<p>Given an edge <span
class="math inline">\(e_{ij}=(v_i,v_j)\)</span></p>
<p>Use a transformer to deal with text</p>
<p>introduce two virtual node tokens to represent $ v_i$ and <span
class="math inline">\(v_j\)</span> to transformer</p>
<p>$ v_i$ 和 <span class="math inline">\(v_j\)</span>
是连接边两个node的embedding]</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230602122208124.png"
alt="image-20230602122208124" />
<figcaption aria-hidden="true">image-20230602122208124</figcaption>
</figure>
<h3 id="text-a-ware-node-representation-learning-edgeformer-n">TEXT-A
WARE NODE REPRESENTATION LEARNING (EDGEFORMER-N)</h3>
<h4 id="node-aggregating-edge-representations">node-Aggregating Edge
Representations</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230602122457727.png"
alt="image-20230602122457727" />
<figcaption aria-hidden="true">image-20230602122457727</figcaption>
</figure>
<h3
id="enhancing-edge-representations-with-the-nodes-local-network-structure">Enhancing
Edge Representations with the Node’s Local Network Structure</h3>
<p>add one more virtual node in edge learning
:获取与边节点连接的邻居边的信息</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230602122540378.png"
alt="image-20230602122540378" />
<figcaption aria-hidden="true">image-20230602122540378</figcaption>
</figure>
<p>邻居边的embedding经过一个新tranformer，获取<cls>节点的embedding，作为参加edge
learning的虚拟节点</p>
<h1 id="gratis">GRATIS</h1>
<p>Paper：GRATIS: Deep Learning <strong>G</strong>raph
<strong>R</strong>epresentation with T<strong>a</strong>sk-specifific
<strong>T</strong>opology and Mult<strong>i</strong>-dimensional Edge
Feature<strong>s</strong></p>
<p>总结：计算一个全局representation-X，X经过MLP和reshape、softmax等操作变成和邻接矩阵大小相同的权重矩阵，然后得到一个edge出现的概率矩阵，概率大于一定阈值就补全边。</p>
<p>edge用向量表示而不是一个一维数（一维权重）。</p>
<h2 id="methology">Methology</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617164254419.png"
alt="image-20230617164254419" />
<figcaption aria-hidden="true">image-20230617164254419</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617164314513.png"
alt="image-20230617164314513" />
<figcaption aria-hidden="true">image-20230617164314513</figcaption>
</figure>
<h3 id="backbone">Backbone</h3>
<p>反正就是各种方法得到一个全局表示X</p>
<h3 id="graph-definition">Graph Definition</h3>
<p>采用图原来的点和边</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617164457095.png"
alt="image-20230617164457095" />
<figcaption aria-hidden="true">image-20230617164457095</figcaption>
</figure>
<h3 id="task-specific-topology-prediction">Task-specific Topology
Prediction</h3>
<p>用X计算出一个概率矩阵</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617164752154.png"
alt="image-20230617164752154" />
<figcaption aria-hidden="true">image-20230617164752154</figcaption>
</figure>
<p>h（x）为mlp</p>
<p>如果概率大于某个阈值，增加新边</p>
<h3
id="multi-dimensional-edge-feature-generation"><strong>Multi-dimensional
Edge Feature Generation</strong></h3>
<p>根据边两端的节点计算边</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617165302836.png"
alt="image-20230617165302836" />
<figcaption aria-hidden="true">image-20230617165302836</figcaption>
</figure>
<p><strong>VCR</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617165318193.png"
alt="image-20230617165318193" />
<figcaption aria-hidden="true">image-20230617165318193</figcaption>
</figure>
<p><strong>VVR</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617165420670.png"
alt="image-20230617165420670" />
<figcaption aria-hidden="true">image-20230617165420670</figcaption>
</figure>
<p>fifinally employ either a pooling layer or a fully-connected layer,
to flatten <span class="math inline">\(F_{i,x,j}\)</span> and <span
class="math inline">\(F_{ *j,x,i*}\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617165554284.png"
alt="image-20230617165554284" />
<figcaption aria-hidden="true">image-20230617165554284</figcaption>
</figure>
<h1 id="surge">SURGE</h1>
<p>Paper： Knowledge-Consistent Dialogue Generation with Knowledge
Graphs</p>
<p>总结：在KG大图中检索与文本相关的子图，用GCN计算node
representation，用ENGNN计算 edge representation。然后用在后面的任务</p>
]]></content>
      <categories>
        <category>GNN</category>
        <category>EdgeLearning</category>
      </categories>
  </entry>
  <entry>
    <title>DHT</title>
    <url>/2023/03/20/DHT/</url>
    <content><![CDATA[<p>DHT, which transforms the edges of a graph into the nodes of a
hypergraph.</p>
<p>ENGNN, use hypergraph after DHT to propagation</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>Before methods only capture edge information implicitly, e.g. used as
weight.</p>
<h1 id="contribute">Contribute</h1>
<ol type="1">
<li>propose DHT, Dual Hypergraph Transformation</li>
<li>propose a novel edge representation learning scheme ENGNN by using
DHT.</li>
<li>propose novel edge pooling methods.</li>
</ol>
<h1 id="method">Method</h1>
<h2 id="dht-how-to-transfer-graph-to-hypergraph">DHT： how to transfer
graph to hypergraph</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230320171020430.png"
alt="image-20230320171020430" />
<figcaption aria-hidden="true">image-20230320171020430</figcaption>
</figure>
<h3 id="step1-get-origin-graph-representation">Step1: Get origin graph
representation</h3>
<p>Firstly, we get the initial node feature and edge feature. <span
class="math display">\[
node \space  feature: X\in R^{n\times d}
\]</span></p>
<p><span class="math display">\[
edge\space feature: E\in R^{m\times d&#39;}
\]</span></p>
<p>Than we use an incidence matrix M rather than an adjacency matrix to
represent graph structure. <span class="math display">\[
incidence\space matrix: M\in \{0,1\}^{n\times m}
\]</span> So the origin graph is <span class="math display">\[
G=(X,M,E)
\]</span></p>
<h3 id="step-2-use-dht-to-get-hypergraph-g">Step 2: Use DHT to get
hypergraph <span class="math inline">\(G^*\)</span></h3>
<p>The hypergraph represent <span class="math display">\[
G^*=(X^*,M^*,E^*)
\]</span></p>
<p><span class="math display">\[
X^*=E
\]</span></p>
<p><span class="math display">\[
M^*=M^T
\]</span></p>
<p><span class="math display">\[
E^*=X
\]</span></p>
<p><span class="math display">\[
DHT:G=(X,M,E)-&gt;G^*=(E,M^T,X)
\]</span></p>
<p>While DHT is a bijective transformation: <span
class="math display">\[
DHT:G^*=(E,M^T,X)-&gt;G=(X,M,E)
\]</span></p>
<h2
id="ehgnn-an-edge-representation-learning-framework-using-dht">EHGNN: an
edge representation learning framework using DHT</h2>
<p><span class="math display">\[
E^{(l+1)}=ENGNN(X^{(l)},M,E^{(l)})=GNN(DHT(X^{(l)},M,E^{(l)}))
\]</span></p>
<p>So ENGNN consists of DHT and GNN, while GNN can be any GNN
function.</p>
<p>After ENGNN, EHGNN, <span class="math inline">\(E^{(L)}\)</span> is
returned to the original graph by applying DHT to dual hypergraph <span
class="math inline">\(G^∗\)</span>. Then, the remaining step is how to
make use of these edge-wise representations to finish the task.</p>
<h2 id="pooling">Pooling</h2>
<p>To be continue...</p>
<h1 id="advantage">Advantage</h1>
<h2 id="dht">DHT</h2>
<ol type="1">
<li>low time complexity</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230320165122363.png"
alt="image-20230320165122363" />
<figcaption aria-hidden="true">image-20230320165122363</figcaption>
</figure>
]]></content>
      <categories>
        <category>GNN</category>
        <category>EdgeLearning</category>
      </categories>
  </entry>
  <entry>
    <title>Data Structure and algorithms</title>
    <url>/2023/04/14/DataStructure/</url>
    <content><![CDATA[<ol type="1">
<li>Complexity</li>
<li>Linear structures</li>
<li>Tree structures</li>
<li>Other common data structures</li>
<li>Search algorithms</li>
<li>Sorting algorithms</li>
</ol>
<span id="more"></span>
<h1 id="complexity">Complexity</h1>
<h1 id="array">Array</h1>
<p>fixed length, indexable, should shift after insertions and
deletions</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230419185630699.png"
alt="image-20230419185630699" />
<figcaption aria-hidden="true">image-20230419185630699</figcaption>
</figure>
<h2 id="array-operation">Array Operation</h2>
<h3 id="insertion-on">Insertion O(n)</h3>
<ol type="1">
<li>insert an element in the specified index</li>
<li>shift the subsequent item to the right</li>
</ol>
<h3 id="deletion-on">Deletion O(n)</h3>
<ol type="1">
<li>delete the specified element</li>
<li>shift the subsequent item to the left</li>
</ol>
<h3 id="searching-in-a-sorted-array">Searching in a sorted array</h3>
<h4 id="linear-search">Linear search</h4>
<p>Best case-O(1)</p>
<p>Worst case-O(n)</p>
<p>Average case-O(n/2)-&gt;O(n)</p>
<h4 id="binary-search-ologn">Binary search O(logn)</h4>
<h3 id="sorting-array">Sorting array</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230419191311851.png"
alt="image-20230419191311851" />
<figcaption aria-hidden="true">image-20230419191311851</figcaption>
</figure>
<h1 id="stack">Stack</h1>
<p>Last In First Out</p>
<h2 id="stack-operation">Stack Operation</h2>
<p>Push(S,x): insert x to the top of the stack S</p>
<p>Pop(S): extract the top of the stack S</p>
<p>Top(s): return the topmost element of stack S without removing it</p>
<p>isEmpty(s): return whether the stack S is empty</p>
<h2 id="implementation">Implementation</h2>
<h3 id="array-1">Array</h3>
<h4 id="push-o1">Push() O(1)</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if s.top=s.len</span><br><span class="line">	error &#x27;full&#x27;</span><br><span class="line">else</span><br><span class="line">	s.top=s.top+1</span><br><span class="line">	s[s.top]=x</span><br></pre></td></tr></table></figure>
<h4 id="pop-o1">Pop() O(1)</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if isEmpty(s)</span><br><span class="line">	error &#x27;empty&#x27;</span><br><span class="line">else</span><br><span class="line">	s.top=s.top-1</span><br><span class="line">	return s[s.top+1]</span><br></pre></td></tr></table></figure>
<h4 id="top-o1">Top() O(1)</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if isEmpty(s)</span><br><span class="line">	error &#x27;empty&#x27;</span><br><span class="line">else</span><br><span class="line">	return s[s.top]</span><br></pre></td></tr></table></figure>
<h4 id="isempty-o1">isEmpty() O(1)</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if s.len=0</span><br><span class="line">	return true</span><br><span class="line">else</span><br><span class="line">	return false</span><br></pre></td></tr></table></figure>
<h4 id="search-on">Search O(n)</h4>
<h2 id="use-stack-implements-a-simple-calculator">Use stack implements a
simple calculator</h2>
<ol type="1">
<li>transfer to postfix notation</li>
</ol>
<p>3-2-1&gt;&gt;32-1-</p>
<p>3-2*1&gt;&gt;321*-</p>
<ol start="2" type="1">
<li>use stack to calculate</li>
</ol>
<p>3-2-1&gt;&gt;32-1-</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">push(3)</span><br><span class="line">push(2)</span><br><span class="line">Meet(-);Pop;Pop;Push(3-2=1)</span><br><span class="line">Push(1)</span><br><span class="line">Meet(-);Pop;Pop;Push(1-1=0)</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>exercise</li>
</ol>
<p>10 + (44 − 1) * 3 + 9 / 2</p>
<p>((1 - 2) - 5) + (6 / 5)</p>
<p>(((22 / 7) + 4) * (6 - 2))</p>
<h1 id="queue">Queue</h1>
<p>First In First Out</p>
<h2 id="operation">Operation</h2>
<p>Enqueue(Q,x): put an element x at the end of queue Q</p>
<p>Dequeue(Q): extract the first element from queue Q</p>
<h2 id="implementation-1">Implementation</h2>
<h3 id="array-1-1">Array 1</h3>
<p>Enqueue in the tail -O(1)</p>
<p>Dequeue in the position0 -O(n)</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230419194745117.png"
alt="image-20230419194745117" />
<figcaption aria-hidden="true">image-20230419194745117</figcaption>
</figure>
<h3 id="array-2">Array 2</h3>
<p>Enqueue in the position0 -O(n)</p>
<p>Dequeue in the tail -O(1)</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230419195104009.png"
alt="image-20230419195104009" />
<figcaption aria-hidden="true">image-20230419195104009</figcaption>
</figure>
<h3 id="array-3--circular-queue">Array 3 -Circular queue</h3>
<p>Enqueue: O(1)</p>
<p>Dequeue: O(1)</p>
<p>Peeking (get the front item without removing it) O(1)</p>
<p>isFull: O(1)</p>
<p>isEmpty: O(1)</p>
<p>search: O(n)</p>
<h4 id="algorithm-implement">algorithm implement</h4>
<p>Data members:</p>
<p>• Q: an array of items • Q.len: length of array • Q.front: position
of the front item • Q.rear: rear item position + 1 (not an item)</p>
<h5 id="enqueue">Enqueue</h5>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if isFull(Q)</span><br><span class="line">	error &#x27;full&#x27;</span><br><span class="line">else</span><br><span class="line">	Q[Q.rear] = x</span><br><span class="line">	if Q.rear==Q.len:</span><br><span class="line">		Q.rear=1</span><br><span class="line">	else</span><br><span class="line">		Q.rear=Q.rear+1</span><br></pre></td></tr></table></figure>
<h5 id="dequeue">Dequeue</h5>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if isEmpty(W)</span><br><span class="line">	error &#x27;Empty&#x27;</span><br><span class="line">else</span><br><span class="line">	x=Q[Q.front]</span><br><span class="line">	if Q.front==Q.len</span><br><span class="line">		Q.front=1</span><br><span class="line">	else</span><br><span class="line">		Q.front=Q.front+1</span><br></pre></td></tr></table></figure>
<h5 id="isfull-and-isempty">isFull and isEmpty</h5>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230419201711632.png"
alt="image-20230419201711632" />
<figcaption aria-hidden="true">image-20230419201711632</figcaption>
</figure>
<h1 id="linked-list">Linked List</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230420110345241.png"
alt="image-20230420110345241" />
<figcaption aria-hidden="true">image-20230420110345241</figcaption>
</figure>
<p>Node: An object containing data and pointer(s) Pointer: Reference to
another node Head: The first node in a linked list Tail: The last node
in a linked list</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230420112447328.png"
alt="image-20230420112447328" />
<figcaption aria-hidden="true">image-20230420112447328</figcaption>
</figure>
<p>No limited to size</p>
<p>require more space per element</p>
<h2 id="initial">Initial</h2>
<p>...</p>
<h2 id="operation-1">Operation</h2>
<ol type="1">
<li><p>print</p></li>
<li><p>insert</p></li>
<li><p>Delete</p></li>
</ol>
<h1 id="hash-table">Hash table</h1>
<p>Dictionary ADT</p>
<p>key:value</p>
<h2 id="implement-of-adt">implement of ADT</h2>
<h3 id="array-3">array</h3>
<p>Space usage: O(n)</p>
<p>Search usage: O(n)</p>
<table>
<thead>
<tr class="header">
<th>Array index</th>
<th>Key</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>3</td>
<td>Coffee</td>
</tr>
<tr class="even">
<td>1</td>
<td>15</td>
<td>Bread</td>
</tr>
<tr class="odd">
<td>2</td>
<td>8</td>
<td>Tea</td>
</tr>
</tbody>
</table>
<h3 id="large-array">Large array</h3>
<p>Space usage: O(U)-max key</p>
<p>Search usage: O(1)</p>
<table>
<thead>
<tr class="header">
<th>Array index</th>
<th>Key</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="even">
<td>3</td>
<td>3</td>
<td>Coffee</td>
</tr>
<tr class="odd">
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="even">
<td>8</td>
<td>8</td>
<td>Tea</td>
</tr>
<tr class="odd">
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="even">
<td>15</td>
<td>15</td>
<td>Bread</td>
</tr>
</tbody>
</table>
<h3 id="hash-table-1">Hash table</h3>
<p>Converts a key (of a large range) to a hash value (of a small range).
e.g. k mod m</p>
<p>Space usage: O(n)</p>
<p>Search usage: O(1)</p>
<h4 id="collision-solution">Collision solution</h4>
<p>collision: different keys have the same hash value</p>
<h5 id="chaining">Chaining</h5>
<h6 id="use-a-linked-list">use a linked list</h6>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230422183421651.png"
alt="image-20230422183421651" />
<figcaption aria-hidden="true">image-20230422183421651</figcaption>
</figure>
<h6 id="load-factor">load factor</h6>
<p><span class="math display">\[
\lambda = \frac{n}{m}
\]</span></p>
<p>n is the number of keys, m is the total number of buckets.</p>
<p>Measures how full the hash table is</p>
<p>it is suggested to keep <span class="math inline">\(\lambda\)</span>
&lt; 1</p>
<h6 id="cost">cost</h6>
<ol type="1">
<li><p>time</p>
<p>cost of search: O(1)+O(l), l is the length of the linked list</p>
<p>worst case: O(1)+O(n)</p>
<p>Average case: O(1)+O(<span
class="math inline">\(\lambda\)</span>)</p></li>
<li><p>space</p>
<p>requires additional space to store the pointers in linked lists of
entries.</p>
<p>Worst case: n-1 additional space</p>
<p>Average case: <span class="math inline">\(\lambda\)</span>-1
additional space</p></li>
</ol>
<h5 id="opening-addressing-probing">Opening addressing: probing</h5>
<h6 id="insert">Insert</h6>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230422185347171.png"
alt="image-20230422185347171" />
<figcaption aria-hidden="true">image-20230422185347171</figcaption>
</figure>
<h6 id="search">Search</h6>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230422185455352.png"
alt="image-20230422185455352" />
<figcaption aria-hidden="true">image-20230422185455352</figcaption>
</figure>
<h6 id="delete">Delete</h6>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230422190114349.png"
alt="image-20230422190114349" />
<figcaption aria-hidden="true">image-20230422190114349</figcaption>
</figure>
<h6 id="load-factor-1">Load factor</h6>
<p>must <span class="math inline">\(\lambda &lt; 1\)</span></p>
<p>How to deal with the hash table when ! becomes large?</p>
<ol type="1">
<li>Make a large hash table and move all elements into it.</li>
<li>Simply add an additional hash table</li>
</ol>
<h6 id="probing-type">probing type</h6>
<ol type="1">
<li><p>Linear probing</p>
<p><span class="math display">\[
H(k,i) = (H_0(k) + i)\space mod \space m
\]</span> have clustering problem, multiple keys are hashed to
consecutive slots.</p>
<p>performance degrade significantly when <span
class="math inline">\(\lambda\)</span>&gt; 0.5.</p></li>
<li><p>Quadratic probing <span class="math display">\[
H(k,i) = (H_0(k) + a\cdot i+b\cdot i^2)\space mod \space m
\]</span> reduces the clustering problem.</p></li>
</ol>
<h3 id="string-key">string key</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230422191105400.png"
alt="image-20230422191105400" />
<figcaption aria-hidden="true">image-20230422191105400</figcaption>
</figure>
<h1 id="tree">Tree</h1>
<p>a tree is an abstract model of a hierarchical structure consists of a
set of nodes and a set of edges.</p>
<p>Every node except the root has exactly one parent node.</p>
<h2 id="definition">Definition</h2>
<ol type="1">
<li><p>Length of a path： The number of edges in the path.</p></li>
<li><p>The height of a node：</p>
<p>The largest path length from that node to any leaf node (not
including ancestors).</p>
<p>Each leaf node has the height 0.</p></li>
<li><p>The height of a tree: The maximum level of a node in a tree is
the tree’s height.</p></li>
<li><p>The depth of a node:</p>
<p>The node's level (depth) of a node is the length of the path from
that node to the root.</p>
<p>The depth of the root is zero.</p></li>
</ol>
<h2 id="binary-tree">Binary Tree</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230424201038461.png"
alt="image-20230424201038461" />
<figcaption aria-hidden="true">image-20230424201038461</figcaption>
</figure>
<h3 id="full-binary-tree">Full Binary Tree</h3>
<p>Every node has either 0 or 2 children.</p>
<h3 id="complete-binary-tree">Complete Binary Tree</h3>
<p>Every level, except the last level, is completely filled, and all
nodes in the last level are as far left as possible (left
justified).</p>
<h3 id="perfect-binary-tree">Perfect Binary Tree</h3>
<p>Every node except the leaf nodes have two children and every level is
completely filled.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230424201302544.png"
alt="image-20230424201302544" />
<figcaption aria-hidden="true">image-20230424201302544</figcaption>
</figure>
<p>list representation: [Root, left-sub-tree, right-sub-tree]</p>
<h3 id="binary-search-tree">Binary Search Tree</h3>
<p>Insertion - O(h)</p>
<p>Search - O(h)</p>
<p>Deletion: O(h)</p>
<p>ℎ is O(log⁡n) if tree is balanced.</p>
<p>all is O(n) in worst case</p>
<p>左小右大</p>
<h4 id="search-operation">Search Operation</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Algorithm SearcℎBST(t, target)</span><br><span class="line">  Input: the BST t <span class="keyword">and</span> the target</span><br><span class="line"></span><br><span class="line">  p = t.root</span><br><span class="line">  <span class="keyword">while</span> p≠null do</span><br><span class="line">      <span class="keyword">if</span> target=p.value do</span><br><span class="line">          <span class="keyword">return</span> p</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> target&lt;p.value do</span><br><span class="line">          p=p.left</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">          p=p.rigℎt</span><br><span class="line">  end</span><br><span class="line">  <span class="keyword">return</span> null</span><br></pre></td></tr></table></figure>
<h4 id="insert-1">Insert</h4>
<p>average case: O(n) = logn</p>
<p>worst case: O(n) = n</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Algorithm Insert(t, node)</span><br><span class="line">  Input: the BST t <span class="keyword">and</span> the node</span><br><span class="line"></span><br><span class="line">  p = t.root</span><br><span class="line">  <span class="keyword">if</span> p=null do</span><br><span class="line">      t.root=node</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  end</span><br><span class="line">  <span class="keyword">while</span> p≠null do</span><br><span class="line">      prev=p</span><br><span class="line">      <span class="keyword">if</span> node.value&lt;p.value do</span><br><span class="line">          p=p.left</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> node.value&gt;p.value do</span><br><span class="line">          p=p.rigℎt</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">          <span class="keyword">return</span></span><br><span class="line">  end</span><br><span class="line">  <span class="keyword">if</span> node.value&lt;prev do</span><br><span class="line">      prev.left=node</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">      prev.rigℎt=node</span><br></pre></td></tr></table></figure>
<h4 id="find-minimun">find minimun</h4>
<p>O(h)-ℎ is the depth of the tree</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Algorithm Minimum(t)</span><br><span class="line">  Input: the BST t</span><br><span class="line"></span><br><span class="line">  p = t.root</span><br><span class="line">  <span class="keyword">while</span> p.left≠null do</span><br><span class="line">      p=p.left</span><br><span class="line">  end</span><br><span class="line">  <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure>
<h4 id="deletion-in-bst">Deletion in BST</h4>
<ol type="1">
<li><p>has no child</p>
<p>delete the node</p></li>
<li><p>has one child</p>
<p>use the child to replace z</p></li>
<li><p>has two children</p>
<p>delete the minimum node x of the right subtree of z (i.e., x is the
successor of z), then replace z by x.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230424233325065.png"
alt="image-20230424233325065" />
<figcaption aria-hidden="true">image-20230424233325065</figcaption>
</figure></li>
</ol>
<h3 id="tree-traversal">Tree Traversal</h3>
<h4 id="depth-first-tree-traversal">Depth-first Tree Traversal</h4>
<h5 id="implement-stack">implement: stack</h5>
<h6 id="preorder-前序">preorder 前序</h6>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preorder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span> </span><br><span class="line">    <span class="built_in">print</span>(root.val)</span><br><span class="line">    preorder(root.left)</span><br><span class="line">    preorder(root.right)</span><br></pre></td></tr></table></figure>
<h6 id="postorder-后序">postorder 后序</h6>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">postorder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span> </span><br><span class="line">    preorder(root.left)</span><br><span class="line">    preorder(root.right)</span><br><span class="line">    <span class="built_in">print</span>(root.val)</span><br></pre></td></tr></table></figure>
<h6 id="inorder-中序">inorder 中序</h6>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inorder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span> </span><br><span class="line">    preorder(root.left)</span><br><span class="line">    <span class="built_in">print</span>(root.val)</span><br><span class="line">    preorder(root.right)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h4 id="breadth-first-tree-traversal">Breadth-first Tree Traversal</h4>
<h5 id="implement-queue">implement: queue</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bfs</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> roor: <span class="keyword">return</span></span><br><span class="line">    q=[root]</span><br><span class="line">    result=[]</span><br><span class="line">    <span class="keyword">while</span> q:</span><br><span class="line">        child=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> q:</span><br><span class="line">            result.append(i)</span><br><span class="line">            <span class="keyword">if</span> i.left:</span><br><span class="line">                child.append(i.left)</span><br><span class="line">            <span class="keyword">if</span> i.right:</span><br><span class="line">                child.append(i.right)</span><br><span class="line">        q=child</span><br><span class="line">     <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h1 id="sorting">Sorting</h1>
<p>stable, unstable, inplace, outplace</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230425140502996.png"
alt="image-20230425140502996" />
<figcaption aria-hidden="true">image-20230425140502996</figcaption>
</figure>
<h2 id="selection-sort">Selection sort</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230425130213964.png"
alt="image-20230425130213964" />
<figcaption aria-hidden="true">image-20230425130213964</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230425130244294.png"
alt="image-20230425130244294" />
<figcaption aria-hidden="true">image-20230425130244294</figcaption>
</figure>
<p>time-<span class="math inline">\(O(n^2)\)</span></p>
<p>in-place</p>
<p>unstable</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">selection_sort</span>(<span class="params">arr</span>):</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr)-<span class="number">1</span>):</span><br><span class="line">		min_ind = i</span><br><span class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, <span class="built_in">len</span>(arr)):</span><br><span class="line">            <span class="keyword">if</span> arr[j]&lt;arr[min_ind]:</span><br><span class="line">                min_index=j</span><br><span class="line">        <span class="keyword">if</span> i!=min_ind:</span><br><span class="line">            arr[min_ind],arr[i]=arr[i],arr[min_index]</span><br><span class="line">    <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure>
<h2 id="insertion-sort">Insertion Sort</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230425130404747.png"
alt="image-20230425130404747" />
<figcaption aria-hidden="true">image-20230425130404747</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230425130733053.png"
alt="image-20230425130733053" />
<figcaption aria-hidden="true">image-20230425130733053</figcaption>
</figure>
<p>time -<span class="math inline">\(O(n^2)\)</span></p>
<p>in-place</p>
<p>stable</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def insertionSort(arr): </span><br><span class="line">    for i in range(1, len(arr)): </span><br><span class="line">        key = arr[i] </span><br><span class="line">        j = i-1</span><br><span class="line">        while j &gt;=0 and key &lt; arr[j] : </span><br><span class="line">                arr[j+1] = arr[j] </span><br><span class="line">                j -= 1</span><br><span class="line">        arr[j+1] = key </span><br></pre></td></tr></table></figure>
<h2 id="merge-sort">Merge Sort</h2>
<p>time - O(nlogn)</p>
<p>not in-place</p>
<p>stable</p>
<ol type="1">
<li>Divide: divide the array A into two sub-arrays (L and R) of n/2
numbers each.</li>
<li>Conquer: sort two sub-arrays recursively.</li>
<li>Combine: merge two sorted sub-arrays into a sorted array.</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230425141231598.png"
alt="image-20230425141231598" />
<figcaption aria-hidden="true">image-20230425141231598</figcaption>
</figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Algorithm MergeSort(A, n)</span><br><span class="line">  Input: the n-size array A</span><br><span class="line"></span><br><span class="line">  if n=1 then</span><br><span class="line">      return A</span><br><span class="line">  (L,R)=A</span><br><span class="line">  L’=MergeSort(L)</span><br><span class="line">  R’=MergeSort(R)</span><br><span class="line">  return Merge(L’, R’)</span><br></pre></td></tr></table></figure>
<h3 id="merge-function---on">Merge function - O(n)</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Algorithm Merge(A, n_A,B,n_B,C)</span><br><span class="line">  Input: the n_A-size array A and n_B-size array B</span><br><span class="line">  Output: the (n_A+n_B−1)-size array C</span><br><span class="line"></span><br><span class="line">  i=0,j=0,k=0</span><br><span class="line">  while i&lt;n_A and j&lt;n_B do</span><br><span class="line">      if A[i]≤B[j] then</span><br><span class="line">          C[k]=A[i], i=i+1</span><br><span class="line">      else</span><br><span class="line">          C[k]=B[j], j=j+1</span><br><span class="line">      end</span><br><span class="line">      k=k+1</span><br><span class="line">  end</span><br><span class="line">  if i=n_A then</span><br><span class="line">      C[k,⋯]=B[j,⋯]</span><br><span class="line">  else</span><br><span class="line">      C[k,⋯]=A[i,⋯]</span><br><span class="line">  end</span><br></pre></td></tr></table></figure>
<h2 id="quick-sort">Quick Sort</h2>
<p>O(nlogn)</p>
<p>in-place</p>
]]></content>
      <categories>
        <category>DS</category>
      </categories>
  </entry>
  <entry>
    <title>GeneralReading</title>
    <url>/2023/03/29/GeneralReading/</url>
    <content><![CDATA[<p>MKR,RKGE,HAGERec,entity2rec,HAKG</p>
<span id="more"></span>
<h1 id="mkr">MKR</h1>
<h2 id="framework">Framework</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230329182212440.png"
alt="image-20230329182212440" />
<figcaption aria-hidden="true">image-20230329182212440</figcaption>
</figure>
<p>The framework of MKR is illustrated in Figure 1a.</p>
<p>MKR consists of three main components: recommendation module, KGE
module, and cross&amp;compress units.</p>
<ol type="1">
<li><p>The recommendation module on the left takes a user and an item as
input, and uses a multi-layer perceptron (MLP) and cross&amp;compress
units to extract short and dense features for the user and the item,
respectively. The extracted features are then fed into another MLP
together to output the predicted probability.</p></li>
<li><p>Similar to the left part, the KGE module in the right part also
uses multiple layers to extract features from the head and relation of a
knowledge triple, and outputs the representation of the predicted tail
under the supervision of a score function f and the real tail.</p></li>
<li><p>The recommendation module and the KGE module are bridged by
specially designed cross&amp;compress units. The proposed unit can
automatically learn high-order feature interactions of items in
recommender systems and entities in the knowledge graph.</p></li>
</ol>
<p>u: MLP to update</p>
<p>v: cross&amp;compress units</p>
<p>r: MLP to update</p>
<p>h: cross&amp;compress units</p>
<h2 id="loss-function">Loss function</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230329184018259.png"
alt="image-20230329184018259" />
<figcaption aria-hidden="true">image-20230329184018259</figcaption>
</figure>
<h1 id="rkge">RKGE</h1>
<p>RKGE first automatically mines all qualified paths between entity
pairs from the KG, which are then encoded via a batch of recurrent
networks, with each path modeled by a single recurrent network.</p>
<p>It then employs a pooling operation to discriminate the importance of
different paths for characterizing user preferences towards items.</p>
<h2 id="framework-1">framework</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230329205324109.png"
alt="image-20230329205324109" />
<figcaption aria-hidden="true">image-20230329205324109</figcaption>
</figure>
<h3 id="semantic-path-mining">Semantic Path Mining</h3>
<p>Strategy</p>
<ol type="1">
<li>We only consider user-to-item paths <span
class="math inline">\(P(u_i,v_j)\)</span> that connect <span
class="math inline">\(u, i\)</span> with all her rated items.</li>
<li>We enumerate paths with a length constraint.</li>
</ol>
<h3 id="encode-path">Encode path</h3>
<p>use recurrent networks</p>
<h4 id="embedding-layer">Embedding layer</h4>
<p>generate the embedding of entities</p>
<h4 id="attention-gated-hidden-layer">Attention-Gated Hidden Layer</h4>
<p>就是一个RNN网络的变种</p>
<h1 id="hagerec">HAGERec</h1>
<h2 id="framework-2">Framework</h2>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230330191130895.png"
alt="image-20230330191130895" />
<figcaption aria-hidden="true">image-20230330191130895</figcaption>
</figure>
<p>four components:</p>
<ol type="1">
<li><p>Flatten and embedding layer: flatten complex high-order relations
and embedding the entities and relations as vectors.</p></li>
<li><p>GCN learning layer: uses GCN model to propagate and update user’s
and item’s embedding via a bi-directional entity propagation
strategy</p></li>
<li><p>Interaction signals unit: preserves interaction signals structure
of an entity and its neighbor network to give a more complete picture
for user’s and item’s representation.</p></li>
<li><p>Prediction layer: utilizes the user’s and item’s aggregated
representation with prediction-level attention to output the predicted
score.</p></li>
</ol>
<h3 id="flatten-and-embedding">Flatten and embedding</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330193624700.png"
alt="image-20230330193624700" />
<figcaption aria-hidden="true">image-20230330193624700</figcaption>
</figure>
<p>flatten high-order connection to the path: <span
class="math inline">\(u\rightarrow^{r1}v\rightarrow^{r2}e_{u1}\rightarrow^{r3}e_{v3}\)</span></p>
<p>embedding: initialized embedding vectors.</p>
<h3 id="gcn-learning-unit">GCN learning unit</h3>
<p>user and item use the same propagation and aggregate strategy.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330200154040.png"
alt="image-20230330200154040" />
<figcaption aria-hidden="true">image-20230330200154040</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330200204043.png"
alt="image-20230330200204043" />
<figcaption aria-hidden="true">image-20230330200204043</figcaption>
</figure>
<p><span class="math inline">\(h^T, W, b\)</span>: learned
parameters</p>
<p>neighbor sample(only get fixed number neighbor): <span
class="math inline">\(\alpha_{e_v,e_{nv}}\)</span> would be regarded as
the similarity of each neighbor entity and central entity. Through this
evidence, those neighbors with lower similarity would be filtered.</p>
<h3 id="interaction-signals-unit">Interaction signals unit</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330201306937.png"
alt="image-20230330201306937" />
<figcaption aria-hidden="true">image-20230330201306937</figcaption>
</figure>
<p>区别：上面是相加，下面事相乘</p>
<p>so GCN unit + interaction unit =</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330201407720.png"
alt="image-20230330201407720" />
<figcaption aria-hidden="true">image-20230330201407720</figcaption>
</figure>
<h3 id="predict">Predict</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330201630216.png"
alt="image-20230330201630216" />
<figcaption aria-hidden="true">image-20230330201630216</figcaption>
</figure>
<h1 id="entity2rec">Entity2rec</h1>
<h2 id="framework-3">Framework</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331170104462.png"
alt="image-20230331170104462" />
<figcaption aria-hidden="true">image-20230331170104462</figcaption>
</figure>
<h3 id="node2vec">node2vec</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331165454743.png"
alt="image-20230331165454743" />
<figcaption aria-hidden="true">image-20230331165454743</figcaption>
</figure>
<p>将图用random walk转化为word格式，用词袋模型计算vector。</p>
<h3 id="property-specific-knowledge-graph-embedding">Property-specific
knowledge graph embedding</h3>
<p>在node2vec基础上加上relation embedding，基于p子图在p空间上优化node
vector</p>
<p>maximize the dot product between vectors of the same neighborhood</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331170246301.png"
alt="image-20230331170246301" />
<figcaption aria-hidden="true">image-20230331170246301</figcaption>
</figure>
<p>Ze-negative sampling</p>
<p>N(e): neighbor of entity</p>
<h3 id="subgraph">subgraph</h3>
<h4 id="collaborative-content-subgraphs">Collaborative-content
subgraphs</h4>
<p>只保留单一relation，但连接性很差，对random walk效果不好</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331170405382.png"
alt="image-20230331170405382" />
<figcaption aria-hidden="true">image-20230331170405382</figcaption>
</figure>
<p>所有子图可以分成两张类型：feedback子图(user-item图)和其他子图</p>
<p>用下面方法来计算推荐分数：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331170819790.png"
alt="image-20230331170819790" />
<figcaption aria-hidden="true">image-20230331170819790</figcaption>
</figure>
<p>R+(u) denotes a set of items liked by the user u in the past.</p>
<p>s(x): similarity socre</p>
<h4 id="hybrid-subgraphs">Hybrid subgraphs</h4>
<p><span class="math inline">\(K_p^+=K_p \cup(u,feedback,i)\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331171032188.png"
alt="image-20230331171032188" />
<figcaption aria-hidden="true">image-20230331171032188</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331171139039.png"
alt="image-20230331171139039" />
<figcaption aria-hidden="true">image-20230331171139039</figcaption>
</figure>
<h1 id="hakg">HAKG</h1>
<h2 id="framework-4">Framework</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230401160018626.png"
alt="image-20230401160018626" />
<figcaption aria-hidden="true">image-20230401160018626</figcaption>
</figure>
<ol type="1">
<li>Subgraph Construction ：it automatically constructs the expressive
subgraph that links the user-item pair to represent their
connectivity;</li>
<li>Hierarchical Attentive Subgraph Encoding ： the subgraph is further
encoded via a hierarchical attentive embedding learning procedure, which
first learns embeddings for entities in the subgraph with a layer-wise
propagation mechanism, and then attentively aggregates the entity
embeddings to derive the holistic subgraph embedding;</li>
<li>Preference Prediction ： with the well-learned embeddings of the
user-item pair and their subgraph connectivity, it uses non-linear
layers to predict the user’s preference towards the item.</li>
</ol>
<h3 id="subgraph-construction">Subgraph Construction</h3>
<p>path sampling and then reconstructs the subgraphs by assembling the
sampled paths between user-item pairs</p>
<h4 id="path-sampling">path sampling</h4>
<p>use random walk get path from u to i and length&lt;=6, uniformly
sample K paths</p>
<h4 id="path-assembling">Path Assembling</h4>
<p>just assemb the K paths</p>
<h3 id="hierarchical-attentive-subgraph-encoding">Hierarchical attentive
subgraph encoding</h3>
<h4 id="entity-embedding-learning">entity embedding learning</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230401173559684.png"
alt="image-20230401173559684" />
<figcaption aria-hidden="true">image-20230401173559684</figcaption>
</figure>
<h5 id="embedding-initialization">Embedding Initialization</h5>
<ol type="1">
<li>initial</li>
<li><span class="math inline">\(e_h^{(0)}=MLP(e_h \space concatenation
\space t_h)\)</span></li>
</ol>
<h5 id="semantics-propagation">Semantics Propagation</h5>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230401173656033.png"
alt="image-20230401173656033" />
<figcaption aria-hidden="true">image-20230401173656033</figcaption>
</figure>
<h5 id="semantics-aggregation">Semantics Aggregation</h5>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230401173801411.png"
alt="image-20230401173801411" />
<figcaption aria-hidden="true">image-20230401173801411</figcaption>
</figure>
<p>final entity embedding is <span
class="math inline">\(e_h^{(L)}\)</span></p>
<p>constitute an entity embedding matrix H(u,i) for the whole subgraph
:</p>
<p><span
class="math inline">\(H_{(u,i)}=[e_1,e_2,\cdots,e_n]\)</span></p>
<h4 id="sub-graph-embedding-learning">sub-graph embedding learning</h4>
<p>use self-attention mechanism optimize the entities embeding of
subgraph</p>
<p>Than use pooling method to get subgraph embedding</p>
<h3 id="prediction">Prediction</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230401175552413.png"
alt="image-20230401175552413" />
<figcaption aria-hidden="true">image-20230401175552413</figcaption>
</figure>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>HyperGraph</title>
    <url>/2023/05/26/HyperGraph/</url>
    <content><![CDATA[<p>Hypergraph</p>
<span id="more"></span>
<h1 id="hgnn">HGNN</h1>
<p>use the matrix to represent hypergraph</p>
<p>a hyperedge convolution operation is designed</p>
<p>can incorporate with multi-modal data and complicated data
correlations(use below figure2 method to combine different data type
)</p>
<h2 id="hypergraph-and-adjacency-matrix">Hypergraph and adjacency
matrix</h2>
<p>hypergraph-一条边可以同时连接多个点</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230605145232861.png"
alt="image-20230605145232861" />
<figcaption aria-hidden="true">image-20230605145232861</figcaption>
</figure>
<p>adjacency matrix:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230605145324681.png"
alt="image-20230605145324681" />
<figcaption aria-hidden="true">image-20230605145324681</figcaption>
</figure>
<h2 id="method">Method</h2>
<h3 id="hypergraph-learning-statement">hypergraph learning
statement</h3>
<p>hypergraph defined as <span class="math inline">\(G = (V,\varepsilon,
W)\)</span></p>
<p>the degree of vertex <span class="math inline">\(v\)</span> defined
as <span class="math inline">\(d(v)=\sum_{e\in
\varepsilon}h(v,e)\)</span>——一个点与多少条边相连</p>
<p>the degree of hyperedge defined as <span class="math inline">\(\delta
(v)=\sum_{v\in V}h(v,e)\)</span>——一条边与多少个点相连</p>
<p><span class="math inline">\(D_e\)</span> and <span
class="math inline">\(D_v\)</span> denote the diagonal matrices of the
edge degrees and the vertex degrees, respectively</p>
<p>example:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230605153837827.png"
alt="image-20230605153837827" />
<figcaption aria-hidden="true">image-20230605153837827</figcaption>
</figure>
<h3 id="spectral-convolution-on-hypergraph">Spectral convolution on
hypergraph</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230605155756968.png"
alt="image-20230605155756968" />
<figcaption aria-hidden="true">image-20230605155756968</figcaption>
</figure>
<p>频域理解不深，要完全看懂要好久，先简单略一下。</p>
<h1 id="hypergcn">HyperGCN</h1>
<p>Hypergraph, a novel way of training a GCN for SSL on hypergraphs
based on tools from sepctral theory of hypergraphs</p>
<p>主要是把hypergraph转为简单拉普拉斯图</p>
<h2 id="method-1">Method</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230606094309361.png"
alt="image-20230606094309361" />
<figcaption aria-hidden="true">image-20230606094309361</figcaption>
</figure>
<h3 id="hypergraph-generation">hypergraph generation</h3>
<ol type="1">
<li>对图上任意边<span class="math inline">\(e\in E\)</span>, 令<span
class="math inline">\((i_e,j_e):=argmax_{i,j\in e}|S_i-S_j|\)</span>
,即返回同一个边上距离最远的两个顶点表示<span
class="math inline">\((i_e,j_e)\)</span>为随机,切断点的联系。</li>
<li>为剩下的边添加权重，权重为hyperedge的权重，构造简单的邻接矩阵。</li>
<li>归一化计算拉普拉斯矩阵</li>
</ol>
<h3 id="gnn">GNN</h3>
<p>利用带权重的拉普拉斯矩阵计算GCN</p>
<h1 id="hypergraph-convolution-and-hypergraph-attention">Hypergraph
Convolution and Hypergraph Attention</h1>
]]></content>
      <categories>
        <category>GNN</category>
        <category>hypergraph</category>
      </categories>
  </entry>
  <entry>
    <title>Initialization</title>
    <url>/2023/06/12/Initialization/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>KGAT</title>
    <url>/2023/02/24/KGAT/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>利用KG作为辅助信息，并将KG与user-item graph 整合为一个图</p>
<h2 id="background-1">Background</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224155340260.png"
alt="image-20230224155340260" />
<figcaption aria-hidden="true">image-20230224155340260</figcaption>
</figure>
<p><strong>Previous model:</strong></p>
<p>CF: behaviorally similar users would exhibit similar preference on
items.</p>
<p><strong>focus on the histories of similar users who also watched
<span class="math inline">\(i1\)</span>, i.e., <span
class="math inline">\(u4\)</span> and <span
class="math inline">\(u5\)</span>;</strong></p>
<p>SL: transform side information into a generic feature vector,
together with user ID and item ID, and feed them into a supervised
learning (SL) model to predict the score.</p>
<p><strong>emphasize the similar items with the attribute <span
class="math inline">\(e1\)</span>, i.e.$ i2$.</strong></p>
<p><strong>current problem:</strong></p>
<p>existing SL methods fail to unify them, and ignore other
relationships in the graph:</p>
<ol type="1">
<li>the users in the yellow circle who watched other movies directed by
the same person <span class="math inline">\(e_1\)</span>.</li>
<li>the items in the grey circle that share other common relations with
<span class="math inline">\(e_1\)</span>.</li>
</ol>
<h2 id="user-item-bipartite-graph-g_1">User-Item Bipartite Graph: <span
class="math inline">\(G_1\)</span></h2>
<p><span class="math display">\[
\{(u,y_{ui},i)|u\in U, i\in I\}
\]</span> <span class="math inline">\(U\)</span>: user sets</p>
<p><span class="math inline">\(I\)</span>: item sets</p>
<p><span class="math inline">\(y_{ui}\)</span>: if user <span
class="math inline">\(u\)</span> interacts with item <span
class="math inline">\(i\)</span> <span
class="math inline">\(y_{ui}\)</span>=, else <span
class="math inline">\(y_{ui}\)</span>=0.</p>
<h2 id="knowledge-graph-g2">Knowledge Graph <span
class="math inline">\(G2\)</span></h2>
<p><span class="math display">\[
\{(h,r,t)|h,t\in E, r\in R\}
\]</span></p>
<p><span class="math inline">\(t\)</span> there is a relationship <span
class="math inline">\(r\)</span> from head entity <em>h</em> to tail
entity <span class="math inline">\(t\)</span>.</p>
<h2 id="ckg-combination-of-g1-and-g2"><span
class="math inline">\(CKG\)</span>: Combination of <span
class="math inline">\(G1\)</span> and <span
class="math inline">\(G2\)</span></h2>
<ol type="1">
<li>represent each user-item behavior as a triplet $ (u,
Interact,i)<span class="math inline">\(, where\)</span> y^{ui}$ =
1.</li>
<li>we establish a set of item-entity alignments</li>
</ol>
<p><span class="math display">\[
A = \{(i, e)|i ∈ I, e ∈ E \}
\]</span></p>
<ol start="3" type="1">
<li>based on the item-entity alignment set, the user-item graph can be
integrated with KG as a unified graph.</li>
</ol>
<p><span class="math display">\[
G = \{(h,r,t)|h,t ∈ E^′,r ∈R^′\}
\]</span></p>
<p><span class="math display">\[
E^′ = E ∪ U
\]</span></p>
<p><span class="math display">\[
R^′ = R ∪ {Interact}
\]</span></p>
<h1 id="methodology">Methodology</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222185609261.png"
alt="image-20230222185609261" />
<figcaption aria-hidden="true">image-20230222185609261</figcaption>
</figure>
<p>KGAT has three main components:</p>
<ol type="1">
<li>Embedding layer</li>
<li>Attentive embedding propagation layer</li>
<li>prediction layer</li>
</ol>
<h2 id="embedding-layer">Embedding layer</h2>
<p>Using <strong>TransR</strong> to calculate embedding</p>
<p><strong>Assumption</strong>: if a triplet (h,r,t) exist in the graph,
<span class="math display">\[
e^r_h+e_r\approx e_t^r
\]</span> Herein, <span class="math inline">\(e^h\)</span>, <span
class="math inline">\(e^t\)</span> ∈ <span
class="math inline">\(R^d\)</span> and <span
class="math inline">\(e^r\)</span> ∈ <span
class="math inline">\(R^k\)</span>are the embedding for <em>h</em>,
<em>t</em>, and <em>r</em>; and <span
class="math inline">\(e^r_h\)</span>, <span
class="math inline">\(e^r_t\)</span> are the projected representations
of <span class="math inline">\(e^h\)</span>, <span
class="math inline">\(e^t\)</span> in the relation <em>r</em>’s
space.</p>
<p><strong>Plausibility score</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222193700417.png"
alt="image-20230222193700417" />
<figcaption aria-hidden="true">image-20230222193700417</figcaption>
</figure>
<p><span class="math inline">\(W_r ∈ R^{k\times d}\)</span> is the
transformation matrix of relation <em>r</em>, which projects entities
from the <em>d</em>-dimension entity space into the <em>k</em> dimension
relation space.</p>
<p>A lower score suggests that the triplet is more likely to be
true.</p>
<p><strong>Loss</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222195306105.png"
alt="image-20230222195306105" />
<figcaption aria-hidden="true">image-20230222195306105</figcaption>
</figure>
<p><span class="math inline">\(\{(h,r,t,t^′ )|(h,r,t) \in G, (h,r,t^′ )
\notin G\}\)</span>, <span class="math inline">\((h,r,t^′ )\)</span> is
a negative sample constructed by replacing one entity in a valid triplet
randomly.</p>
<p><em>σ</em>(·): sigmoid function, ——》将分数映射再0-1区间，归一化</p>
<p>？？？？？？？？？？？why this layer model working as a
regularizer</p>
<h2 id="attentive-embedding-propagation-layersupon-gcn">Attentive
Embedding Propagation Layers(upon GCN)</h2>
<h3 id="first-order-propagation">First-order propagation</h3>
<p>和之前模型不同，这个的propagation layer encode了<span
class="math inline">\(e_r\)</span>.</p>
<p>For entity h, the information propagating from neighbor is :</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222201217039.png"
alt="image-20230222201217039" />
<figcaption aria-hidden="true">image-20230222201217039</figcaption>
</figure>
<p><span class="math inline">\(π(h,r,t)\)</span>: to controls the decay
factor on each propagation on edge (<em>h</em>,<em>r</em>,<em>t</em>),
indicating how much information is propagated</p>
<p>from <em>t</em> to <em>h</em> conditioned to relation <em>r</em>.</p>
<p>For <span class="math inline">\(π(h,r,t)\)</span>, we use attention
mechanism:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222204949423.png"
alt="image-20230222204949423" />
<figcaption aria-hidden="true">image-20230222204949423</figcaption>
</figure>
<p>This makes the attention score dependent on the distance between
<span class="math inline">\(e^h\)</span> and <span
class="math inline">\(e^t\)</span> in the relation <em>r</em>’s
space.</p>
<p>这里，tanh用于增加非线性因素；但不缺定是否有归一化作用？？？？？归一化就可以把这个function的大小集中在角度上，但是这样<span
class="math inline">\(e^h_t\)</span>也没有归一化，到时候看看输出参数</p>
<p>and than use softmax to normalize(no need to use as<span
class="math inline">\(\frac1{|N_t |}\)</span><span
class="math inline">\(\frac1{|N_t ||N_h |}\)</span>)</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222234256082.png"
alt="image-20230222234256082" />
<figcaption aria-hidden="true">image-20230222234256082</figcaption>
</figure>
<p>The final part is aggregation, threre are three choices:</p>
<ol type="1">
<li>GCN aggregator</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222235616598.png"
alt="image-20230222235616598" />
<figcaption aria-hidden="true">image-20230222235616598</figcaption>
</figure>
<ol start="2" type="1">
<li>GraphSage aggregator</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222235806956.png"
alt="image-20230222235806956" />
<figcaption aria-hidden="true">image-20230222235806956</figcaption>
</figure>
<ol start="3" type="1">
<li>Bi-Interaction aggregator</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223000647293.png"
alt="image-20230223000647293" />
<figcaption aria-hidden="true">image-20230223000647293</figcaption>
</figure>
<h3 id="multi-layer-propagation">Multi-layer propagation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223000834658.png"
alt="image-20230223000834658" />
<figcaption aria-hidden="true">image-20230223000834658</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223000850960.png"
alt="image-20230223000850960" />
<figcaption aria-hidden="true">image-20230223000850960</figcaption>
</figure>
<h2 id="model-prediction">Model Prediction</h2>
<p>multi-layers combination and inner product</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223001515690.png"
alt="image-20230223001515690" />
<figcaption aria-hidden="true">image-20230223001515690</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223001526127.png"
alt="image-20230223001526127" />
<figcaption aria-hidden="true">image-20230223001526127</figcaption>
</figure>
<h2 id="optimizazion">Optimizazion</h2>
<h3 id="loss">loss</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223002144083.png"
alt="image-20230223002144083" />
<figcaption aria-hidden="true">image-20230223002144083</figcaption>
</figure>
<p><span class="math inline">\(L_{cf}\)</span> is BPR Loss</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223002439536.png"
alt="image-20230223002439536" />
<figcaption aria-hidden="true">image-20230223002439536</figcaption>
</figure>
<p><span class="math inline">\(L_{kg}\)</span> is loss forTranR .</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222195306105.png"
alt="image-20230222195306105" />
<figcaption aria-hidden="true">image-20230222195306105</figcaption>
</figure>
<h3 id="optimizer">Optimizer</h3>
<p>Adam</p>
<h3 id="updata-method">updata method</h3>
<p>we update the embeddings for all nodes;</p>
<p>hereafter, we sample a batch of (<em>u</em>,<em>i</em>, <em>j</em>)
randomly, retrieve their representations after <em>L</em> steps of
propagation, and then update model parameters by using the gradients of
the prediction loss.</p>
<p>在同一个epoch中，先把所以数据扔进tranR训练，得到loss（此时不更新参数）</p>
<p>然后sample算BPR LOSS</p>
<h1 id="experiments">EXPERIMENTS</h1>
<h2 id="rq1-performance-comparison">RQ1: Performance Comparison</h2>
<ol type="1">
<li>regular dataset</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223004843914.png"
alt="image-20230223004843914" />
<figcaption aria-hidden="true">image-20230223004843914</figcaption>
</figure>
<ol start="2" type="1">
<li><p>Sparsity Levels</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223005253034.png"
alt="image-20230223005253034" />
<figcaption aria-hidden="true">image-20230223005253034</figcaption>
</figure></li>
</ol>
<p>KGAT outperforms the other models in most cases, especially on the
two sparsest user groups.</p>
<p>说明KGAT能够缓解稀疏性影响</p>
<h2 id="rq2study-of-kgat">RQ2：Study of KGAT</h2>
<ol type="1">
<li>study of layer influence and effect of aggregators</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223010038345.png"
alt="image-20230223010038345" />
<figcaption aria-hidden="true">image-20230223010038345</figcaption>
</figure>
<ol start="2" type="1">
<li><p>cut attention layer and TransR layer</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223010347815.png"
alt="image-20230223010347815" />
<figcaption aria-hidden="true">image-20230223010347815</figcaption>
</figure></li>
</ol>
<h1 id="source-code">Source code</h1>
<h2 id="dataprocess">DataProcess</h2>
<h3 id="load-data">Load data</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data:[[u1,interacted_item1],[u1,interacted_item2],[u2,interacted_item1]]</span><br><span class="line"></span><br><span class="line">train_user_dict:&#123;</span><br><span class="line">    user_id1:[interacted_item1,interacted_item2,...],</span><br><span class="line">    user_id2:[...]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">kg_data:[[head_e,relation,tail_e],[head_e,relation,tail_e]]</span><br><span class="line"></span><br><span class="line">kg_dict:&#123;</span><br><span class="line">    head:[(tail,relation), (tail,relation),...]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">relation_dict:&#123;</span><br><span class="line">    relation:[(head,tail),(head,tail),...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3
id="generate-the-adjacency-matrices-and-matrices-after-laplacian">generate
the adjacency matrices and matrices after Laplacian</h3>
<ol type="1">
<li><p>regard interacted as relation 0, now the number of relations is
<span class="math inline">\(self.n\_relations+1\)</span></p></li>
<li><p>every relation <span class="math inline">\((idx)\)</span> convert
to 2 adjacency matrix (by inversing cols and rows), which representate
as 2 new relations <span class="math inline">\((idx,
self.n\_relations+idx)\)</span>：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/643ED74EDA6A241DF772EA9C9435EFBD.png"
alt="643ED74EDA6A241DF772EA9C9435EFBD" />
<figcaption
aria-hidden="true">643ED74EDA6A241DF772EA9C9435EFBD</figcaption>
</figure></li>
</ol>
<p>As a result: we get adj_list, adj_r_list</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">adj_list: [adjancy matrix1, adjancy matrix2,adjancy matrix3,...]</span><br><span class="line">adj_r_list: The relation adjancy matrix Correspondento to</span><br><span class="line">			e.g.[0,self.n_relations+0,1,self.n_relations+1,2,self.n_relations+2,...]</span><br></pre></td></tr></table></figure>
<p>Than, genarate adjancy matrix after laplacian normalization and save
in self.lap_list.</p>
<h3 id="update-kg-dict">Update kg dict</h3>
<p>according to the change of relation, update kg dict</p>
<h3 id="generate-batch-data">Generate batch data</h3>
<h2 id="build_model">build_model</h2>
<h3 id="placeholder-definition">Placeholder definition</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_build_inputs</span>(<span class="params">self</span>):</span><br><span class="line">    tf.compat.v1.disable_eager_execution()</span><br><span class="line">    <span class="comment"># placeholder definition</span></span><br><span class="line">    self.users = tf.placeholder(tf.int32, shape=(<span class="literal">None</span>,))</span><br><span class="line">    self.pos_items = tf.placeholder(tf.int32, shape=(<span class="literal">None</span>,))</span><br><span class="line">    self.neg_items = tf.placeholder(tf.int32, shape=(<span class="literal">None</span>,))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># for knowledge graph modeling (TransD)</span></span><br><span class="line">    self.A_values = tf.placeholder(tf.float32, shape=[<span class="built_in">len</span>(self.all_v_list)],</span><br><span class="line">                                   name=<span class="string">&#x27;A_values&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    self.h = tf.placeholder(tf.int32, shape=[<span class="literal">None</span>], name=<span class="string">&#x27;h&#x27;</span>)</span><br><span class="line">    self.r = tf.placeholder(tf.int32, shape=[<span class="literal">None</span>], name=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    self.pos_t = tf.placeholder(tf.int32, shape=[<span class="literal">None</span>], name=<span class="string">&#x27;pos_t&#x27;</span>)</span><br><span class="line">    self.neg_t = tf.placeholder(tf.int32, shape=[<span class="literal">None</span>], name=<span class="string">&#x27;neg_t&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="trainable-weight-definition">trainable weight definition</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_build_weights</span>(<span class="params">self</span>):</span><br><span class="line">    all_weights = <span class="built_in">dict</span>()</span><br><span class="line">    initializer = tf.keras.initializers.glorot_normal()</span><br><span class="line"></span><br><span class="line">    all_weights[<span class="string">&#x27;user_embed&#x27;</span>] = tf.Variable(initializer([self.n_users, self.emb_dim]),</span><br><span class="line">                                            name=<span class="string">&#x27;user_embed&#x27;</span>)</span><br><span class="line">    all_weights[<span class="string">&#x27;entity_embed&#x27;</span>] = tf.Variable(initializer([self.n_entities,</span><br><span class="line">                                                           self.emb_dim]),</span><br><span class="line">                                                           name=<span class="string">&#x27;entity_embed&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    all_weights[<span class="string">&#x27;relation_embed&#x27;</span>] = tf.Variable(initializer([self.n_relations,</span><br><span class="line">                                             self.kge_dim]),name=<span class="string">&#x27;relation_embed&#x27;</span>)</span><br><span class="line">    <span class="comment"># E_h, E_t to E_r space</span></span><br><span class="line">    all_weights[<span class="string">&#x27;trans_W&#x27;</span>] = tf.Variable(initializer([self.n_relations, </span><br><span class="line">                                                      self.emb_dim, self.kge_dim]))</span><br><span class="line">    self.weight_size_list = [self.emb_dim] + self.weight_size</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>KGCN</title>
    <url>/2023/03/02/KGCN/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<h2 id="cf-questions">CF Questions</h2>
<ol type="1">
<li>sparsity</li>
<li>cold start</li>
</ol>
<h2 id="kg-benefits">KG Benefits</h2>
<ol type="1">
<li>The rich semantic relatedness among items in a KG can help explore
their latent connections and improve the <em>precision</em> of
results;</li>
<li>The various types of relations in a KG are helpful for extending a
user’s interests reasonably and increasing the <em>diversity</em> of
recommended items;</li>
<li>KG connects a user’s historically-liked and recommended items,
thereby bringing <em>explainability</em> to recommender systems.</li>
</ol>
<h2 id="previous-kg-method">Previous KG Method</h2>
<h3 id="knowledge-graph-embedding">Knowledge graph embedding</h3>
<p>Example: TransE [1] and TransR [12] assume <em>head</em> +
<em>relation</em> = <em>tail</em>, which focus on modeling rigorous
semantic relatedness</p>
<p>Problem: KGE methods are more suitable for in-graph applications such
as KG completion and link prediction rather than the recommendation
system.</p>
<h3 id="path-base-method">Path-base Method</h3>
<p>Example: PER, FMG</p>
<p>problem: Labor sensitivity</p>
<h2 id="ripple-net">Ripple Net</h2>
<p>problem:</p>
<ol type="1">
<li>the importance of relations is weakly characterized in RippleNet,
because the relation <strong>R</strong> can hardly be trained to capture
the sense of importance in the quadratic form <strong>v</strong>
⊤<strong>Rh</strong> (<strong>v</strong> and <strong>h</strong> are
embedding vectors of two entities).</li>
<li>The size of ripple set may go unpredictably with the increase of the
size of KG, which incurs heavy computation and storage overhead.</li>
</ol>
<h2 id="solution-kgcn">Solution: KGCN</h2>
<ol type="1">
<li>Propagation and aggregation mechanism.</li>
<li>Attention mechanism.</li>
<li>sample a fixed-size neighborhood to control compute cost.</li>
</ol>
<h1 id="model">Model</h1>
<h2 id="single-layer">Single layer</h2>
<p>Consider a pair(u,v)</p>
<h3 id="overall-of-single-layer">Overall of single layer</h3>
<p>!!!Propagation only use for updating of item's vector</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114017500.png"
alt="image-20230302114017500" />
<figcaption aria-hidden="true">image-20230302114017500</figcaption>
</figure>
<h3 id="propagation">Propagation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302111400335.png"
alt="image-20230302111400335" />
<figcaption aria-hidden="true">image-20230302111400335</figcaption>
</figure>
<p><span class="math inline">\(N(v)\)</span> is the neighbor set of
v</p>
<p>e$ is the embedding of entity(parameter to train)</p>
<p><span class="math inline">\(\pi^u_{r_{v,e}}\)</span> is attention
weight</p>
<p><span class="math inline">\(r_{v,e}\)</span> represent the relation
of v and e</p>
<h3 id="attention">Attention</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302112401231.png"
alt="image-20230302112401231" />
<figcaption aria-hidden="true">image-20230302112401231</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302112315690.png"
alt="image-20230302112315690" />
<figcaption aria-hidden="true">image-20230302112315690</figcaption>
</figure>
<p><span class="math inline">\(g : R^d ×R^d → R\)</span> (e.g., inner
product:内积能计算相似度)to compute the score between a user and a
relation</p>
<p><span class="math inline">\(u\in R\)</span>, <span
class="math inline">\(r\in R\)</span> : embedding of user and
relation(parameter to train)</p>
<p><span class="math inline">\(\pi^u_{r_{v,e}}\)</span>characterizes the
importance of relation <em>r</em> to user <em>u</em>. E.g. example, a
user may have more interests in the movies that share the same “star"
with his historically liked ones, while another user may be more
concerned about the “genre" of movies.</p>
<p>!!!!!!!!!!个性化！！！用户对不同关系重视程度不同！！</p>
<p>所以KGCN不用propagation更新用户的原因是否是因为希望user的embedding能专注于提取个性化信息（提高用户和重要relation的相似度），但是这样是否会让user和item没那么好聚类？</p>
<h3 id="sample-the-number-of-neighbors">Sample the number of
neighbors</h3>
<p>limit the neighbor number in K(can be config)</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302113734945.png"
alt="image-20230302113734945" />
<figcaption aria-hidden="true">image-20230302113734945</figcaption>
</figure>
<p>S(<em>v</em>) is also called the (single layer) <em>receptive
field</em> of entity <em>v</em></p>
<p>example K=2:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302113907523.png"
alt="image-20230302113907523" />
<figcaption aria-hidden="true">image-20230302113907523</figcaption>
</figure>
<h3 id="aggregation">aggregation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114047762.png"
alt="image-20230302114047762" />
<figcaption aria-hidden="true">image-20230302114047762</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114057869.png"
alt="image-20230302114057869" />
<figcaption aria-hidden="true">image-20230302114057869</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114106975.png"
alt="image-20230302114106975" />
<figcaption aria-hidden="true">image-20230302114106975</figcaption>
</figure>
<p><span class="math inline">\(\sigma\)</span> is ReLU</p>
<h2 id="multi-layer">Multi layer</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/9C6A6E4346910DDDD43EBB55E1A2FE6C.png"
alt="9C6A6E4346910DDDD43EBB55E1A2FE6C" />
<figcaption
aria-hidden="true">9C6A6E4346910DDDD43EBB55E1A2FE6C</figcaption>
</figure>
<p>First we consider the Receptive-Field:</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/1739B546615C1AEDC69F7A3514E2E458.png" alt="1739B546615C1AEDC69F7A3514E2E458" style="zoom:67%;" /></p>
<p>We first update eneities in M[0] by using propagation and aggregation
to get the high-hop neighbor information.</p>
<p>And then gradually narrow it down, and finally focus on v.</p>
<p>Note that we have only one user in one pair, every relations will
calculate the score with this user embedding</p>
<h2 id="predict">Predict</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114437063.png"
alt="image-20230302114437063" />
<figcaption aria-hidden="true">image-20230302114437063</figcaption>
</figure>
<h2 id="loss-function">Loss function</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302120154981.png"
alt="image-20230302120154981" />
<figcaption aria-hidden="true">image-20230302120154981</figcaption>
</figure>
<p><span class="math inline">\(J\)</span> is cross-entropy loss</p>
<p><em>P</em> is a negative sampling distribution, and <span
class="math inline">\(T_u\)</span> is the number of negative samples for
user <em>u</em>. In this paper,</p>
<h1 id="experiment">Experiment</h1>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>KR-GCN</title>
    <url>/2023/04/26/KR-GCN/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>previous study:</p>
<ol type="1">
<li>error propagation: consider all paths between every user-item pair
might involve irrelevant one</li>
<li>weak explainability</li>
</ol>
<h1 id="model">Model</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230426111943016.png"
alt="image-20230426111943016" />
<figcaption aria-hidden="true">image-20230426111943016</figcaption>
</figure>
<p>4 parts:</p>
<ol type="1">
<li>the Graph encoding module: learn the representations of nodes in the
heterogeneous graph.</li>
<li>the Path Extraction and Selection module: extract paths between
users and items from the heterogeneous graph and select higher-quality
reasoning paths</li>
<li>the Path Encoding module: learn the representations of the selection
reasoning paths.</li>
<li>the Preference Prediction module: predicts users’ preferences
according to the reasoning paths.</li>
</ol>
<h2 id="graph-encoding---gcn">Graph Encoding - GCN</h2>
<ol type="1">
<li>propagation and aggregation</li>
</ol>
<p>initialized randomly</p>
<p>weighted sum aggregator : the neighborhood nodes are aggregated via
mean function.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230426113233853.png"
alt="image-20230426113233853" />
<figcaption aria-hidden="true">image-20230426113233853</figcaption>
</figure>
<ol start="2" type="1">
<li>weight sum to merge every layers</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230426114004010.png"
alt="image-20230426114004010" />
<figcaption aria-hidden="true">image-20230426114004010</figcaption>
</figure>
<h2 id="path-extraction">Path Extraction</h2>
<p>we prune irrelevant paths between each user-item pair.</p>
<p>we extract multi-hop paths with the limitation that hops in every
single path are less than l.</p>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>LightGCN</title>
    <url>/2023/02/24/LightGCN/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>question: why concentrated to sum</p>
<h2 id="main-contributes">Main contributes</h2>
<ol type="1">
<li><p>We empirically show that two common designs in GCN, feature
transformation and nonlinear activation, have no positive effect on the
effectiveness of collaborative filtering.</p>
<p>GCN is originally proposed for node classification on the attributed
graph, where each node has rich attributes as input features; whereas in
the user-item interaction graph for CF, each node (user or item) is only
described by a one-hot ID, which has no concrete semantics besides being
an identifier.</p></li>
<li><p>Propose LightGCN.</p></li>
</ol>
<h1 id="analyze-about-ngcf">Analyze about NGCF</h1>
<h2 id="brief">Brief</h2>
<p>完全想不起来的话建议先看NGCF的笔记</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213192715250.png"
alt="image-20230213192715250" />
<figcaption aria-hidden="true">image-20230213192715250</figcaption>
</figure>
<p>## Some experiment</p>
<h3 id="method">Method</h3>
<p>Using ablation studies, implement three simplified variants of
NGCF:</p>
<ol type="1">
<li>NGCF-f: which removes the feature transformation matrices <span
class="math inline">\(W1\)</span> and <span
class="math inline">\(W2\)</span>.</li>
<li>NGCF-n: which removes the non-linear activation function $ σ$.</li>
<li>NGCF-fn: which removes both the feature transformation matrices and
non-linear activation function.</li>
</ol>
<p><strong>Note</strong>: Since the core of GCN is to refine embeddings
by propagation, we are more interested in the embedding quality under
the same embedding size. Thus, we change the way of obtaining final
embedding from concatenation (i.e., <span
class="math inline">\(e_u^*=e_u^{(0)}\|e_u^{(1)}\|...\|e_u^{(L)}\)</span>)
to sum(i.e., <span
class="math inline">\(e_u^*=e_u^{(0)}+e_u^{(1)}+...+e_u^{(L)}\)</span>).</p>
<p>This change has little effect on NGCF’s performance but makes the
following ablation studies more indicative of the embedding quality
refined by GCN.</p>
<h3 id="result">Result</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213193937873.png"
alt="image-20230213193937873" />
<figcaption aria-hidden="true">image-20230213193937873</figcaption>
</figure>
<ol type="1">
<li>Adding feature transformation imposes negative effect on NGCF, since
removing it in both models of NGCF and NGCF-n improves the performance
significantly;</li>
<li>Adding nonlinear activation affects slightly when feature
transformation is included, but it imposes negative effect when feature
transformation is disabled.</li>
<li>As a whole, feature transformation and nonlinear activation impose
rather negative effect on NGCF, since by removing them simultaneously,
NGCF-fn demonstrates large improvements over NGCF.</li>
</ol>
<h3 id="conclusion">Conclusion</h3>
<p>The deterioration of NGCF stems from the training
difficulty(underfitting), rather than overfitting, because:</p>
<ol type="1">
<li><p>Such lower training loss of NGCF-fn successfully transfers to
better recommendation accuracy.</p></li>
<li><p>NGCF is more powerful and complex, but it demonstrates higher
training loss and worse generalization performance than NGCF-f.</p></li>
</ol>
<h1 id="model-of-lightgcn">Model of LightGCN</h1>
<p>Consisting four parts:</p>
<ol type="1">
<li>initialize users and items embedding.</li>
<li>Light Graph Convolution (LGC)</li>
<li>Layer Combination</li>
<li>Model Prediction</li>
</ol>
<h2 id="light-graph-convolution-lgc">Light Graph Convolution (LGC)</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213194939594.png"
alt="image-20230213194939594" />
<figcaption aria-hidden="true">image-20230213194939594</figcaption>
</figure>
<p>$ $： symmetric normalization, which can avoid the scale of
embeddings increasing with graph convolution operations. Here can use
other normalization, but symmetric normalization has good
performance.</p>
<p><strong>Note</strong>: Without self-connection, because the layer
combination operation of LightGCN captures the same effect as
self-connections.</p>
<h2 id="layer-combination">Layer Combination</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213195607414.png"
alt="image-20230213195607414" />
<figcaption aria-hidden="true">image-20230213195607414</figcaption>
</figure>
<p><span class="math inline">\(α_k\)</span>can be treated as a
hyperparameter to be tuned manually, or as a model parameter, and
setting <span class="math inline">\(α_k\)</span> uniformly as <span
class="math inline">\(1/(K + 1)\)</span> leads to good performance in
general.</p>
<p>This is probably because the training data does not contain
sufficient signal to learn good α that can generalize to unknown
data.</p>
<p>The reason of using the Layer Combination:</p>
<ol type="1">
<li>With the increasing of the number of layers, the embeddings will be
over-smoothed [27]. Thus simply using the last layer is
problematic.</li>
<li>The embeddings at different layers capture different semantics.</li>
<li>Combining embeddings at different layers with weighted sum captures
the effect of graph convolution with self-connections, an important
trick in GCNs.</li>
</ol>
<h2 id="model-prediction">Model Prediction</h2>
<p>inner product</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213200915280.png"
alt="image-20230213200915280" />
<figcaption aria-hidden="true">image-20230213200915280</figcaption>
</figure>
<h2 id="matrix-form">Matrix form</h2>
<p>Similar to NGCF, and there are some explanations in detail in NGCF
note.</p>
<p>Light Graph Convolution:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201201896.png"
alt="image-20230213201201896" />
<figcaption aria-hidden="true">image-20230213201201896</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201107785.png"
alt="image-20230213201107785" />
<figcaption aria-hidden="true">image-20230213201107785</figcaption>
</figure>
<p>Layer combination:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201221159.png"
alt="image-20230213201221159" />
<figcaption aria-hidden="true">image-20230213201221159</figcaption>
</figure>
<h1 id="analyze-about-lightgcn">Analyze about LightGCN</h1>
<h2 id="relation-with-sgcn">Relation with SGCN</h2>
<p><strong>Purpose</strong>: by doing layer combination, LightGCN
subsumes the effect of self-connection thus there is no need for
LightGCN to add self-connection in adjacency matrix.</p>
<p>SGCN: a recent linear GCN model that integrates self-connection into
graph convolution.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201725744.png"
alt="image-20230213201725744" />
<figcaption aria-hidden="true">image-20230213201725744</figcaption>
</figure>
<p>In the following analysis, we omit the <span class="math inline">\((D
+ I)^{-\frac{1}{2}}\)</span> terms for simplicity, since they only
re-scale embeddings.</p>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230213212117430.png"
alt="image-20230213212117430" />
<figcaption aria-hidden="true">image-20230213212117430</figcaption>
</figure>
<p>The above derivation shows that, inserting self-connection into A and
propagating embeddings on it, is essentially equivalent to a weighted
sum of the embeddings propagated at each LGC layer.</p>
<p>because <span class="math inline">\(AE^{(0)}=E^{(1)}\)</span>...<span
class="math inline">\(A^KE^{(0)}=E^{(K)}\)</span></p>
<h2 id="relation-with-appnp">Relation with APPNP</h2>
<p><strong>Purpose</strong>: shows the underlying equivalence between
LightGCN and APPNP, thus our LightGCN enjoys the sames benefits in
propagating long-range with controllable overs-moothing.</p>
<p>APPNP: a recent GCN variant that addresses over-smoothing. APPNP
complements each propagation layer with the starting features.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213212642134.png"
alt="image-20230213212642134" />
<figcaption aria-hidden="true">image-20230213212642134</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213212845456.png"
alt="image-20230213212845456" />
<figcaption aria-hidden="true">image-20230213212845456</figcaption>
</figure>
<p>also equivalent to a weighted sum of the embeddings propagated at
each LGC layer.</p>
<h2 id="second-order-embedding-smoothness">Second-Order Embedding
Smoothness</h2>
<p><strong>Purpose</strong>: providing more insights into the working
mechanism of LightGCN.</p>
<p>below is influence from2-order neighbor to target node.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213213500081.png"
alt="image-20230213213500081" />
<figcaption aria-hidden="true">image-20230213213500081</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213213521206.png"
alt="image-20230213213521206" />
<figcaption aria-hidden="true">image-20230213213521206</figcaption>
</figure>
<p><strong>conclusion</strong>: the influence of a second-order neighbor
v on u is determined by</p>
<ol type="1">
<li>the number of co-interacted items, the more the larger.</li>
<li>the popularity of the co-interacted items, the less popularity
(i.e., more indicative of user personalized preference) the larger</li>
<li>the activity of v, the less active the larger.</li>
</ol>
<h1 id="model-train">Model Train</h1>
<h2 id="loss-function">Loss function</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213213949666.png"
alt="image-20230213213949666" />
<figcaption aria-hidden="true">image-20230213213949666</figcaption>
</figure>
<h2 id="optimizer-adam">Optimizer: Adam</h2>
<h2 id="no-dropout-strategy">No dropout strategy</h2>
<p>The reason is that we do not have feature transformation weight
matrices in LightGCN, thus enforcing L2 regularization on the embedding
layer is sufficient to prevent overfitting.</p>
<h1 id="experiment">Experiment</h1>
<h2 id="compared-with-ngcf">compared with NGCF</h2>
<ol type="1">
<li>LightGCN performs better than NGCF and NGCF-fn, as NGCF-fn still
contains more useless operations than LightGCN.</li>
<li>Increasing the number of layers can improve performance, but the
benefits diminish. Increasing the layer number from 0 to 1 leads to the
largest performance gain, and using a layer number of 3 leads to
satisfactory performance in most cases.</li>
<li>LightGCN consistently obtains lower training loss, which indicates
that LightGCN fits the training data better than NGCF. Moreover, the
lower training loss successfully transfers to better testing accuracy,
indicating the strong generalization power of LightGCN. In contrast, the
higher training loss and lower testing accuracy of NGCF reflect the
practical difficulty to train such a heavy model it well.</li>
</ol>
<h2 id="ablation-and-effectiveness-analyses">Ablation and Effectiveness
Analyses</h2>
<h3 id="impact-of-layer-combination">Impact of Layer Combination</h3>
<h4 id="using-models">Using models:</h4>
<ol type="1">
<li>LightGCN</li>
<li>LightGCN-single: does not use layer combination</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215151920378.png"
alt="image-20230215151920378" />
<figcaption aria-hidden="true">image-20230215151920378</figcaption>
</figure>
<h4 id="conclusion-1">Conclusion</h4>
<ol type="1">
<li>Focusing on LightGCN-single, we find that its performance first
improves and then drops when the layer number increases from 1 to 4.
This indicates that smoothing a node’s embedding with its first-order
and secondorder neighbors is very useful for CF, but will suffer from
oversmoothing issues when higher-order neighbors are used.</li>
<li>Focusing on LightGCN, we find that its performance gradually
improves with the increasing of layers even using 4 layers. This
justifies the effectiveness of layer combination for addressing
over-smoothing.</li>
<li>we find that LightGCN consistently outperforms LightGCN-single on
Gowalla, but not on AmazonBook and Yelp2018. There are two reason:
<ol type="1">
<li>LightGCN-single is special case of LightGCN that sets αK to 1 and
other αk to 0;</li>
<li>we do not tune the <span class="math inline">\(αk\)</span> and
simply set it as <span class="math inline">\(\frac{1}{K+1}\)</span>
uniformly for LightGCN.</li>
</ol></li>
</ol>
<h3 id="impact-of-symmetric-sqrt-normalization">Impact of Symmetric Sqrt
Normalization</h3>
<h4 id="setting">Setting:</h4>
<ol type="1">
<li>LightGCN-L: normalization only at the left side (i.e., the target
node’s coefficient).</li>
<li>LightGCN-R: the right side (i.e., the neighbor node’s
coefficient).</li>
<li>LightGCN-L1: use L1 normalization( i.e., removing the square
root).</li>
<li>LightGCN-L1-L: use L1 normalization only on the left side.</li>
<li>LightGCN-L1-R: use L1 normalization only on the right side.</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215153734701.png"
alt="image-20230215153734701" />
<figcaption aria-hidden="true">image-20230215153734701</figcaption>
</figure>
<h4 id="conclusion-2">Conclusion</h4>
<ol type="1">
<li>The best setting in general is using sqrt normalization at both
sides (i.e., the current design of LightGCN). Removing either side will
drop the performance largely.</li>
<li>The second best setting is using L1 normalization at the left side
only (i.e., LightGCN-L1-L). This is equivalent to normalize the
adjacency matrix as a stochastic matrix by the
in-degree(norm后矩阵无对称性).</li>
<li>Normalizing symmetrically on two sides is helpful for the sqrt
normalization, but will degrade the performance of L1
normalization.</li>
</ol>
<h3 id="analysis-of-embedding-smoothness">Analysis of Embedding
Smoothness</h3>
<p><strong>Object</strong>: Making sure such
smoothing（有点像聚类的感觉） of embeddings is the key reason of
LightGCN’s effectiveness.</p>
<p><strong>Method</strong>: we first define the smoothness of user
embeddings as(用于衡量2-order
neighbor的embedding差别大小，是否合理聚类的感觉):</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215160123446.png"
alt="image-20230215160123446" />
<figcaption aria-hidden="true">image-20230215160123446</figcaption>
</figure>
<p>where the L2 norm on embeddings is used to eliminate the impact of
the embedding’s scale.</p>
<p><strong>result</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215160328103.png"
alt="image-20230215160328103" />
<figcaption aria-hidden="true">image-20230215160328103</figcaption>
</figure>
<p><strong>Conclusion</strong>: the smoothness loss of LightGCN-single
is much lower than that of MF.</p>
<p>This indicates that by conducting light graph convolution, the
embeddings become smoother and more suitable for recommendation.</p>
<h2 id="hyper-parameter-studies">Hyper-parameter Studies</h2>
<p><strong>object</strong>: Ensure the L2 regularization coefficient
<span class="math inline">\(λ\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215161009450.png"
alt="image-20230215161009450" />
<figcaption aria-hidden="true">image-20230215161009450</figcaption>
</figure>
<p><strong>Conclusion</strong>:</p>
<ol type="1">
<li>LightGCN is relatively insensitive to λ.</li>
<li>Even when λ sets to 0, LightGCN is better than NGCF, which
additionally uses dropout to prevent overfitting. This shows that
LightGCN is less prone to overfitting</li>
<li>When λ is larger than 1e−3, the performance drops quickly, which
indicates that too strong regularization will negatively affect model
normal training and is not encouraged.</li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title>Math in DNN</title>
    <url>/2023/06/24/MATHinDNN/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="相似度">相似度</h1>
<h2 id="余弦相似度">余弦相似度</h2>
<h2 id="皮尔逊相关系数">皮尔逊相关系数</h2>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
  </entry>
  <entry>
    <title>NGCF</title>
    <url>/2023/02/24/NGCF/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>question: example(the Laplacian)</p>
<h2 id="some-definition">Some Definition</h2>
<ol type="1">
<li><p>Recommendation system: Estimate how likely a user will adopt an
item based on the historical interaction like purchase and
click.</p></li>
<li><p>Collaborative filtering(CF): behaviorally similar users would
exhibit similar preference on items.</p>
<p>CF consists of</p>
<ol type="1">
<li><p>embedding: transforms users and items into vectorized
representations. e.g. matrix factorization(MF),deep learning
function...</p></li>
<li><p>interaction modeling: reconstructs historical interactions based
on the embeddings. e.g. inner product, neural function...</p></li>
</ol></li>
<li><p>collaborative signal: signal latent in user-item
interactions</p></li>
</ol>
<h2 id="existing-problem">Existing Problem</h2>
<p>The current embedding process of CF doesn't encode a collaborative
signal. Most of them focus on the descriptive feature(e.g. user id,
attributes). When the embeddings are insufficient in capturing CF, the
methods have to rely on the interaction function to make up for the
deficiency of suboptimal embeddings</p>
<h2 id="main-contribute">Main contribute</h2>
<ol type="1">
<li><p>Highlight the critical importance of explicitly exploiting the
collaborative signal in the embedding function of model-based CF
methods.</p></li>
<li><p>Propose NGCF, a new recommendation framework based on a graph
neural network, which explicitly encodes the collaborative signal in the
form of high-order connectivities by performing embedding
propagation.</p></li>
</ol>
<h1 id="model">Model</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211111222966.png"
alt="image-20230211111222966" />
<figcaption aria-hidden="true">image-20230211111222966</figcaption>
</figure>
<p>There are three components in the framework:</p>
<ol type="1">
<li>Embedding layer: offers and initialization of user embeddings and
item embeddings;</li>
<li>Multiple embedding propagation layers: refine the embeddings by
injecting high-order connectivity relations;</li>
<li>Prediction layer: aggregates the refined embeddings from different
propagation layers and outputs the affinity score of a user-item
pair.</li>
</ol>
<h2 id="embedding-layer">Embedding layer</h2>
<p>Just initializing user embeddings and item embeddings by using ID or
other features.</p>
<p>Get user embedding <span class="math inline">\(e_i\)</span> and item
embedding <span class="math inline">\(e_u\)</span>.</p>
<h2 id="multiple-embedding-propagation-layers">Multiple Embedding
Propagation Layers</h2>
<h3 id="one-layer-propagation">One layer propagation</h3>
<p>It consists of two parts: Message Construction and Message
aggregation.</p>
<h4 id="message-construction">Message Construction</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211112521161.png"
alt="image-20230211112521161" />
<figcaption aria-hidden="true">image-20230211112521161</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211111736136.png"
alt="image-20230211111736136" />
<figcaption aria-hidden="true">image-20230211111736136</figcaption>
</figure>
<p><span class="math inline">\(m_{u&lt;-i}\)</span>: the result of the
message construction module. It is a message embedding that will be used
to update the target node.</p>
<p><span class="math inline">\(e_i\)</span>: Embedding of neighbor
item.</p>
<p><strong>meaning</strong> : encode neighbor item's feature.</p>
<p><span class="math inline">\(e_i⊙e_u\)</span> : element-wise product
of <span class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_u\)</span>.</p>
<p><strong>meaning</strong>: encodes the interaction between <span
class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_u\)</span> into the message and makes the
message dependent on the affinity between <span
class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_j\)</span>.</p>
<p><span class="math inline">\(W_1\)</span>, <span
class="math inline">\(W_2\)</span>: trainable weight matrices， the
shape is (<span class="math inline">\(d&#39;\)</span>, <span
class="math inline">\(d\)</span>), while <span
class="math inline">\(d\)</span> is the size of the initial embedding,
<span class="math inline">\(d&#39;\)</span> is the size of
transformation size.</p>
<p><span class="math inline">\(P_{ui}\)</span>: to control the decay
factor on each propagation on edge (u, i). Here, we set <span
class="math inline">\(P_{ui}\)</span> as <strong>Laplacian norm</strong>
$ $, $ N_u$, $ N_i$ is the first-hot neighbors of user u and item i.
(就是拉普拉斯矩阵归一化！！<span
class="math inline">\(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\)</span>)</p>
<p><strong>meaning</strong> -From the viewpoint of representation
learning: <span class="math inline">\(P_{ui}\)</span> reflects how much
the historical item contributes to the user preference.</p>
<p>From the viewpoint of the message passing: <span
class="math inline">\(P_{ui}\)</span> can be interpreted as a discount
factor, considering the messages being propagated should decay with the
path length.</p>
<h4 id="message-aggregation">Message Aggregation</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211151741633.png"
alt="image-20230211151741633" />
<figcaption aria-hidden="true">image-20230211151741633</figcaption>
</figure>
<p><span class="math inline">\(e_u^{(1)}\)</span>: the representation of
user u after 1 propagation layer.</p>
<p><span class="math inline">\(m_{u&lt;-u}\)</span>: self-connection of
u. Here is <span class="math inline">\(W1e_u\)</span>.</p>
<p><strong>meaning</strong>: retain information of original feature.</p>
<p><span class="math inline">\(m_{u&lt;-i}\)</span>： neighbor node
propagation.</p>
<h3 id="high-order-propagation">High-order propagation</h3>
<h4 id="formulate-form">Formulate Form</h4>
<p>By stacking l-embedding propagation layers, a user (and an item) is
capable of receiving the messages propagated from its l-hop neighbors.
The formulates are similar to one-layer propagation.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212105956664.png"
alt="image-20230212105956664" />
<figcaption aria-hidden="true">image-20230212105956664</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212110019741.png"
alt="image-20230212110019741" />
<figcaption aria-hidden="true">image-20230212110019741</figcaption>
</figure>
<h4 id="matrix-form">Matrix Form</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212110725475.png"
alt="image-20230212110725475" />
<figcaption aria-hidden="true">image-20230212110725475</figcaption>
</figure>
<p><span class="math inline">\(E^{(l)}\)</span> : the representations
for users and items obtained after l-layers propagation. Shape is
(N+M,d)</p>
<p>L: Laplacian matrix for the user-item graph.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212111719667.png"
alt="image-20230212111719667" />
<figcaption aria-hidden="true">image-20230212111719667</figcaption>
</figure>
<p>D is the diagonal degree matrix. where <span
class="math inline">\(D_{tt}=\vert N_t\vert\)</span> meaning the
<code>D[t][t]</code> is the number of neighbors' node. The shape is
(N+M, N+M), because there are totally n+m node(including user and
item)</p>
<p>A is the adjacency matrix. The shape of R is (N, M), while the shape
of A is (N+M, N+M).</p>
<p>some extra knowledge: <a
href="https://zhuanlan.zhihu.com/p/362416124/">理解拉普拉斯矩阵</a></p>
<p>I: identity matrix</p>
<h5 id="a-simple-example-for-matrix-form">A simple example for matrix
form:</h5>
<p>Suppose we have 2 users (A, B), 3 items(C, D, E), N=2 and M=3.</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/D9B00E7DDF74FF18B83E42668335328A.png" alt="D9B00E7DDF74FF18B83E42668335328A" style="zoom: 25%;" /></p>
<p>Let consider this part: <span
class="math inline">\((L+I)E^{(l-1)}W^{(l)}\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/81DBE0096BF060771E3355F2E6A34151.png"
alt="81DBE0096BF060771E3355F2E6A34151" />
<figcaption
aria-hidden="true">81DBE0096BF060771E3355F2E6A34151</figcaption>
</figure>
<p>After calculating <span
class="math inline">\((L+I)E^{(l-1)}\)</span>, we get information on
self-connection and neighbor-propagation (after the Laplacian norm), and
then we can multiply the trainable parameter W1(MLP).</p>
<p>拉普拉斯矩阵归一化的不成熟小理解：</p>
<p>①target node由n个邻居点做贡献，为了避免邻居越多，target
node的value越大的情况，首先除<span
class="math inline">\(\frac{1}{\sqrt{N_n}}\)</span>,
大概也可以理解为邻居越多，每个邻居对其造成的影响越小</p>
<p>②只做一次norm影响对称性，所以为了保持对称性在做一次<span
class="math inline">\(\frac{1}{\sqrt{N_t}}\)</span>,可以理解为neighbor
node有多少邻居对他给到每个邻居的权重有影响，是否能理解为邻居越多说明这个node能提供的信息更普通没价值（例如所有用户购买了水，对推荐系统来说，水能提供的信息就没那么有用）</p>
<p>x class UV_Aggregator(nn.Module):    """   item and user aggregator:
for aggregating embeddings of neighbors (item/user aggreagator).   """​  
 def <strong>init</strong>(self, v2e, r2e, u2e, embed_dim, cuda="cpu",
uv=True):        ...​    def forward(self, nodes, history_uv, history_r):
       # create a container for result, shpe of embed_matrix is
(batchsize,embed_dim)        embed_matrix = torch.empty(len(history_uv),
self.embed_dim, dtype=torch.float).to(self.device)​        # deal with
each single item nodes' neighbors        for i in
range(len(history_uv)):            history = history_uv[i]          
 num_histroy_item = len(history)            tmp_label = history_r[i]​    
       # e_uv : turn neighbors(user node) id to embedding            #
uv_rep : turn single node(item node) to embedding            if self.uv
== True:                # user component                e_uv =
self.v2e.weight[history]                uv_rep =
self.u2e.weight[nodes[i]]            else:                # item
component                e_uv = self.u2e.weight[history]              
 uv_rep = self.v2e.weight[nodes[i]]​            # get rating score
embedding            e_r = self.r2e.weight[tmp_label]            #
concatenated rating and neighbor, and than through two layers mlp to get
fjt            x = torch.cat((e_uv, e_r), 1)            x =
F.relu(self.w_r1(x))​            o_history = F.relu(self.w_r2(x))        
   # calculate neighbor attention and fjt*weight to finish aggregation  
         att_w = self.att(o_history, uv_rep, num_histroy_item)          
 att_history = torch.mm(o_history.t(), att_w)            att_history =
att_history.t()​            embed_matrix[i] = att_history        # result
(batchsize, embed_dim)        to_feats = embed_matrix        return
to_featspython</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/8EE43A6D961CA0F0145CD44C62B9F9BE.png"
alt="8EE43A6D961CA0F0145CD44C62B9F9BE" />
<figcaption
aria-hidden="true">8EE43A6D961CA0F0145CD44C62B9F9BE</figcaption>
</figure>
<p>We get information on the interaction between <span
class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_u\)</span> (after the Laplacian norm), and then
we can multiply the trainable parameter W2(MLP).</p>
<p>Add two parts and through LeakyRelu, we get user or item embedding
after l-layers propagation.</p>
<h2 id="model-prediction">Model Prediction</h2>
<p>Just concatenate all propagation layers' output embedding, and use
inner product to estimate the user's preference towards the target
item.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212173756733.png"
alt="image-20230212173756733" />
<figcaption aria-hidden="true">image-20230212173756733</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212173813003.png"
alt="image-20230212173813003" />
<figcaption aria-hidden="true">image-20230212173813003</figcaption>
</figure>
<h1 id="optimization">Optimization</h1>
<h2 id="loss">Loss</h2>
<p>BPR Loss: assumes that the observed interactions, which are more
reflective of a user’s preferences, should be assigned higher prediction
values than unobserved ones.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212212248890.png"
alt="image-20230212212248890" />
<figcaption aria-hidden="true">image-20230212212248890</figcaption>
</figure>
<h2 id="optimizer-adam">Optimizer: Adam</h2>
<h2 id="model-size">Model Size</h2>
<p>In NGCF, only W1 and W2 in the propagation layer need to be trained,
so has <span class="math inline">\(2Ld_ld_{l-1}\)</span> more
parameters, while L is always smaller than 5 and <span
class="math inline">\(d\)</span> is set as the embedding size(e.g. 64)
which is also small.</p>
<h2 id="message-and-node-dropout">Message and Node Dropout</h2>
<ol type="1">
<li><p><strong>Message dropout</strong>: randomly drops out the outgoing
messages (equal to dropout edge).</p>
<p><strong>meaning</strong>: endows the representations more robustness
against the presence or absence of single connections between users and
items.</p>
<p><strong>example</strong>: For the <span
class="math inline">\(l-th\)</span> propagation layer, we drop out the
messages being propagated, with a probability <span
class="math inline">\(p1\)</span>.</p></li>
<li><p><strong>Node dropout</strong>: randomly blocks a particular node
and discards all its outgoing messages.</p>
<p><strong>meaning</strong>: focuses on reducing the influences of
particular users or items.</p>
<p><strong>example</strong>: For the <span
class="math inline">\(l-th\)</span> propagation layer, we randomly drop
<span class="math inline">\((M + N)p2\)</span> nodes of the Laplacian
matrix, where <span class="math inline">\(p2\)</span> is the dropout
ratio.</p></li>
</ol>
<p>区别：对于message
dropout，计算时node的邻居数、拉普拉斯norm都是正常的，就是更新embedding的时候遗漏了信息，作用是提高一下鲁棒性和容错性；对于Node
dropout，直接在拉普拉斯矩阵中屏蔽若干个node，可能影响临界点数、归一化数值等，在矩阵运算时候就有影响，作用是希望模型不要过于依赖某些特定邻接点，没了部分点依然能正常运行。</p>
<h1 id="experiment">Experiment</h1>
<h2 id="conclusions-from-comparing-with-other-models">Conclusions from
comparing with other models</h2>
<ol type="1">
<li>The inner product is insufficient to capture the complex relations
between users and items.</li>
<li>Nonlinear feature interactions between users and items are
important</li>
<li>Neighbor information can improve embedding learning, and using the
attention mechanism is better than using equal and heuristic
weight.</li>
<li>Considering high-order connectivity or neighbor is better than only
considering first-order neighbor.</li>
<li>that exploiting high-order connectivity greatly facilitates
representation learning for inactive users, as the collaborative signal
can be effectively captured. And the embedding propagation is beneficial
to relatively inactive users.</li>
</ol>
<h2 id="study-for-ngcf">Study for NGCF</h2>
<p>....</p>
<h2 id="effect-of-high-order-connectivity">Effect of High-order
Connectivity</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212225247958.png"
alt="image-20230212225247958" />
<figcaption aria-hidden="true">image-20230212225247958</figcaption>
</figure>
<ol type="1">
<li>the representations of NGCF-3 exhibit discernible clustering,
meaning that the points with the same colors (<em>i.e.,</em> the items
consumed by the same users) tend to form the clusters.</li>
<li>when stacking three embedding propagation layers, the embeddings of
their historical items tend to be closer. It qualitatively verifies that
the proposed embedding propagation layer is capable of injecting the
explicit collaborative signal (via NGCF-3) into the
representations.</li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title>GraphRec</title>
    <url>/2023/02/24/Note_for_GraphRec/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="graphrec">GraphRec</h1>
<h1 id="graphrec-feature">GraphRec feature</h1>
<ol type="1">
<li><p>Can capture both interactions and opinions in user-item
graph.</p></li>
<li><p>Consider different strengths of social relations.</p></li>
<li><p>Use attention mechanism.</p></li>
</ol>
<h1 id="overall-architecture">Overall architecture</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/Snipaste_2023-02-02_15-10-34.png"
alt="Snipaste_2023-02-02_15-10-34" />
<figcaption aria-hidden="true">Snipaste_2023-02-02_15-10-34</figcaption>
</figure>
<h2 id="three-import-module">Three import module:</h2>
<ol type="1">
<li><p>User Modeling: used to compute User Latent Factor(vector
containing many useful information)</p></li>
<li><p>Item Modeling: used to compute Item Latent Factor.</p></li>
<li><p>Rating Prediction: used to predict the item which user would like
to interact with.</p></li>
</ol>
<h1 id="source-code-analyses">Source code analyses</h1>
<h2 id="data">Data</h2>
<h3 id="what-kind-of-datas-we-use"><strong>What kind of datas we
use?</strong></h3>
<ol type="1">
<li><p>User-Item graph: record interation(e.g. purchase) and
opinion(e.g. five star rating) between user and item</p></li>
<li><p>User-User social graph: relationship between user and
user</p></li>
</ol>
<h3 id="how-to-represent-these-datas-in-code"><strong>How to represent
these datas in code?</strong></h3>
<h4 id="user-item-graph">User-Item graph:</h4>
<ol type="1">
<li>history_u_lists, history_ur_lists: user's purchased history (item
set in training set), and his/her rating score (dict)</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history_u_list = &#123;</span><br><span class="line">    user_id1:[item_id1, item_id2, item_id3...],</span><br><span class="line">    user_id2:[item_id4...],</span><br><span class="line">&#125;</span><br><span class="line">history_ur_list = &#123;</span><br><span class="line">    user_id1:[rating_score_u1i1, rating_score_u1i2, rating_score_u1i3...],</span><br><span class="line">    user_id2:[rating_score_u2i4...],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">e.g.</span><br><span class="line">history_u_list = &#123;</span><br><span class="line">    <span class="number">681</span>: [<span class="number">0</span>, <span class="number">156</span>], </span><br><span class="line">    <span class="number">81</span>: [<span class="number">1</span>, <span class="number">41</span>, <span class="number">90</span>]&#125;</span><br><span class="line">history_ur_list = &#123;</span><br><span class="line">    <span class="number">681</span>: [<span class="number">5</span>,<span class="number">4</span>],</span><br><span class="line">    <span class="number">81</span>: [<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>]&#125;</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>history_v_lists, history_vr_lists: user set (in training set) who
have interacted with the item, and rating score (dict). Similar with
history_u_lists, history_ur_lists but key is item id and value is user
id.</li>
</ol>
<h4 id="user-user-socal-graph">User-User socal graph</h4>
<ol type="1">
<li>social_adj_lists: user's connected neighborhoods</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">social_adj_lists = &#123;</span><br><span class="line">    user_id1:[user_id2, user_id3, user_id4...],</span><br><span class="line">    user_id2:[user_id1...],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="other">other</h4>
<ol type="1">
<li>train_u, train_v, train_r: used for model training, one by one based
on index (user, item, rating)</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_u = [user_id1, user_id2,....]</span><br><span class="line">train_v = [item_id34, item_id1,...]</span><br><span class="line">train_r = [rating_socre_u1i34, rating_socre_u2i1]</span><br><span class="line"><span class="built_in">len</span>(train_u) = <span class="built_in">len</span>(train_v) = <span class="built_in">len</span>(train_r)</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li><p>test_u, test_v, test_r: similar with training datas</p></li>
<li><p>ratings_list: rating value from 0.5 to 4.0 (8 opinion embeddings)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;2.0: 0, 1.0: 1, 3.0: 2, 4.0: 3, 2.5: 4, 3.5: 5, 1.5: 6, 0.5: 7&#125;</span><br></pre></td></tr></table></figure></p></li>
</ol>
<h3 id="how-to-pre-process-data"><strong>How to pre-process
data?</strong></h3>
<p>use <code>torch.utils.data.TensorDataset</code> and
<code>torch.utils.data.DataLoader</code> generate
<code>training_dataset</code> and <code>testing_dataset</code> (user,
item, rating)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">support batchsize = <span class="number">5</span></span><br><span class="line">[tensor([<span class="number">409</span>,  <span class="number">88</span>, <span class="number">134</span>, <span class="number">298</span>, <span class="number">340</span>]),                             <span class="comment">#user id</span></span><br><span class="line">tensor([<span class="number">1221</span>,  <span class="number">761</span>,   <span class="number">39</span>,  <span class="number">145</span>,    <span class="number">0</span>]),                         <span class="comment">#item id</span></span><br><span class="line">tensor([<span class="number">1.0000</span>, <span class="number">2.0000</span>, <span class="number">3.5000</span>, <span class="number">0.5000</span>, <span class="number">1.5000</span>, <span class="number">3.5000</span>])        <span class="comment">#rating score</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h2 id="model">Model</h2>
<h3 id="init">Init</h3>
<p>Translate user_id, item_id and rating_id to low-dimension vector,
just random initize, the weight of embedding layers will be trained.</p>
<p>After translate we get</p>
<pre><code>qj-embedding of item vj, 
pi-embedding of user ui, 
er-embedding of rating.</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">u2e = nn.Embedding(num_users, embed_dim).to(device)</span><br><span class="line">v2e = nn.Embedding(num_items, embed_dim).to(device)</span><br><span class="line">r2e = nn.Embedding(num_ratings, embed_dim).to(device)</span><br><span class="line"><span class="built_in">print</span>(u2e, v2e, r2e)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;Output</span></span><br><span class="line"><span class="string">Embedding(705, 64) </span></span><br><span class="line"><span class="string">Embedding(1941, 64) </span></span><br><span class="line"><span class="string">Embedding(8, 64)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>So that, we can easily get embedding through U2e, V2e and r2e.</p>
<h3 id="overall-architecture-1">Overall architecture</h3>
<figure>
<img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/GraphRec.jpg"
alt="GraphRec" />
<figcaption aria-hidden="true">GraphRec</figcaption>
</figure>
<p>GraphRec consist of User Modeling, Item Modeling and Rating
Prediction. The forward code of GraphRec is as follow:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GraphRec</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_u, enc_v_history, r2e</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes_u, nodes_v</span>):</span><br><span class="line">        <span class="comment"># nodes_u : [128] 128(batchsize) user id</span></span><br><span class="line">        <span class="comment"># nodes_v : [128] 128(batchsize) item id</span></span><br><span class="line">        <span class="comment"># self.enc_u is the User Modeling part(including Item Aggregation and Social Aggregation )</span></span><br><span class="line">        <span class="comment"># self.enc_v_history is the Item Modeling part(User Aggregation)</span></span><br><span class="line">        embeds_u = self.enc_u(nodes_u)</span><br><span class="line">        embeds_v = self.enc_v_history(nodes_v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># After aggregation information, forward two layer MLP， and get the Latent vector of user and item</span></span><br><span class="line">        x_u = F.relu(self.bn1(self.w_ur1(embeds_u)))</span><br><span class="line">        x_u = F.dropout(x_u, training=self.training)</span><br><span class="line">        x_u = self.w_ur2(x_u)</span><br><span class="line">        x_v = F.relu(self.bn2(self.w_vr1(embeds_v)))</span><br><span class="line">        x_v = F.dropout(x_v, training=self.training)</span><br><span class="line">        x_v = self.w_vr2(x_v)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># concatenated user vector and item vector, use three layer MLP to predict</span></span><br><span class="line">        x_uv = torch.cat((x_u, x_v), <span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.bn3(self.w_uv1(x_uv)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = F.relu(self.bn4(self.w_uv2(x)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        scores = self.w_uv3(x)</span><br><span class="line">        <span class="keyword">return</span> scores.squeeze()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, nodes_u, nodes_v, labels_list</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>full code of GraphRec class</p>
<details>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GraphRec</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_u, enc_v_history, r2e</span>):</span><br><span class="line">        <span class="built_in">super</span>(GraphRec, self).__init__()</span><br><span class="line">        self.enc_u = enc_u</span><br><span class="line">        self.enc_v_history = enc_v_history</span><br><span class="line">        self.embed_dim = enc_u.embed_dim</span><br><span class="line"></span><br><span class="line">        self.w_ur1 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_ur2 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_vr1 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_vr2 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_uv1 = nn.Linear(self.embed_dim * <span class="number">2</span>, self.embed_dim)</span><br><span class="line">        self.w_uv2 = nn.Linear(self.embed_dim, <span class="number">16</span>)</span><br><span class="line">        self.w_uv3 = nn.Linear(<span class="number">16</span>, <span class="number">1</span>)</span><br><span class="line">        self.r2e = r2e</span><br><span class="line">        self.bn1 = nn.BatchNorm1d(self.embed_dim, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(self.embed_dim, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(self.embed_dim, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.bn4 = nn.BatchNorm1d(<span class="number">16</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes_u, nodes_v</span>):</span><br><span class="line">        embeds_u = self.enc_u(nodes_u)</span><br><span class="line">        embeds_v = self.enc_v_history(nodes_v)</span><br><span class="line"></span><br><span class="line">        x_u = F.relu(self.bn1(self.w_ur1(embeds_u)))</span><br><span class="line">        x_u = F.dropout(x_u, training=self.training)</span><br><span class="line">        x_u = self.w_ur2(x_u)</span><br><span class="line">        x_v = F.relu(self.bn2(self.w_vr1(embeds_v)))</span><br><span class="line">        x_v = F.dropout(x_v, training=self.training)</span><br><span class="line">        x_v = self.w_vr2(x_v)</span><br><span class="line"></span><br><span class="line">        x_uv = torch.cat((x_u, x_v), <span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.bn3(self.w_uv1(x_uv)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = F.relu(self.bn4(self.w_uv2(x)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        scores = self.w_uv3(x)</span><br><span class="line">        <span class="keyword">return</span> scores.squeeze()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, nodes_u, nodes_v, labels_list</span>):</span><br><span class="line">        scores = self.forward(nodes_u, nodes_v)</span><br><span class="line">        <span class="keyword">return</span> self.criterion(scores, labels_list)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</details>
<h3 id="user-modeling">User Modeling</h3>
<p>It contain Item Aggregation and Social Aggregation</p>
<p>在这里本质上是先做了一层Item
Aggregation之后，用得到的结果再做一层Social Aggregation 所以这里的Item
Aggregation，本质上是Social Aggregation中的self-connection</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Social_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, social_adj_lists, aggregator, base_model=<span class="literal">None</span>, cuda=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># to_neighs is a list which element is list recording social neighbor node, and len(list) is batchsize,</span></span><br><span class="line">        to_neighs = []</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            to_neighs.append(self.social_adj_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Social aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, to_neighs)  <span class="comment"># user-user network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Item aggregation</span></span><br><span class="line">        self_feats = self.features(torch.LongTensor(nodes.cpu().numpy())).to(self.device)</span><br><span class="line">        self_feats = self_feats.t()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        <span class="comment"># Concatenate Item Aggregation and Social Aggregation, and through one layer MLP</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<p>full code of User Modeling</p>
<details>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Social_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, social_adj_lists, aggregator, base_model=<span class="literal">None</span>, cuda=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Social_Encoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.features = features</span><br><span class="line">        self.social_adj_lists = social_adj_lists</span><br><span class="line">        self.aggregator = aggregator</span><br><span class="line">        <span class="keyword">if</span> base_model != <span class="literal">None</span>:</span><br><span class="line">            self.base_model = base_model</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.device = cuda</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">2</span> * self.embed_dim, self.embed_dim)  <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># to_neighs is a list which element is list recording social neighbor node, and len(list) is batchsize,</span></span><br><span class="line">        to_neighs = []</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            to_neighs.append(self.social_adj_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Item aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, to_neighs)  <span class="comment"># user-user network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Social aggregation</span></span><br><span class="line">        self_feats = self.features(torch.LongTensor(nodes.cpu().numpy())).to(self.device)</span><br><span class="line">        self_feats = self_feats.t()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        <span class="comment"># Concatenate Item Aggregation and Social Aggregation, and through one layer MLP</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
</details>
<h4 id="item-aggregation">Item Aggregation</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, history_uv_lists, history_r_lists, aggregator, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        tmp_history_uv = []</span><br><span class="line">        tmp_history_r = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">#get nodes(batch) neighbors</span></span><br><span class="line">        <span class="comment">#tmp_history_uv is a list which len is 128,while it&#x27;s element is also a list meaning that the each node&#x27;s(in batch) neighbor item id list</span></span><br><span class="line">        <span class="comment">#tmp_history_r is similar with tmp_history_uv, but record the rating score instead of item id</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            tmp_history_uv.append(self.history_uv_lists[<span class="built_in">int</span>(node)])</span><br><span class="line">            tmp_history_r.append(self.history_r_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># after neigh aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  <span class="comment"># user-item network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># id to embedding (features : u2e)</span></span><br><span class="line">        self_feats = self.features.weight[nodes]</span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<p>And the <code>self.aggregator</code> in neigh aggregation is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Aggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    item and user aggregator: for aggregating embeddings of neighbors (item/user aggreagator).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, v2e, r2e, u2e, embed_dim, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, history_uv, history_r</span>):</span><br><span class="line">        <span class="comment"># create a container for result, shpe of embed_matrix is (batchsize,embed_dim)</span></span><br><span class="line">        embed_matrix = torch.empty(<span class="built_in">len</span>(history_uv), self.embed_dim, dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># deal with each single nodes&#x27; neighbors</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(history_uv)):</span><br><span class="line">            history = history_uv[i]</span><br><span class="line">            num_histroy_item = <span class="built_in">len</span>(history)</span><br><span class="line">            tmp_label = history_r[i]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># e_uv : turn neighbors id to embedding</span></span><br><span class="line">            <span class="comment"># uv_rep : turn single node to embedding</span></span><br><span class="line">            <span class="keyword">if</span> self.uv == <span class="literal">True</span>:</span><br><span class="line">                <span class="comment"># user component</span></span><br><span class="line">                e_uv = self.v2e.weight[history]</span><br><span class="line">                uv_rep = self.u2e.weight[nodes[i]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># item component</span></span><br><span class="line">                e_uv = self.u2e.weight[history]</span><br><span class="line">                uv_rep = self.v2e.weight[nodes[i]]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># get rating score embedding</span></span><br><span class="line">            e_r = self.r2e.weight[tmp_label]</span><br><span class="line">            <span class="comment"># concatenated rating and neighbor, and than through two layers mlp to get xia</span></span><br><span class="line">            x = torch.cat((e_uv, e_r), <span class="number">1</span>)</span><br><span class="line">            x = F.relu(self.w_r1(x))</span><br><span class="line"></span><br><span class="line">            o_history = F.relu(self.w_r2(x))</span><br><span class="line">            <span class="comment"># calculate neighbor attention and xia*weight to finish aggregation</span></span><br><span class="line">            att_w = self.att(o_history, uv_rep, num_histroy_item)</span><br><span class="line">            att_history = torch.mm(o_history.t(), att_w)</span><br><span class="line">            att_history = att_history.t()</span><br><span class="line"></span><br><span class="line">            embed_matrix[i] = att_history</span><br><span class="line">        <span class="comment"># result (batchsize, embed_dim)</span></span><br><span class="line">        to_feats = embed_matrix</span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
<p>While <code>self.att</code> is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dims</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, node1, u_rep, num_neighs</span>):</span><br><span class="line">        <span class="comment"># pi</span></span><br><span class="line">        uv_reps = u_rep.repeat(num_neighs, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># concatenated neighbot and pi</span></span><br><span class="line">        x = torch.cat((node1, uv_reps), <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># through 3 layers MLP</span></span><br><span class="line">        x = F.relu(self.att1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = F.relu(self.att2(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.att3(x)</span><br><span class="line">        <span class="comment"># get weights</span></span><br><span class="line">        att = F.softmax(x, dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> att</span><br></pre></td></tr></table></figure>
<h4 id="social-aggregation">Social Aggregation</h4>
<p>use the result of Item Aggregation and pi as input</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Social_Aggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Social Aggregator: for aggregating embeddings of social neighbors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, u2e, embed_dim, cuda=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, to_neighs</span>):</span><br><span class="line">        <span class="comment">#return a uninitialize matrix as result container, which shape is (batchsize, embed_dim)</span></span><br><span class="line">        embed_matrix = torch.empty(<span class="built_in">len</span>(nodes), self.embed_dim, dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nodes)):</span><br><span class="line">            <span class="comment"># get social graph neighbor</span></span><br><span class="line">            tmp_adj = to_neighs[i]</span><br><span class="line">            num_neighs = <span class="built_in">len</span>(tmp_adj)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># fase : can use user embedding instead of result of item aggregation to improve speed</span></span><br><span class="line">            <span class="comment"># e_u = self.u2e.weight[list(tmp_adj)] # fast: user embedding </span></span><br><span class="line">            <span class="comment"># slow: item-space user latent factor (item aggregation)</span></span><br><span class="line">            feature_neigbhors = self.features(torch.LongTensor(<span class="built_in">list</span>(tmp_adj)).to(self.device))</span><br><span class="line">            e_u = torch.t(feature_neigbhors)</span><br><span class="line"></span><br><span class="line">            u_rep = self.u2e.weight[nodes[i]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># concatenated node embedding and neigbor vector (result of item aggregation) </span></span><br><span class="line">            <span class="comment"># and than through MLPs and Softmax to calculate weights</span></span><br><span class="line">            att_w = self.att(e_u, u_rep, num_neighs)</span><br><span class="line">            <span class="comment"># weight*neighbor vector</span></span><br><span class="line">            att_history = torch.mm(e_u.t(), att_w).t()</span><br><span class="line">            embed_matrix[i] = att_history</span><br><span class="line">        to_feats = embed_matrix</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
<h3 id="item-modeling">Item Modeling</h3>
<p>Similar with the Item Aggregation of User Modeling</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, history_uv_lists, history_r_lists, aggregator, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        tmp_history_uv = []</span><br><span class="line">        tmp_history_r = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">#get nodes(batch) neighbors of item</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            tmp_history_uv.append(self.history_uv_lists[<span class="built_in">int</span>(node)])</span><br><span class="line">            tmp_history_r.append(self.history_r_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># after neigh aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  <span class="comment"># user-item network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># id to embedding (features : v2e)</span></span><br><span class="line">        self_feats = self.features.weight[nodes]</span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<p>And the <code>self.aggregator</code> in neigh aggregation is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Aggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    item and user aggregator: for aggregating embeddings of neighbors (item/user aggreagator).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, v2e, r2e, u2e, embed_dim, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, history_uv, history_r</span>):</span><br><span class="line">        <span class="comment"># create a container for result, shpe of embed_matrix is (batchsize,embed_dim)</span></span><br><span class="line">        embed_matrix = torch.empty(<span class="built_in">len</span>(history_uv), self.embed_dim, dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># deal with each single item nodes&#x27; neighbors</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(history_uv)):</span><br><span class="line">            history = history_uv[i]</span><br><span class="line">            num_histroy_item = <span class="built_in">len</span>(history)</span><br><span class="line">            tmp_label = history_r[i]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># e_uv : turn neighbors(user node) id to embedding</span></span><br><span class="line">            <span class="comment"># uv_rep : turn single node(item node) to embedding</span></span><br><span class="line">            <span class="keyword">if</span> self.uv == <span class="literal">True</span>:</span><br><span class="line">                <span class="comment"># user component</span></span><br><span class="line">                e_uv = self.v2e.weight[history]</span><br><span class="line">                uv_rep = self.u2e.weight[nodes[i]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># item component</span></span><br><span class="line">                e_uv = self.u2e.weight[history]</span><br><span class="line">                uv_rep = self.v2e.weight[nodes[i]]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># get rating score embedding</span></span><br><span class="line">            e_r = self.r2e.weight[tmp_label]</span><br><span class="line">            <span class="comment"># concatenated rating and neighbor, and than through two layers mlp to get fjt</span></span><br><span class="line">            x = torch.cat((e_uv, e_r), <span class="number">1</span>)</span><br><span class="line">            x = F.relu(self.w_r1(x))</span><br><span class="line"></span><br><span class="line">            o_history = F.relu(self.w_r2(x))</span><br><span class="line">            <span class="comment"># calculate neighbor attention and fjt*weight to finish aggregation</span></span><br><span class="line">            att_w = self.att(o_history, uv_rep, num_histroy_item)</span><br><span class="line">            att_history = torch.mm(o_history.t(), att_w)</span><br><span class="line">            att_history = att_history.t()</span><br><span class="line"></span><br><span class="line">            embed_matrix[i] = att_history</span><br><span class="line">        <span class="comment"># result (batchsize, embed_dim)</span></span><br><span class="line">        to_feats = embed_matrix</span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>SocialRec</category>
      </categories>
  </entry>
  <entry>
    <title>tutorial of PyG</title>
    <url>/2023/05/23/PyG/</url>
    <content><![CDATA[<p>Some simple knowledge of PyG</p>
<span id="more"></span>
<h1 id="basic">Basic</h1>
<h2 id="data">Data</h2>
<p>A single graph in PyG is described by an instance of
<code>torch_geometric.data.Data</code>, which holds the following
attributes by default:</p>
<ol type="1">
<li><p><code>data.x</code>: Node feature matrix
<code>[num_nodes,num_node_features_dim]</code></p></li>
<li><p><code>data.edge_index</code>: Graph connectivity in <a
href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs">COO
format</a> with shape <code>[2, num_edges]</code> and type
<code>torch.long</code></p></li>
<li><p><code>data.edge_attr</code>: Edge feature matrix with shape
<code>[num_edges, num_edge_features_dim]</code></p></li>
<li><p><code>data.y</code>: Target to train(label). <em>e.g.</em>,
node-level targets of shape <code>[num_nodes, *]</code> or graph-level
targets of shape <code>[1, *]</code></p>
<p>...</p></li>
</ol>
<p>example:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230523172031612.png"
alt="image-20230523172031612" />
<figcaption aria-hidden="true">image-20230523172031612</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> Data</span><br><span class="line"></span><br><span class="line">edge_index = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                           [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                           [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                           [<span class="number">2</span>, <span class="number">1</span>]], dtype=torch.long)</span><br><span class="line">x = torch.tensor([[-<span class="number">1</span>], [<span class="number">0</span>], [<span class="number">1</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##t()会让原变量和新变量直接有依赖（类似浅拷贝），contiguous()断开依赖</span></span><br><span class="line">data = Data(x=x, edge_index=edge_index.t().contiguous())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Data(edge_index=[<span class="number">2</span>, <span class="number">4</span>], x=[<span class="number">3</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p><strong>Note</strong>： Although the graph has only two edges, we
need to define four index tuples to account for both directions of an
edge.</p>
<p>operation</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(data.keys)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;edge_index&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;x&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor([[-<span class="number">1.0</span>],</span><br><span class="line">            [<span class="number">0.0</span>],</span><br><span class="line">            [<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key, item <span class="keyword">in</span> data:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;key&#125;</span> found in data&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x found <span class="keyword">in</span> data</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>edge_index found <span class="keyword">in</span> data</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;edge_attr&#x27;</span> <span class="keyword">in</span> data</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="literal">False</span></span><br><span class="line"></span><br><span class="line">data.num_nodes</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">3</span></span><br><span class="line"></span><br><span class="line">data.num_edges</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">4</span></span><br><span class="line"></span><br><span class="line">data.num_node_features</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1</span></span><br><span class="line"></span><br><span class="line">data.has_isolated_nodes()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="literal">False</span></span><br><span class="line"></span><br><span class="line">data.has_self_loops()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="literal">False</span></span><br><span class="line"></span><br><span class="line">data.is_directed()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Transfer data object to GPU.</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">data = data.to(device)</span><br></pre></td></tr></table></figure>
<h2 id="minibatch">Minibatch</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> TUDataset</span><br><span class="line"><span class="keyword">from</span> torch_geometric.loader <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = TUDataset(root=<span class="string">&#x27;/tmp/ENZYMES&#x27;</span>, name=<span class="string">&#x27;ENZYMES&#x27;</span>, use_node_attr=<span class="literal">True</span>)</span><br><span class="line">loader = DataLoader(dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> loader:</span><br><span class="line">    batch</span><br><span class="line">    &gt;&gt;&gt; DataBatch(batch=[<span class="number">1082</span>], edge_index=[<span class="number">2</span>, <span class="number">4066</span>], x=[<span class="number">1082</span>, <span class="number">21</span>], y=[<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line">    batch.num_graphs</span><br><span class="line">    &gt;&gt;&gt; <span class="number">32</span></span><br></pre></td></tr></table></figure>
<h1 id="message-passing-network">Message Passing Network</h1>
<ul>
<li><a
href="https://blog.csdn.net/weixin_39925939/article/details/121360884">(149条消息)
pytorch geometric教程一: 消息传递源码详解（MESSAGE
PASSING）+实例_每天都想躺平的大喵的博客-CSDN博客</a></li>
</ul>
]]></content>
      <categories>
        <category>GNN</category>
        <category>Frame</category>
      </categories>
  </entry>
  <entry>
    <title>王树森推荐系统公开课</title>
    <url>/2023/03/13/Recommendation-WangShusen/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="基本概念">基本概念</h1>
<h2 id="指标">指标</h2>
<h3 id="消费指标">消费指标</h3>
<p>点击率=点击次数/曝光次数</p>
<p>点赞量=点赞次数/点击次数</p>
<p>收藏率=收藏次数/点击次数</p>
<p>转发率=转发次数/点击次数</p>
<p>阅读完成率=滑动到底次数/点击次数<span class="math inline">\(\times
f(笔记长度)\)</span></p>
<h3 id="北极星指标">北极星指标</h3>
<p>用户规模：日活用户数（DAU），月活用户数（MAU）</p>
<p>消费：人均使用推荐时长、人均阅读笔记数量</p>
<p>发布： 发布渗透率、人均发布量</p>
<h2 id="推荐系统链路">推荐系统链路</h2>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230313220835272.png"
alt="image-20230313220835272" />
<figcaption aria-hidden="true">image-20230313220835272</figcaption>
</figure>
<ol type="1">
<li>召回：快速从海量数据中取回几千个用户可能感兴趣的物品。</li>
<li>粗排：用小规模的模型的神经网络给召回的物品打分，然后做截断，选出分数最高的几百个物品。</li>
<li>精排：
用大规模神经网络给粗排选中的几百个物品打分，可以做截断，也可以不做截断。</li>
<li>重排：
对精排结果做多样性抽样，得到几十个物品，然后用规则调整物品的排序。</li>
</ol>
<h2 id="实验流程">实验流程</h2>
<h3
id="概要03推荐系统的ab测试没仔细看以后补">概要03推荐系统的AB测试：没仔细看以后补</h3>
<h1 id="召回">召回</h1>
<h2 id="协同过滤">协同过滤</h2>
<h3 id="基于物品的协同过滤-itemcf">基于物品的协同过滤 ItemCF</h3>
<p>基本思想：如果用户喜欢item1,而item1与item2相似，那么用户很可能喜欢item2.</p>
<h4 id="基本结构">基本结构：</h4>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230313223949470.png"
alt="image-20230313223949470" />
<figcaption aria-hidden="true">image-20230313223949470</figcaption>
</figure>
<p>我们从用户历史互动知道用户对<span
class="math inline">\(item_j\)</span>，感兴趣利用下面公式计算对候选物品的兴趣分数
<span class="math display">\[
\sum_jlike(user,item_j)\times sim(item_j,item)
\]</span> 在这个例子中，用户对候选item的兴趣是：<span
class="math inline">\(2\times
0.1+1\times0.4+4\times0.2+3\times0.6=3.2\)</span>,我们计算所有item的分数，然后返回分数最高的若干个item</p>
<h5 id="计算item相似度">计算item相似度</h5>
<p>可以通过与item交互过的用户重合度计算item相似度（其中一种方法，也可以用KG）</p>
<ol type="1">
<li>方法1：不考虑用户对物品的喜欢程度</li>
</ol>
<p><span class="math display">\[
sim(i_1,i_2) = \frac{|W1 \cap W2|}{\sqrt[2]{|W1|\cdot |W2|}}
\]</span></p>
<p>其中，喜欢物品<span class="math inline">\(i_1\)</span>的用户记作<span
class="math inline">\(W_1\)</span>,喜欢物品<span
class="math inline">\(i_2\)</span>的用户记作<span
class="math inline">\(W_2\)</span>.</p>
<ol start="2" type="1">
<li><p>方法2： 考虑用户对物品的喜欢程度,使用余弦相似度！</p>
<p>把每个item用向量表示 <span class="math display">\[
i_1=[like(u_1,i_1),like(u_2,i_1),\cdots ,like(u_n,i_1)] \space u_n\in W
\]</span></p>
<p><span class="math display">\[
i_2=[like(u_1,i_2),like(u_2,i_2),\cdots ,like(u_n,i_2)] \space u_n\in W
\]</span></p>
<p><span class="math display">\[
W=W_1\cup W_2
\]</span></p>
<p>我们使用余弦相似度计算： <span class="math display">\[
similarity=cos(\theta) = \frac{A\cdot B}{||A||\space ||B||}
\]</span> 如果有用户k只喜欢其中一个物品:只喜欢<span
class="math inline">\(i_1\)</span>不喜欢<span
class="math inline">\(i_2\)</span>,那么<span
class="math inline">\(i_2[k]=0\)</span>，所以点乘后第k项为0，所以点乘只与同时喜欢<span
class="math inline">\(i_1,i_2\)</span>的用户有关系，如下面公式 <span
class="math display">\[
sim(i_1,i_2) = \frac{\sum_{v\in V}like(v,i_i)\cdot
like(v,i_2)}{\sqrt[2]{\sum_{u_1\in
W_1}like^2(u_1,i_1)}\sqrt[2]{\sum_{u_2\in W_2}like^2(u_2,i_2)}}
\]</span></p></li>
<li></li>
<li><p>皮尔逊系数 <span class="math display">\[
sim(i,j)=\frac{\sum_{p\in P}(R_{i,p}-\bar R_i)(R_{j,p}-\bar
R_j)}{\sqrt{\sum_{p\in P}(R_{i,p}-\bar R_i)^2}\sqrt{\sum_{p\in
P}(R_{j,p}-\bar R_j)^2}}
\]</span></p></li>
</ol>
<h4 id="运作基本流程">运作基本流程</h4>
<ol type="1">
<li><p>实现做离线计算，预先计算两个索引：</p>
<ol type="1">
<li><p>“user2item”：记录每个用户最近点击交互过的n个物品ID（lastN）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># example 不一定是公司真实的保存方式</span></span><br><span class="line">user2item=&#123;</span><br><span class="line">    <span class="string">&#x27;u1&#x27;</span>:[[i1,like(u1,i1)],[i2,like(u1,i2)],...,[<span class="keyword">in</span>,like(u1,<span class="keyword">in</span>)]]</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>"item2item":计算物品之间两两相似度，记录每个物品最相似的k个物品。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">item2item=&#123;</span><br><span class="line">	#target item:[[similar item, similarity score]...]</span><br><span class="line">	&#x27;i1&#x27;:[[i2,0.9],[i6,0.88]...]</span><br><span class="line">	&#x27;i2&#x27;:...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol></li>
<li><p>线上做召回</p>
<ol type="1">
<li>给定用户ID，通过“user2item”找到用户近期感兴趣的物品列表(last-n)</li>
<li>对于last-n列表中每个物品，通过“item2item"找到top-k相似物品。现在有1个user，n个互动物品，nxk个候选物品。</li>
<li>计算候选物品兴趣分数</li>
<li>返回分数最高的100个物品作为推荐结果</li>
</ol></li>
</ol>
<h3 id="swing召回通道">Swing召回通道</h3>
<p>如果两个Item的重合用户来源于一个小圈子（微信群），一个小圈子用户同时与两个Item交互，不能说明两个Item相似，如果很多不相关的用户交互两个Item，说明Item相似。</p>
<h4 id="基本结构-1">基本结构</h4>
<ol type="1">
<li>计算用户重合度</li>
</ol>
<p>用户<span class="math display">\[u_1\]</span>喜欢的物品记作集合<span
class="math display">\[J_1\]</span></p>
<p>用户<span class="math display">\[u_2\]</span>喜欢的物品记作集合<span
class="math display">\[J_2\]</span></p>
<p>定义两个用户的重合度： <span class="math display">\[
overlap(u_1,u_2)=|J_1\cap J_2|
\]</span> 用户<span class="math display">\[u_1\]</span>和<span
class="math display">\[u_2\]</span>的重合度高，则他们可能来自一个小圈子，要降低他们的权重。</p>
<ol start="2" type="1">
<li>计算物品相似度</li>
</ol>
<p>喜欢物品<span class="math display">\[i_1\]</span>的用户记作集合<span
class="math display">\[W_1\]</span></p>
<p>喜欢物品<span class="math display">\[i_2\]</span>的用户记作集合<span
class="math display">\[W_2\]</span> <span class="math display">\[
V=W_1\cap W_2
\]</span></p>
<p><span class="math display">\[
sim(i_1,i_2) = \sum_{u_1\in V}\sum_{u_2\in
V}\frac{1}{\alpha+overlap(u_1,u_2)}
\]</span></p>
<p>u1u2都对物品i1i2感兴趣，这样的用户越多，说明物品越相似</p>
<p><span class="math display">\[\alpha\]</span>是超参数</p>
<h3 id="基于用户的协同过滤usercf">基于用户的协同过滤（UserCF）</h3>
<p>假设：u1与u2兴趣十分相似，u1可能会对u2交互的item感兴趣</p>
<p>何为兴趣相似：</p>
<ol type="1">
<li>点击、点赞、收藏、转发的笔记有很大重合</li>
<li>关注的作者有很大的重合</li>
</ol>
<h4 id="基本结构-2">基本结构</h4>
<p><img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230525170605162.png"
alt="image-20230525170605162" /> <span class="math display">\[
\sum_jsim(user,user_j)\times like(user_j,item)
\]</span></p>
<h5 id="计算user相似度">计算User相似度</h5>
<ol type="1">
<li><p>计算User相似度</p>
<p>把每个用户表示为一个稀疏向量，向量每个元素对应一个物品。相似度sim就是两个向量夹角的余弦。<span
class="math display">\[u_1\cdot u_2\]</span>结果就是<span
class="math display">\[|I|\]</span></p></li>
</ol>
<p><span class="math display">\[
sim(u_1,u_2) = \frac{|I|}{\sqrt{|J_1|\cdot|J_2|}}
\]</span></p>
<p><span class="math display">\[J_1\]</span>: 用户<span
class="math display">\[u_1\]</span>喜欢的物品集合</p>
<p><span class="math display">\[J_2\]</span>: 用户<span
class="math display">\[u_2\]</span>喜欢的物品集合</p>
<p><span class="math display">\[I\]</span>：<span
class="math display">\[J_1\cap J_2\]</span></p>
<p>|*|:集合的大小</p>
<p><span
class="math display">\[sim(u_1,u_2)\in[0,1]\]</span>,越大代表用户越相似</p>
<ol start="2" type="1">
<li>降低热门物品权重</li>
</ol>
<p>大家都喜欢哈利波特，哈利波特对用户相似度计算意义小,所以我们降低热门物品权重
<span class="math display">\[
sim(u_1,u_2) = \frac{\sum_{l\in I}weight(l)}{\sqrt{|J_1|\cdot|J_2|}}
\]</span></p>
<p><span class="math display">\[
weight(l) = \frac{1}{log(1+n_l)}
\]</span></p>
<p><span class="math display">\[n_l\]</span>:
喜欢物品l的用户数量，反应物品的热门程度。<span
class="math display">\[n_l\]</span>越大，<span
class="math display">\[log(1+n_l)\]</span>越大，权重越小</p>
<h4 id="运作基本流程-1">运作基本流程</h4>
<ol type="1">
<li><p>实现做离线计算，预先计算两个索引：</p>
<ol type="1">
<li><p>“user2item”：记录每个用户最近点击交互过的n个物品ID（lastN）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># example 不一定是公司真实的保存方式</span></span><br><span class="line">user2item=&#123;</span><br><span class="line">    <span class="string">&#x27;u1&#x27;</span>:[[i1,like(u1,i1)],[i2,like(u1,i2)],...,[<span class="keyword">in</span>,like(u1,<span class="keyword">in</span>)]]</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>"user2user":计算用户之间两两相似度，记录每个用户最相似的k个用户。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">user2user=&#123;</span><br><span class="line">	#target user:[[similar user, similarity score]...]</span><br><span class="line">	&#x27;u1&#x27;:[[u2,0.9],[u6,0.88]...]</span><br><span class="line">	&#x27;u2&#x27;:...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol></li>
<li><p>线上做召回</p>
<ol type="1">
<li>给定用户ID，通过“user2user”找到top-k相似用户</li>
<li>对于top-k列表中每个用户，通过“user2item"找到用户近期感兴趣物品列表(last-n)。</li>
<li>对于召回的nk个相似物品，用公式预估用户对每个物品的兴趣分数</li>
<li>返回分数最高的100个物品，作为召回结果</li>
</ol></li>
</ol>
<h3 id="协同过滤缺点">协同过滤缺点</h3>
<p>。。。</p>
<h2 id="向量召回">向量召回</h2>
<h3 id="矩阵补充-matrix-completion">矩阵补充 Matrix Completion</h3>
<p>用于填充评分矩阵中无评分的部分，通过求user与item embedding的内积</p>
<p><img src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230626164840701.png" alt="image-20230626164840701" style="zoom:33%;" /></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626163645000.png"
alt="image-20230626163645000" />
<figcaption aria-hidden="true">image-20230626163645000</figcaption>
</figure>
<h4 id="数据集">数据集</h4>
<ol type="1">
<li><p>（用户ID,物品ID，兴趣分数）————》<span
class="math inline">\(dataset={(u,i,y)}\)</span></p></li>
<li><p>正负例子（0-4分）：</p>
<ol type="1">
<li>负例子：曝光没有点击-0分</li>
<li>正例子：点击、点赞、收藏、转发-各1分</li>
</ol></li>
</ol>
<h4 id="训练">训练</h4>
<p><span class="math display">\[
min_{A,B}\sum _{(u,i,y)\in dataset}(y-&lt;a_u,b_i&gt;)^2
\]</span></p>
<h4 id="缺点">缺点</h4>
<ol type="1">
<li>仅用ID embedding，没利用物品、用户的属性。</li>
<li>负样本选取方法不对。</li>
<li>做训练方法不好
<ol type="1">
<li>内积效果不如余弦相似度</li>
<li>用回归方法不如用分类方法。</li>
</ol></li>
</ol>
<h4 id="运作基本流程-2">运作基本流程</h4>
<ol type="1">
<li>离线计算
<ol type="1">
<li>训练矩阵A、B（embedding层的参数，A for user, B for item）</li>
<li>由于矩阵很大，为了快速读取使用hash方法：
<ol type="1">
<li>把矩阵A存储到key-value表{user_id: user_embedding}。</li>
<li>（加速最近邻查找）将item分区保存至key-value表</li>
</ol></li>
</ol></li>
<li>线上服务
<ol type="1">
<li>通过用户ID查询用户向量，记作A。</li>
<li>最近邻查找：查找用户最优可能感兴趣的k个物品作为召回结果。
<ol type="1">
<li>第i号物品的embedding向量记作<span
class="math inline">\(b_i\)</span></li>
<li>求<span class="math inline">\(&lt;a,b_i&gt;\)</span></li>
<li>返回内积最大的k个物品</li>
</ol></li>
</ol></li>
</ol>
<p><strong>加速最近邻查找方法</strong>：</p>
<p>一般item有几亿个，暴力计算内积并排序过慢</p>
<p>方法：</p>
<ol type="1">
<li><p>确定衡量最近邻标注：欧氏距离最小（L2距离），向量内积最大（内积相似度），向量夹角余弦最大（cosine相似度）</p></li>
<li><p>根据衡量标准将所有item
embedding分块，下面为根据余弦相似度分块的例子，每一个区域用一个向量E表示，通过key-value表保存区域向量E与区域中所有向量的embedding。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626171141459.png"
alt="image-20230626171141459" />
<figcaption aria-hidden="true">image-20230626171141459</figcaption>
</figure></li>
<li><p>求区域向量与user的余弦相似度，获取结构最大区域。再将区域中所有的item暴力枚举算相似度。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230626171541263.png"
alt="image-20230626171541263" />
<figcaption aria-hidden="true">image-20230626171541263</figcaption>
</figure></li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>BasicTurtorial</category>
      </categories>
  </entry>
  <entry>
    <title>Relation-enhance Rec</title>
    <url>/2023/06/17/Relation-enhance_Rec/</url>
    <content><![CDATA[<p>relation-enhance KG</p>
<span id="more"></span>
<h1 id="re-kgr">RE-KGR</h1>
<p>Paper: RE-KGR: Relation-Enhanced Knowledge Graph Reasoning for
Recommendation</p>
<p>总结：</p>
<p>把relation当做向量空间，同时考虑relation的方向性，最后基于路径概率预测</p>
<h2 id="methodology">Methodology</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617151926949.png"
alt="image-20230617151926949" />
<figcaption aria-hidden="true">image-20230617151926949</figcaption>
</figure>
<p>given a CKG</p>
<h3 id="embedding-layer">Embedding Layer</h3>
<p>for every entity and relation, one-hot to dense vector</p>
<h3 id="rgc-layer">RGC Layer</h3>
<p><strong>First-order Aggregation</strong>:</p>
<p>project each entity t to a different semantic space conditioned to
the relation r:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153022663.png"
alt="image-20230617153022663" />
<figcaption aria-hidden="true">image-20230617153022663</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153033948.png"
alt="image-20230617153033948" />
<figcaption aria-hidden="true">image-20230617153033948</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153045142.png"
alt="image-20230617153045142" />
<figcaption aria-hidden="true">image-20230617153045142</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153143790.png"
alt="image-20230617153143790" />
<figcaption aria-hidden="true">image-20230617153143790</figcaption>
</figure>
<p>Here, <span class="math inline">\(M_{r−1}\)</span>, <span
class="math inline">\(M_r\)</span> are mapping matrices, and r and r−1
are a pair of inverse relations,such as AuthorOf and WrittenBy.</p>
<p><strong>High-order Aggregation</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153235776.png"
alt="image-20230617153235776" />
<figcaption aria-hidden="true">image-20230617153235776</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153301643.png"
alt="image-20230617153301643" />
<figcaption aria-hidden="true">image-20230617153301643</figcaption>
</figure>
<p>Here, is the concatenation operator, and e(0) denotes initial
embeddings.(dense connectivity)</p>
<h3 id="local-similarity-layer">Local Similarity Layer</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153403760.png"
alt="image-20230617153403760" />
<figcaption aria-hidden="true">image-20230617153403760</figcaption>
</figure>
<h3 id="prediction-layer">Prediction Layer</h3>
<p>Predict Based on path:</p>
<p>use <span class="math inline">\(P_{UIIP}={(h,r,t)|(h,r,t)\in
G}\)</span> to describe an acyclic UIIP, the probability of the UIIP
is:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153641218.png"
alt="image-20230617153641218" />
<figcaption aria-hidden="true">image-20230617153641218</figcaption>
</figure>
<p>we use Pui to denote all acyclic UIIPs that start and end with user u
and item i.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617153723353.png"
alt="image-20230617153723353" />
<figcaption aria-hidden="true">image-20230617153723353</figcaption>
</figure>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>RippleNet</title>
    <url>/2023/03/02/RippleNet/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p><strong>CF</strong>: sparsity, cold start</p>
<p><strong>KG-benefit</strong>:</p>
<ol type="1">
<li>KG introduces semantic relatedness among items, which can help find
their latent connections and improve the <em>precision</em> of
recommended items;</li>
<li>KG consists of relations with various types, which is helpful for
extending a user’s interests reasonably and increasing the
<em>diversity</em> of recommended items;</li>
<li>KG connects a user’s historical records and the recommended ones,
thereby bringing <em>explainability</em> to recommender systems.</li>
</ol>
<p><strong>Existing KG model</strong>:</p>
<ol type="1">
<li><strong>embedding-based method</strong>: DKN, CKE, SHINE, but more
suitable for in-graph applications</li>
<li><strong>path-based method</strong>: rely heavily on manually
designed meta-paths</li>
</ol>
<p>so the author proposes RippleNet:</p>
<ol type="1">
<li>combine embedding-based and path-based() methods
<ol type="1">
<li>RippleNet incorporates the KGE methods into recommendation naturally
by preference propagation;<br />
</li>
<li>RippleNet can automatically discover possible paths from an item in
a user’s history to a candidate item.</li>
</ol></li>
</ol>
<h1 id="method">Method</h1>
<p>专注于挖掘KG中用户感兴趣的实体！！</p>
<h2 id="input">Input</h2>
<p>interaction matrix <strong>Y</strong> <em>and knowledge graph</em>
<strong>G</strong></p>
<h2 id="some-definition">Some definition</h2>
<h3 id="relevant-entity">Relevant entity</h3>
<p>the set of <strong>k</strong>-hop relevant entities for user
<strong>u</strong> is defined as</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302155703683.png"
alt="image-20230302155703683" />
<figcaption aria-hidden="true">image-20230302155703683</figcaption>
</figure>
<p><span class="math inline">\(\varepsilon_u^0=V_u =
\{v|y_{uv}=1\}\)</span> is the items which the user interacts with, and
they can link with entities in knowledge graph</p>
<p>can be seen as the seed set of user u in
KG(就是user如何参与到KG中)</p>
<h3 id="ripple-set">Ripple set</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302155653732.png"
alt="image-20230302155653732" />
<figcaption aria-hidden="true">image-20230302155653732</figcaption>
</figure>
<h2 id="model">Model</h2>
<h3 id="first-layer-propagation">First layer propagation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302163505799.png"
alt="image-20230302163505799" />
<figcaption aria-hidden="true">image-20230302163505799</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302165453023.png"
alt="image-20230302165453023" />
<figcaption aria-hidden="true">image-20230302165453023</figcaption>
</figure>
<p>v: embedding of item. Item embedding can incorporate one-hot ID ,
attributes of an item, based on the application scenario.</p>
<p>r: embedding of relation between head entity and tail entity.</p>
<p>h: embedding of head entity.</p>
<p>t: embedding of tail entity.</p>
<p>attention weight <span class="math inline">\(p_i\)</span> can be
regarded as the similarity of item <strong>v</strong> and the entity
<span class="math inline">\(h_i\)</span> measured in the space of
relation <span class="math inline">\(r_i\)</span>.</p>
<p><span class="math inline">\(r_i\)</span> is important, since an
item-entity pair may have different similarities when measured by
different relations</p>
<h3 id="multi-layer">Multi-layer</h3>
<p>the second layer just replace v with <span
class="math inline">\(o_u^1\)</span></p>
<p><span class="math display">\[
p_i = softmax(o_u^{1T}R_ih_i) =
\frac{exp(o_u^{1T}T_ih_i)}{\sum_{(h,r,t)\in S_u^2}exp(o_u^{1T}Rh)}
\]</span></p>
<p><span class="math display">\[
o_u^2 = \sum_{(h_i,r_i,t_i)\in S_u^2}p_it_i
\]</span></p>
<p>and third layer replace <span class="math inline">\(o_u^1\)</span>
with <span class="math inline">\(o_u^2\)</span></p>
<p>while</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302165932080.png"
alt="image-20230302165932080" />
<figcaption aria-hidden="true">image-20230302165932080</figcaption>
</figure>
<h3 id="predict">predict</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302170052985.png"
alt="image-20230302170052985" />
<figcaption aria-hidden="true">image-20230302170052985</figcaption>
</figure>
<h3 id="whole-process">Whole process</h3>
<p><strong>Propagation only used in KG-graph</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/6C506EFAADC22D9AC38B07273F102601.png"
alt="6C506EFAADC22D9AC38B07273F102601" />
<figcaption
aria-hidden="true">6C506EFAADC22D9AC38B07273F102601</figcaption>
</figure>
<p>模型不断扩散，不断获取更高层数neighbor的信息，最后通过加在一起汇总</p>
<p>所以与曾经互动过的item有关系的实体信息（KG信息）汇总为user
embedding，最后再与没互动过的item计算估计互动概率，</p>
<p>所以是否能理解为user汇总的KG信息</p>
<h3 id="loss-function还没想明白">Loss Function（还没想明白）</h3>
<p>别人的笔记：：</p>
<p>这里的分成三个部分：分别是预测分数的交叉熵损失，知识图谱特征表示的损失，参数正则化的损失：</p>
<p>预测部分的损失很好理解，就是用户和该item之间的预测值和真实值的loss</p>
<p>知识图谱特征表示的损失：我们在计算每个阶段的加权求和时上面说了，假设前提是hR=t，这是假设，所以我们需要设一个loss让模型学习，学习的内容就是hR和t之间计算相似度后，预测0,1是否相似</p>
<p>l2正则化损失：每一个hop中h，r，t分别和自己相乘后，求和再求均值得到一个值，即为该loss（这里我理解的不是很深，有了解的可以评论区说说）</p>
<h1 id="experiment">Experiment</h1>
<h1 id="other">Other</h1>
<ol type="1">
<li><p>ripple set 可能太大，</p>
<p>在RippleNet中，我们可以对固定大小的邻居集进行采样，而不是使用完整的纹波集来进一步减少计算开销。</p></li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>Unsolve problem</title>
    <url>/2023/03/02/Unsolve_question/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<p>KGCN Note</p>
<p>about ripple net, why</p>
<p>the relation <strong>R</strong> can hardly be trained to capture the
sense of importance in the quadratic form <strong>v</strong>
⊤<strong>Rh</strong> ??</p>
<p>about attention：</p>
<p>所以KGCN不用propagation更新用户的原因是否是因为希望user的embedding能专注于提取个性化信息，但是这样是否会让user和item没那么好聚类？</p>
<ol start="2" type="1">
<li><p>RippleNet</p>
<p>然后将Rh和v相乘并删除上一步增加的维度得到样本v和实体h在关系R的空间中的相似度？？为啥是R空间</p>
<p>为什么KGCN又不能</p></li>
</ol>
<p>问老师</p>
<p>1.如果训练的太慢怎么办</p>
<p>2.是否要做CF</p>
<p>3.对于先进技术用于推荐系统</p>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>常见激活函数</title>
    <url>/2023/02/28/activate_function/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="常见激活函数">常见激活函数</h1>
<p>激活函数作用：加入非线性因素</p>
<h2 id="sigmoid">Sigmoid</h2>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1+exp(-x)}
\]</span></p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228224935533.png" alt="image-20230228224935533" style="zoom:40%;" /></p>
<p>输出的值范围在[0,1]之间。但是<code>sigmoid</code>型函数的输出存在<strong>均值不为0</strong>的情况，并且存在<strong>梯度消失的问题</strong>，在深层网络中被其他激活函数替代。在<strong>逻辑回归</strong>中使用的该激活函数用于输出<strong>分类</strong>。</p>
<h3 id="求导公式">求导公式</h3>
<p>链式法则</p>
<h3 id="梯度消失原因">梯度消失原因：</h3>
<p><span class="math display">\[
\sigma&#39;(x) = \sigma\space \cdot (1-\sigma)
\]</span></p>
<ol type="1">
<li>sigmoid函数两边的斜率趋向0，很难继续学习</li>
<li>sigmoid导数两个部分都小于1，在深层神经网络中，靠前layer参数会因为后面多层sigmoid导数叠加（链式法则）导致更新的特别慢。</li>
</ol>
<h3 id="缺点解决办法">缺点解决办法</h3>
<ol type="1">
<li>在深层网络中被其他激活函数替代。如<code>ReLU(x)</code>、<code>Leaky ReLU(x)</code>等</li>
<li>在分类问题中，sigmoid做激活函数时，使用交叉熵损失函数替代均方误差损失函数。</li>
<li>采用正确的权重初始化方法（让初始化的数据尽量不要落在梯度消失区域）</li>
<li>加入BN层（同上，避免数据落入梯度消失区）</li>
<li>分层训练权重</li>
</ol>
<h2 id="tanh">tanh</h2>
<p><span class="math display">\[
tanh(x) = \frac{e^x-e^{(-x)}}{e^x+e^{(-x)}} =\frac{e^{2x}-1}{e^{2x}+1}=
2 \cdot sigmoid(2x)-1
\]</span></p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194241514.png" alt="image-20230307194241514" style="zoom:67%;" /></p>
<p><code>tanh(x)</code>型函数可以解决<code>sigmoid</code>型函数的<strong>期望（均值）不为0</strong>的情况。函数输出范围为(-1,+1)。但<code>tanh(x)</code>型函数依然存在<strong>梯度消失的问题</strong>。</p>
<p>在LSTM中使用了<code>tanh(x)</code>型函数。</p>
<h2 id="relu">Relu</h2>
<p><code>ReLU(x)</code>型函数可以有效避免<strong>梯度消失的问题</strong>，公式如下：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228222815687.png"
alt="image-20230228222815687" />
<figcaption aria-hidden="true">image-20230228222815687</figcaption>
</figure>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194352199.png" alt="image-20230307194352199" style="zoom:67%;" /></p>
<p><code>ReLU(x)</code>型函数的缺点是<strong>负值成为“死区”</strong>，神经网络无法再对其进行响应。Alex-Net使用了<code>ReLU(x)</code>型函数。当我们训练深层神经网络时，最好使用<code>ReLU(x)</code>型函数而不是<code>sigmoid(x)</code>型函数。</p>
<p>ReLU梯度稳定，值还比sigmoid大，所以<strong>可以加快网络训练</strong>。</p>
<p>但是要注意，我们在输入图像时就要注意，应该使用Min-Max归一化，而不能使用Z-score归一化。（避免进入死区）</p>
<h3 id="在0点不可导">在0点不可导</h3>
<p>人为将梯度规定为0（源码就是这么写的）</p>
<h2 id="relu6">Relu6</h2>
<p>Relu的正值输出是[0，无穷大]，但计算机内存优先，所以限定relu最大值为6</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194457525.png" alt="image-20230307194457525" style="zoom:67%;" /></p>
<h2 id="leakyrelu">LeakyRelu</h2>
<p>为<strong>负值增加了一个斜率</strong>，缓解了“死区”现象，公式如下：</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194659126.png" alt="image-20230307194659126" style="zoom:67%;" /></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228222900735.png"
alt="image-20230228222900735" />
<figcaption aria-hidden="true">image-20230228222900735</figcaption>
</figure>
<p><code>Leaky ReLU(x)</code>型函数缺点是，<strong>超参数a（阿尔法）合适的值不好设定</strong>。当我们想让神经网络能够学到负值信息，那么使用该激活函数。</p>
<h2 id="p-relu-参数化relu">P-Relu 参数化Relu</h2>
<p>数化ReLU（P-ReLU）。参数化ReLU为了解决超参数a（阿尔法）合适的值不好设定的问题，干脆将这个参数也融入模型的整体训练过程中。也使用误差反向传播和随机梯度下降的方法更新参数。</p>
<h2 id="r-relu-随机化relu">R-Relu 随机化Relu</h2>
<p>就是超参数a（阿尔法）随机化，<strong>让不同的层自己学习不同的超参数</strong>，但随机化的超参数的分布符合均值分布或高斯分布。</p>
<h2 id="mish激活函数">Mish激活函数</h2>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194824945.png" alt="image-20230307194824945" style="zoom:67%;" />
<span class="math display">\[
Mish(x) = x\cdot tanh(log(1+e^x))
\]</span></p>
<p>在负值中，允许有一定的梯度流入。</p>
<h2 id="elu指数化线性单元">ELU指数化线性单元</h2>
<p>也是为了解决死区问题，公式如下：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194918301.png"
alt="image-20230307194918301" />
<figcaption aria-hidden="true">image-20230307194918301</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228224119801.png"
alt="image-20230228224119801" />
<figcaption aria-hidden="true">image-20230228224119801</figcaption>
</figure>
<p>缺点是<strong>指数计算量大</strong>。</p>
<h2 id="maxout">Maxout</h2>
<p>就是用一个MLP层作为激活函数。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307195003327.png"
alt="image-20230307195003327" />
<figcaption aria-hidden="true">image-20230307195003327</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307195013184.png"
alt="image-20230307195013184" />
<figcaption aria-hidden="true">image-20230307195013184</figcaption>
</figure>
<p>与常规的激活函数不同，<strong>Maxout</strong>是一个可以学习的<strong>分段线性函数</strong>。其原理是，任何ReLU及其变体等激活函数都可以看成分段的线性函数，而Maxout加入的一层神经元正是一个可以学习参数的分段线性函数。</p>
<p>优点是其拟合能力很强，理论上可以拟合任意的凸函数。缺点是参数量激增！在Network-in-Network中使用的该激活函数。</p>
<h1 id="softmax求导">Softmax求导</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307195822249.png"
alt="image-20230307195822249" />
<figcaption aria-hidden="true">image-20230307195822249</figcaption>
</figure>
<p>要结合交叉熵loss函数考虑</p>
<p><span class="math inline">\(\frac{dL}{dz}=\frac{dL}{da}\cdot
\frac{da}{dz}\)</span></p>
<p>假设第j个类别是正确的，<span
class="math inline">\(y_j=1\)</span>,其它为0</p>
<p><span class="math inline">\(L = -\sum_{i=1}^ny_iln(a_i)\)</span></p>
<p><span class="math inline">\(\frac{dL}{da} =
-y_iln(a_j)=-ln(a_j)\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307200559541.png"
alt="image-20230307200559541" />
<figcaption aria-hidden="true">image-20230307200559541</figcaption>
</figure>
<p>所以最终Loss只跟label类别有关</p>
<p>所以当i=j：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307201705230.png"
alt="image-20230307201705230" />
<figcaption aria-hidden="true">image-20230307201705230</figcaption>
</figure>
<p>当i!=j:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307201737792.png"
alt="image-20230307201737792" />
<figcaption aria-hidden="true">image-20230307201737792</figcaption>
</figure>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>算法</title>
    <url>/2023/02/13/algorithm/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="二叉树">二叉树</h1>
<h2 id="完全二叉树">完全二叉树</h2>
<ol type="1">
<li>树的深度=一直遍历最左节点的长度</li>
<li>左子树深度==右子树深度，左子树是全满的完全二叉树，如果左子树深度大于右子树深度，右子树是全满的完全二叉树</li>
</ol>
<h2 id="平衡二叉树">平衡二叉树</h2>
<p>所有节点左右子树高度不大于1</p>
<h1 id="字典序">字典序</h1>
<p>就是按照字典排列顺序，英文字母按下面方式排列：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ABCDEFG HIJKLMN OPQRST UVWXYZ</span><br><span class="line">abcdefg hijklmn opqrst uvwxyz</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>Hyper Knowledge Graph</title>
    <url>/2023/06/17/hyper-knowledgegraph/</url>
    <content><![CDATA[<p>Hypergraph knowledge graph</p>
<span id="more"></span>
<h1 id="khnn">KHNN</h1>
<p>Paper: Knowledge-Aware Hypergraph Neural Network for Recommender
Systems</p>
<p>总结：用CKAN方法表示user和item，用hyperedge将l-hop的node全部连在一起，用（l-1）hop和l-hop
concate卷积计算出l-hop节点的权重与l-hop节点相乘，再做conv最后得到该层的一维embedding，然后再aggregation</p>
<h2 id="methodology">Methodology</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617154902916.png"
alt="image-20230617154902916" />
<figcaption aria-hidden="true">image-20230617154902916</figcaption>
</figure>
<h3 id="knowledge-aware-hypergraph-construction">Knowledge-Aware
Hypergraph Construction</h3>
<p><strong>Initial Hyperedge Construction</strong></p>
<p>use user’s interacted items to represent user u</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617155650437.png"
alt="image-20230617155650437" />
<figcaption aria-hidden="true">image-20230617155650437</figcaption>
</figure>
<p>use items, which have been watched by the same user, to construct the
initial item set of item v</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617155746509.png"
alt="image-20230617155746509" />
<figcaption aria-hidden="true">image-20230617155746509</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617155754397.png"
alt="image-20230617155754397" />
<figcaption aria-hidden="true">image-20230617155754397</figcaption>
</figure>
<p><strong>Knowledge Hyperedge Construction</strong></p>
<p>让l-hop neighbor 与(l-1)-hop
neighbor在相连，即所有节点被一条hyper-edge相连，主要服务于下面的neighborhood
convolution</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617161110960.png"
alt="image-20230617161110960" />
<figcaption aria-hidden="true">image-20230617161110960</figcaption>
</figure>
<h3 id="knowledge-aware-hypergraph-convolution"><strong>Knowledge-Aware
Hypergraph Convolution</strong></h3>
<p><strong>Neighborhood Convolution</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617162200596.png"
alt="image-20230617162200596" />
<figcaption aria-hidden="true">image-20230617162200596</figcaption>
</figure>
<ol type="1">
<li><p>learn the transform matrix T from the entity vectors in both
l-order and l-1-order hyperedges for vector permutation and
weighting(entity vectors in l-order) . use 1-d conv to generate T , use
another 1-d conv to aggregate the transformed vectors.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617162545551.png"
alt="image-20230617162545551" />
<figcaption aria-hidden="true">image-20230617162545551</figcaption>
</figure>
<p><em>conv</em>1 and <em>conv</em>2 are 1-dimension convolution but
withdifffferent out channels.</p></li>
<li><p>for the initial hyper-edge</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617162625945.png"
alt="image-20230617162625945" />
<figcaption aria-hidden="true">image-20230617162625945</figcaption>
</figure></li>
<li><p>add item v information</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617162651714.png"
alt="image-20230617162651714" />
<figcaption aria-hidden="true">image-20230617162651714</figcaption>
</figure></li>
<li><p>combine</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617162708343.png"
alt="image-20230617162708343" />
<figcaption aria-hidden="true">image-20230617162708343</figcaption>
</figure></li>
<li><p>aggregation</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617162731699.png"
alt="image-20230617162731699" />
<figcaption aria-hidden="true">image-20230617162731699</figcaption>
</figure></li>
</ol>
<h1 id="lgcl">LGCL</h1>
<p>Paper：Line Graph Contrastive Learning for Link Prediction</p>
<h2 id="methodology-1">Methodology</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617174642449.png"
alt="image-20230617174642449" />
<figcaption aria-hidden="true">image-20230617174642449</figcaption>
</figure>
<h1 id="hpr">HPR</h1>
<p>Paper: Empowering Knowledge Graph Construction with Hyper-graph for
Personalized Recommendation</p>
<p>basic idea: You might like something that someone with similar
preferences likes you.</p>
<p>总结：将相似度高的user作为hyper-edge（本质上是扩展了target
user的1-hop neighbor），通过计算user的l-hop neighbor与target
item的相似度来计算出user的embedding</p>
<h2 id="methodology-2">Methodology</h2>
<p>we would like to calculate the probability of <span
class="math inline">\(u_1\)</span> will interact with <span
class="math inline">\(i\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617144000884.png"
alt="image-20230617144000884" />
<figcaption aria-hidden="true">image-20230617144000884</figcaption>
</figure>
<h3 id="hyper-graph-learning">Hyper-graph learning</h3>
<ol type="1">
<li>adopt the cosine similarity to estimate the relevance between
users.</li>
<li>select some users with the highest similarity as the
hyper-edge.</li>
</ol>
<p>这里假设u1与u2最为相似，所以将u1、u2成为一条hypergraph</p>
<h3 id="knowledge-graph-construction">Knowledge Graph Construction</h3>
<p>given:</p>
<p><span class="math inline">\(\vec{i}\)</span>: the embedding of item
i</p>
<p><span class="math inline">\(S_{u_l}^1\)</span>​ : the l-hop neighbor
of user1, which takes the entities which with implicit interaction
behaviour with users as head entities.</p>
<p><span class="math inline">\(S_{u_2}^l\)</span> : the l-hop neighbor
of user2, because there is a hyperedge consist of u1 and u2</p>
<p>1-hop cal:</p>
<p>gain information between item-i and the 1-hop neighbor of user</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617151011714.png"
alt="image-20230617151011714" />
<figcaption aria-hidden="true">image-20230617151011714</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617150959919.png"
alt="image-20230617150959919" />
<figcaption aria-hidden="true">image-20230617150959919</figcaption>
</figure>
<p><span class="math inline">\(N_u^1:S^1_{u_1} \or S^1_{u_2}\)</span>
user one-hop neighbor (include hyperedge)</p>
<p>multi-hop cal: get <span class="math inline">\(q_u^{l}\)</span></p>
<p>the user embedding:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617151325200.png"
alt="image-20230617151325200" />
<figcaption aria-hidden="true">image-20230617151325200</figcaption>
</figure>
<p>predict:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230617151352536.png"
alt="image-20230617151352536" />
<figcaption aria-hidden="true">image-20230617151352536</figcaption>
</figure>
<h1 id="hype">HypE</h1>
<p>Paper:Knowledge Hypergraphs: Extending Knowledge Graphs Beyond Binary
Relations</p>
<p>score： convolution-based embedding method for knowledge
hypergraph</p>
<p>总结：做KG图的连接预测，无GNN方法，主要是embedding计算方法。考虑到entity在triple中的i个位置，在这个位置有训练出来的filter，对embbeding进行转换，最后计算概率score，</p>
<p>计算成本较低。</p>
]]></content>
      <categories>
        <category>GNN</category>
        <category>hypergraph</category>
      </categories>
  </entry>
  <entry>
    <title>常见损失函数</title>
    <url>/2023/02/28/loss_function/</url>
    <content><![CDATA[<p>常见损失函数及常见问题</p>
<span id="more"></span>
<h1 id="常见损失函数">常见损失函数</h1>
<p><strong>损失函数</strong>用来评价模型的<strong>预测值</strong>和<strong>真实值</strong>不一样的程度，在模型正常拟合的情况下，损失函数值越低，模型的性能越好。不同的模型用的损失函数一般也不一样。</p>
<p><strong>损失函数</strong>分为<strong>经验风险损失函数</strong>和<strong>结构风险损失函数</strong>。经验风险损失函数指<strong>预测结果</strong>和<strong>实际结果</strong>的差值，结构风险损失函数是指<strong>经验风险损失函数</strong>加上<strong>正则项</strong>。</p>
<h2 id="常用">常用</h2>
<h3 id="用于回归">用于<strong>回归</strong>：</h3>
<h4 id="绝对值损失函数">绝对值损失函数</h4>
<p><span class="math display">\[
L(Y,f(x)) = |Y-f(x)|
\]</span></p>
<h4 id="平方损失函数">平方损失函数</h4>
<p><span class="math display">\[
L(Y,f(x)) = (Y-f(x))^2
\]</span></p>
<p>对n个数据求平方损失后加和求平均叫<strong>均方误差MSE</strong>，常在<strong>线性回归</strong>使用
<span class="math display">\[
\frac{1}{N}\sum_n(Y-f(x))^2
\]</span></p>
<h3 id="用于分类">用于分类</h3>
<h4 id="损失函数zero-one-loss">0-1损失函数（zero-one loss）</h4>
<p><span class="math display">\[
L(Y,f(x)) = \left\{
\begin{array}{rcl}
1   &amp;   &amp;{Y!=f(x)}\\
0   &amp;   &amp;{Y=f(x)}
\end{array} \right.
\]</span></p>
<p>非黑即白，过于严格，用的很少，比如<strong>感知机</strong>用。</p>
<p>可通过设置阈值放宽条件 <span class="math display">\[
L(Y,f(x)) = \left\{
\begin{array}{rcl}
1   &amp;   &amp;{|Y-f(x)&gt;=T}\\
0   &amp;   &amp;{|Y-f(x)&lt;T}
\end{array} \right.
\]</span></p>
<h4 id="对数损失函数log-loss">对数损失函数（log loss）</h4>
<p><span class="math display">\[
L(Y,P(Y|X)) = -logP(Y|X)
\]</span></p>
<p>Y为真实分类，<span
class="math inline">\(P(Y|X)\)</span>为X条件下分类为Y的概率。用于最大似然估计，等价于交叉熵损失函数</p>
<p>加负号原因：习惯在模型更准确的情况下，loss函数越小</p>
<p>加log原因：这和最大（极大）似然估计有关，对数损失是用于最大似然估计的。</p>
<p><strong>最大似然估计</strong>：<strong>利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值</strong>。</p>
<p>我们假定一组参数（<span
class="math inline">\(\Theta\)</span>）在一堆数据（样本结果<span
class="math inline">\(x_1,x_2...\)</span>）下的<strong>似然值</strong>为<code>P(θ|x1,x2,...,xn)=P(x1|θ)*P(x2|θ)*...*P(xn|θ)</code>，可以看出来，似然值等于每一条数据在这组参数下的条件概率<strong>之积</strong>。求概率是<strong>乘性</strong>，而求损失是<strong>加性</strong>，所以才需要借助log（对数）来<strong>转积为和</strong>，另一方面也是为了简化运算。</p>
<p>对数损失在<strong>逻辑回归</strong>和<strong>多分类任务</strong>上广泛使用。交叉熵损失函数的标准型就是对数损失函数，本质没有区别。</p>
<h4 id="交叉熵损失函数">交叉熵损失函数</h4>
<p>双分类： <span class="math display">\[
L(Y,f(x)) = -[Ylnf(x)+(1-y)ln(1-f(x))]
\]</span> 多分类： <span class="math display">\[
L(Y,f(x)) = -Ylnf(x)
\]</span></p>
<h4 id="合页损失函数hinge-loss">合页损失函数(hinge loss)</h4>
<p><span class="math display">\[
L(Y,f(x)) = max(0, 1-Y\cdot f(x))
\]</span></p>
<p>SVM就是使用的合页损失，还加上了正则项。公式意义是，当样本被正确分类且函数间隔大于1时，合页损失是0，否则损失是<span
class="math inline">\(1-Y\cdot f(x)\)</span>.</p>
<p>SVM中<span class="math inline">\(Y\cdot
f(x)\)</span>为函数间隔，对于函数间隔：</p>
<ol type="1">
<li><p>正负</p>
<p>当样本被正确分类时，<span class="math inline">\(Y\cdot
f(x)&gt;0\)</span>；当样本被错误分类时，<span
class="math inline">\(Y\cdot f(x)&lt;0\)</span>。</p></li>
<li><p>大小</p>
<p><span class="math inline">\(Y\cdot
f(x)\)</span>的绝对值代表样本距离决策边界的远近程度。<span
class="math inline">\(Y\cdot
f(x)\)</span>的绝对值越大，表示样本距离决策边界越远。因此，我们可以知道：</p></li>
</ol>
<p>​ 当<span class="math inline">\(Y\cdot f(x)&gt;0\)</span>时，<span
class="math inline">\(Y\cdot
f(x)\)</span>的绝对值越大表示决策边界对样本的区分度越好</p>
<p>​ 当<span class="math inline">\(Y\cdot f(x)&lt;0\)</span>时，<span
class="math inline">\(Y\cdot
f(x)\)</span>的绝对值越大表示决策边界对样本的区分度越差</p>
<h4 id="指数损失函数exponential-loss">指数损失函数(exponential
loss)</h4>
<p><span class="math display">\[
L(Y,f(x)) = exp(-Y\cdot f(x)) = \frac{exp(f(x))}{exp(Y)}
\]</span></p>
<p>常用于AdaBoost算法，</p>
<p><strong>那么为什么AdaBoost算法使用指数损失函数，而不使用其他损失函数呢？</strong></p>
<p>这是因为，当<strong>前向分步算法的损失函数是指数损失函数</strong>时，其学习的具体操作等价于AdaBoost算法的学习过程。</p>
<h3 id="用于分割">用于分割</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230221201538734.png"
alt="image-20230221201538734" />
<figcaption aria-hidden="true">image-20230221201538734</figcaption>
</figure>
<h3 id="用于检测">用于检测</h3>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230228205041367.png"
alt="image-20230228205041367" />
<figcaption aria-hidden="true">image-20230228205041367</figcaption>
</figure>
<h1 id="常见损失函数问题">常见<strong>损失函数问题</strong></h1>
<h2 id="交叉熵相关">交叉熵相关</h2>
<h3
id="交叉熵函数与最大似然函数的联系和区别">交叉熵函数与最大似然函数的联系和区别？</h3>
<p><strong>区别</strong>：</p>
<p><strong>交叉熵函数</strong>使用来描述模型预测值和真实值的差距大小，越大代表越不相近；</p>
<p><strong>极大似然</strong>就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！即“模型已定，参数未知”</p>
<p><strong>联系</strong>：</p>
<p><strong>交叉熵函数</strong>可以由<strong>最大似然函数</strong>在<strong>伯努利分布</strong>的条件下推导出来，或者说<strong>最小化交叉熵函数</strong>的本质就是<strong>对数似然函数的最大化</strong>。</p>
<p><img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/IMG_0115(20230224-203706).PNG" /></p>
<h3
id="在用sigmoid作为激活函数的时候为什么要用交叉熵损失函数而不用均方误差损失函数">在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？</h3>
<p>另一个问法其实是在分类问题中为什么不用均方误差做损失函数。</p>
<ol type="1">
<li><p><strong>sigmoid</strong>作为激活函数的时候，如果采用<strong>均方误差损失函数</strong>，那么这是一个<strong>非凸优化</strong>问题，不宜求解。而采用<strong>交叉熵损失函数</strong>依然是一个<strong>凸优化</strong>问题，更容易优化求解。（凸优化问题中局部最优解同时也是全局最优解）。而且<span
class="math inline">\(\frac{dL}{dW}\)</span>中，有地方为0，如果参数刚好导致<span
class="math inline">\(\frac{dL}{dW}\)</span>为0，参数就不会更新。</p>
<p><img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224210154928.png" /></p></li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224211215148.png"
alt="image-20230224211215148" />
<figcaption aria-hidden="true">image-20230224211215148</figcaption>
</figure>
<ol start="2" type="1">
<li>因为<strong>交叉熵损失函数</strong>可以<strong>完美解决平方损失函数权重更新过慢</strong>的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。</li>
</ol>
<p>​ 方损失函数权重更新过慢原因：</p>
<p>​ 梯度更新公式为：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224211408952.png"
alt="image-20230224211408952" />
<figcaption aria-hidden="true">image-20230224211408952</figcaption>
</figure>
<p>这里a是预测值，y是实际值</p>
<p>有<span
class="math inline">\(\sigma&#39;(z)\)</span>这一项而sigmoid函数两端梯度很小，导致参数更新缓慢。</p>
<p>而交叉熵函数不会有这个问题虽然有<span
class="math inline">\(\sigma(z)\)</span>但没有<span
class="math inline">\(\sigma&#39;(z)\)</span>,求导detail如下：</p>
<details>
<img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224211928602.png" >
</details>
<h3 id="交叉熵和均分函数区别">交叉熵和均分函数区别</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224212504307.png"
alt="image-20230224212504307" />
<figcaption aria-hidden="true">image-20230224212504307</figcaption>
</figure>
<h3 id="如何推导出交叉熵函数">如何推导出交叉熵函数</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224215208050.png"
alt="image-20230224215208050" />
<figcaption aria-hidden="true">image-20230224215208050</figcaption>
</figure>
<h3 id="为什么交叉熵函数有log项">为什么交叉熵函数有log项</h3>
<p>第一种：因为是公式推导出来的，比如第六题的推导，推导出来的有log项。</p>
<p>第二种：通过最大似然估计的方式求得交叉熵公式，这个时候引入log项。这是因为似然函数（概率）是乘性的，而loss函数是加性的，所以需要引入log项“<strong>转积为和</strong>”。而且也是为了<strong>简化运算</strong>。</p>
<h3 id="交叉熵的设计思想">交叉熵的设计思想</h3>
<p><strong>交叉熵函数</strong>的本质是对数函数。</p>
<p><strong>交叉熵函数</strong>使用来描述模型预测值和真实值的差距大小，越大代表越不相近。</p>
<p><strong>交叉熵损失函数</strong>可以<strong>完美解决平方损失函数权重更新过慢</strong>的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。</p>
<p>对数损失在<strong>逻辑回归</strong>和<strong>多分类任务</strong>上广泛使用。交叉熵损失函数的标准型就是对数损失函数，本质没有区别。</p>
<h2 id="cv相关">CV相关</h2>
<h3 id="yolo损失函数">Yolo损失函数</h3>
<p>Yolo是用于模板检测的模型</p>
<p>Yolo的损失函数由四部分组成：</p>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823639975/8B2446F6E2BC3932829E4B801BDBDF05"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测的中心坐标做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823762782/488A1D20613F3E03B97A925F2C63D9AF"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测边界框的宽高做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823867484/DE22034D2077B5200B2C5440D47249FC"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测的类别做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823980181/9CE8A218F55F619B9EAEBCDFCFBF6446"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测的置信度做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618824075778/79B60A7E11ACBF428FD0510200949CFC"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>我们发现每一项loss的计算都是L2
loss（平方差），即使是分类问题也是。所以说yolo是把<strong>分类</strong>问题转为了<strong>回归</strong>问题。</p>
<h3 id="iou与miou计算">IOU与MIOU计算</h3>
<p>IOU（Intersection over Union），交集占并集的大小。</p>
<figure>
<img
src="https://www.nowcoder.com/equation?tex=%0A%20%20IOU%3DJaccard%20%3D%5Cfrac%7B%7CA%5Ccap%20B%7C%7D%20%7B%7CA%5Ccup%20B%7C%7D%3D%5Cfrac%7B%7CA%5Ccap%20B%7C%7D%20%7B%7CA%7C%2B%7CB%7C-%7CA%5Ccap%20B%7C%7D%20%5C%5C%0A%20%20%5Ctag%7B.%7D%0A%20%20&amp;preview=true"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>mIOU一般都是基于类进行计算的，将每一类的IOU计算之后累加，再进行平均，得到的就是mIOU。</p>
<h2 id="其它">其它</h2>
<h3 id="kl散度">KL散度</h3>
<p>相对熵（relative
entropy），又被称为Kullback-Leibler散度（Kullback-Leibler
divergence）或信息散度（information
divergence），是<strong>两个概率分布（probability
distribution）间差异的非对称性度量</strong>
。在信息理论中，<strong>相对熵等价于两个概率分布的信息熵（Shannon
entropy）的差值</strong>。</p>
<p>设<img
src="https://www.nowcoder.com/equation?tex=P(x)&amp;preview=true"
alt="img" />，<img
src="https://www.nowcoder.com/equation?tex=Q(x)&amp;preview=true"
alt="img" />是随机变量<img
src="https://www.nowcoder.com/equation?tex=X&amp;preview=true"
alt="img" />上的两个概率分布，则在离散和连续随机变量的情形下，相对熵的定义分别为：</p>
<figure>
<img
src="https://www.nowcoder.com/equation?tex=%0AKL(P%7C%7CQ)%3D%5Csum%7BP(x)log%20%5Cfrac%7BP(x)%7D%7BQ(x)%7D%7D%20%5C%5C%0AKL(P%7C%7CQ)%3D%5Cint%7BP(x)log%20%5Cfrac%7BP(x)%7D%7BQ(x)%7Ddx%7D%20%5C%5C%0A%5Ctag%7B.%7D%0A&amp;preview=true"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><img
src="https://www.nowcoder.com/equation?tex=Q(x)&amp;preview=true"
alt="img" />为<strong>理论概率分布</strong>，<img
src="https://www.nowcoder.com/equation?tex=P(x)&amp;preview=true"
alt="img" />为模型<strong>预测概率分布</strong>，而KL就是度量这两个分布的差异性，当然差异越小越好，所以KL也可以用作损失函数。</p>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>常见优化函数</title>
    <url>/2023/03/07/optimizer/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="常见优化函数">常见优化函数</h1>
<h2 id="梯度下降gd决定优化方向">梯度下降GD(决定优化方向)</h2>
<p><strong>梯度下降的核心思想：负梯度方向是使函数值下降最快的方向</strong></p>
<h3 id="批次梯度下降bgd">批次梯度下降BGD</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307202943629.png"
alt="image-20230307202943629" />
<figcaption aria-hidden="true">image-20230307202943629</figcaption>
</figure>
<p><strong>优点</strong>：在梯度下降法中，因为每次都遍历了完整的训练集，<strong>其能保证结果为全局最优</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307204257470.png"
alt="image-20230307204257470" />
<figcaption aria-hidden="true">image-20230307204257470</figcaption>
</figure>
<p><strong>缺点</strong>：我们需要对于每个参数求偏导，且在对每个参数求偏导的过程中还需要对训练集遍历一次，当训练集（m）很大时，计算费时</p>
<p><strong>解决方法</strong>：使用minibatch去更新</p>
<h3 id="随机梯度下降">随机梯度下降</h3>
<p>为了解决BGD耗时过长，它是利用单个样本的损失函数对θ求偏导得到对应的梯度，来更新θ，更新过程如下：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307204324179.png"
alt="image-20230307204324179" />
<figcaption aria-hidden="true">image-20230307204324179</figcaption>
</figure>
<p>速度快，但受抽样影响大，<strong>噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307204553630.png"
alt="image-20230307204553630" />
<figcaption aria-hidden="true">image-20230307204553630</figcaption>
</figure>
<p>因为每一次迭代的梯度受抽样的影响比较大，学习率需要逐渐减少，否则模型很难收敛。在实际操作中，一般采用线性衰减：
<span class="math display">\[
\eta_k=(1-\alpha)\eta_0+\alpha\eta_{\tau}
\]</span></p>
<p><span class="math display">\[
\alpha=\frac{k}{\tau}
\]</span></p>
<p><span class="math inline">\(\eta_0\)</span>:初始学习率</p>
<p><span class="math inline">\(\eta_{\tau}\)</span>：
最后一次迭代的学习率</p>
<p><span class="math inline">\(\tau\)</span>：自然迭代次数</p>
<p><span class="math inline">\(\eta_{\tau}\)</span>设为<span
class="math inline">\(\eta_0\)</span>的1%，k一般设为100的倍数。</p>
<p><strong>优点</strong>：收敛速度快</p>
<p><strong>缺点</strong>：</p>
<ol type="1">
<li><p>训练不稳定：噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p></li>
<li><p>选择适当的学习率可能很困难。
太小的学习率会导致收敛性缓慢，而学习速度太大可能会妨碍收敛，并导致损失函数在最小点波动。</p></li>
<li><p>无法逃脱鞍点</p></li>
</ol>
<details>
在数学中，鞍点或极小值点是函数图形表面上的一个点，其正交方向上的斜率(导数)均为零(临界点)，但不是函数的局
部极值。一句话概括就是：一个不是局部极值点的驻点称为鞍点。
*驻点：函数在一点处的一阶导数为零。
<img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307205942585.png">
<details>
<h3 id="min-batch-小批量梯度下降mbgd">min-batch 小批量梯度下降MBGD</h3>
<p><strong>算法的训练过程比较快，而且也要保证最终参数训练的准确率</strong></p>
<p>m表示一个批次的数据个数</p>
<h2 id="动量方法">动量方法</h2>
<h3 id="momentum随机梯度下降">Momentum随机梯度下降</h3>
<p>核心思想：Momentum借用了物理中的<strong>动量</strong>概念,即前一次的梯度也会参与运算。为了表示动量，引入了<strong>一阶动量</strong>m。<img
src="https://www.nowcoder.com/equation?tex=m&amp;preview=true"
alt="img" />是之前的梯度的累加,但是每回合都有一定的衰减。公式如下：
<span class="math display">\[
m_t=\beta m_{t-1}+(1-\beta)\cdot g_t
\]</span></p>
<p><span class="math display">\[
w_{t+1}=w_t-\eta \cdot m_t
\]</span></p>
<p><span class="math inline">\(g_t\)</span>：
为第t次计算的梯度（就是现在要算这次）</p>
<p><span class="math inline">\(m_{t-1}\)</span>: 为之前梯度的累加</p>
<p><span class="math inline">\(\beta\)</span>: 动量因子</p>
<p>所以当前权值的改变受上一次改变的影响，类似加上了<strong>惯性</strong>。</p>
<p>优点：momentum能够加速SGD收敛，抑制震荡。并且动量有机会逃脱局部极小值(鞍点)。</p>
<ol type="1">
<li>在梯度方向改变时，momentum能够降低参数更新速度，从而减少震荡；</li>
<li>在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛。</li>
</ol>
<h3 id="nesterov动量随机梯度下降法">Nesterov动量随机梯度下降法</h3>
<p>Nesterov是Momentum的变种。与Momentum唯一区别就是，计算梯度的不同。Nesterov动量中，先用当前的速度临时更新一遍参数，在用更新的临时参数计算梯度。</p>
<p>在momentum更新梯度时加入对当前梯度的校正，让梯度“多走一步”，可能跳出局部最优解：
<span class="math display">\[
w_t^*=\beta m_{t-1}+w_t
\]</span></p>
<p><span class="math display">\[
m_t=\beta m_{t-1}+(1-\beta)\cdot g_t
\]</span></p>
<p><span class="math display">\[
w_{t+1}=w_t-\eta \cdot m_t
\]</span></p>
<p>这里的<span class="math inline">\(g_t\)</span>用临时点<span
class="math inline">\(w_t^*\)</span>计算的</p>
<h2 id="更新学习率方法">更新学习率方法</h2>
<h3 id="adagrad">Adagrad</h3>
<p>引入<strong>二阶动量</strong>，根据训练轮数的不同，对学习率进行了动态调整：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307213914026.png"
alt="image-20230307213914026" />
<figcaption aria-hidden="true">image-20230307213914026</figcaption>
</figure>
<p><strong>缺点</strong>：仍然需要人为指定一个合适的全局学习率，同时网络训练到一定轮次后，分母上梯度累加过大使得学习率为0而导致训练提前结束。</p>
<h3 id="adadelta不是很懂">Adadelta(不是很懂)</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307215135905.png"
alt="image-20230307215135905" />
<figcaption aria-hidden="true">image-20230307215135905</figcaption>
</figure>
<h3 id="rmsprop">RMSProp</h3>
<p>AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSprop算法对Adagrad算法做了一点小小的修改，RMSprop使用指数衰减只保留过去给定窗口大小的梯度，使其能够在找到凸碗状结构后快速收敛。RMSProp法可以视为Adadelta法的一个特例，即依然使用全局学习率替换掉Adadelta法中的<span
class="math inline">\(s_t\)</span>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307215341546.png"
alt="image-20230307215341546" />
<figcaption aria-hidden="true">image-20230307215341546</figcaption>
</figure>
<p>推荐<span
class="math inline">\(\eta_{global}=1,\rho=0.9,\epsilon=10^{-6}\)</span></p>
<p>缺点：依然使用了全局学习率，需要根据实际情况来设定 优点：</p>
<ol type="1">
<li>分母不再是一味的增加，它会重点考虑距离它较近的梯度（指数衰减的效果）</li>
<li>只用了部分梯度加和而不是所有，这样避免了梯度累加过大使得学习率为0而导致训练提前结束。</li>
</ol>
<h3 id="adam">Adam</h3>
<p>https://zhuanlan.zhihu.com/p/377968342</p>
<p>Adam公式如下： <span class="math display">\[
m_t:=beta_1*m_{t-1}+(1-beta_1)*g
\]</span></p>
<p><span class="math display">\[
v_t:=beta_2*v_{t-1}+(1-beta_2)*g*g
\]</span></p>
<p><span class="math display">\[
variable:=variable-lr_t*\frac{m_t}{\sqrt{v_t+\epsilon}}
\]</span></p>
<p><span
class="math inline">\(m_t\)</span>可以理解为求历史梯度加强平均，思想来自动量方法，防止震荡。</p>
<p><span class="math inline">\(v_t\)</span>则是用于调整lr的，即是<span
class="math inline">\(\frac{lr}{\sqrt{v_t+\epsilon}}\)</span>,</p>
<p>在迭代过程中，如果某一维度一直以很小的梯度进行更新，证明此方向梯度变换较为稳定，因此可以加大学习率，以较大的学习率在此维度更新，体现在公式上就是：对历史梯度平方进行一阶指数平滑后，公式2会得到一个很小的值，公式3中的自适应学习率会相对较大</p>
<p>相反，某一维度在迭代过程中一直以很大的梯度进行更新，明此方向梯度变换较为剧烈（不稳定），因此可减小学习率，以较小的学习率在此维度更新
体现在公式上就是：对历史梯度平方进行一阶指数平滑后，公式2则会得到一个很大的值，公式3中的自适应学习率会相对较小</p>
<p><span
class="math inline">\(v_t\)</span>也可以解决<strong>梯度稀疏</strong>的问题；频繁更新的梯度将会被赋予一个较小的学习率，而稀疏的梯度则会被赋予一个较大的学习率，通过上述机制，在数据分布稀疏的场景，能更好利用稀疏梯度的信息，比标准的SGD算法更有效地收敛。</p>
<h1 id="常见优化函数问题">常见优化函数问题</h1>
<h2
id="sgd和adam谁收敛的比较快谁能达到全局最优解">SGD和Adam谁收敛的比较快？谁能达到全局最优解？</h2>
<p>SGD算法没有动量的概念，SGD和Adam相比，缺点是下降速度慢，对学习率要求严格。</p>
<p>而Adam引入了一阶动量和二阶动量，下降速度比SGD快，Adam可以自适应学习率，所以初始学习率可以很大。</p>
<p>SGD相比Adam，更容易达到全局最优解。主要是后期Adam的学习率太低，影响了有效的收敛。</p>
<p>我们可以前期使用Adam，后期使用SGD进一步调优。</p>
<h2 id="adam用到二阶矩的原理是什么">adam用到二阶矩的原理是什么</h2>
<p>引入二阶动量，根据训练轮数不同对学习率进行调整。</p>
<p>可以看出来，公式将前面的训练梯度平方加和，在网络训练的前期，由于分母中梯度的累加（<span
class="math inline">\(v_t\)</span>）较小，所以一开始的学习率<span
class="math inline">\(\eta_t\)</span>比较大；随着训练后期梯度累加较大时，<span
class="math inline">\(\eta_t\)</span>逐渐减小，而且是自适应地减小。</p>
<p>而且如果某个维度频繁震荡梯度大，学习率就降低；如果梯度小而稳定，学习率就大。</p>
<h2
id="batch的大小如何选择过大的batch和过小的batch分别有什么影响">Batch的大小如何选择，过大的batch和过小的batch分别有什么影响</h2>
<p><strong>Batch选择时尽量采用2的幂次，如8、16、32等</strong></p>
<p>在合理范围内，增大Batch_size的<strong>好处</strong>：</p>
<ol type="1">
<li>提高了<strong>内存利用率</strong>以及大矩阵乘法的并行化效率。</li>
<li>减少了跑完一次epoch(全数据集）所需要的迭代次数，加快了对于相同数据量的处理速度。</li>
</ol>
<p>盲目增大Batch_size的<strong>坏处</strong>：</p>
<ol type="1">
<li>提高了内存利用率，但是内存容量可能不足。</li>
<li>跑完一次epoch(全数据集)所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加，从而对参数的修正也就显得更加缓慢。</li>
<li>Batch_size增大到一定程度，其确定的下降方向已经基本不再变化。</li>
</ol>
<p>Batch_size过小的<strong>影响</strong>：</p>
<ol type="1">
<li>训练时不稳定，可能不收敛</li>
<li>精度可能更高。</li>
</ol>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>Regularization</title>
    <url>/2023/03/14/regularization/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="什么是正则化">什么是正则化</h1>
<p>目的：防止模型过拟合</p>
<p>原理：正则化通过在损失函数中<strong>引入惩罚项来限制模型的复杂度</strong>，以防止模型过度拟合训练数据。惩罚项会在优化过程中对模型的参数进行调整，以平衡模型的拟合能力和泛化能力。</p>
<p>https://www.zhihu.com/question/20924039</p>
<p>最直接的防止过拟合的方法就是减少特征数量，就是减少0范数（向量中非零元素的个数），但是0范数很难求，所以就有了1范数，2范数。</p>
<p>作用：</p>
<ol type="1">
<li>防止过拟合</li>
<li>特征选择：l1正则化</li>
<li>改善模型稳定性</li>
</ol>
<h1 id="常见正则化">常见正则化</h1>
<h2 id="l1正则化">l1正则化</h2>
<p><span class="math display">\[
l1=\lambda||\vec{w}||_1=\sum_i|w_i|
\]</span></p>
<p><span class="math inline">\(\lambda\)</span>控制约束程度</p>
<p>l1不仅可以<strong>约束参数量</strong>，还可以使<strong>参数更稀疏</strong>。因为对目标函数经过优化后，一部分参数会变为0，另一部分参数为非零实值。<strong>非零实值说明这部分参数是最重要的特征</strong>。</p>
<p>假设参数分布是Laplace分布。</p>
<h3 id="稀疏原因">稀疏原因</h3>
<p>https://blog.csdn.net/b876144622/article/details/81276818</p>
<p>https://www.zhihu.com/question/37096933/answer/70426653</p>
<p>0处导数突变，如果此时0+导数为正，优化时放负方向跑，0-导数为负数，优化时往正方向跑，就很容易落入0</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230314211808962.png"
alt="image-20230314211808962" />
<figcaption aria-hidden="true">image-20230314211808962</figcaption>
</figure>
<h3 id="缺点">缺点</h3>
<ul>
<li>非光滑性：L1
正则化的正则化项是参数的绝对值之和，这导致目标函数在参数为零时不可导。这使得优化过程变得更加困难，特别是在使用梯度下降等基于梯度的优化算法时。在参数为零附近，梯度不连续，可能导致优化过程出现问题。</li>
<li>多重共线性：当特征之间存在高度相关性（多重共线性）时，L1
正则化倾向于选择其中一个特征，而忽略其他相关特征。这可能导致模型的解释性下降，因为被忽略的相关特征可能包含有用的信息。相比之下，L2
正则化对相关特征的惩罚更均衡，可以保留更多相关特征的权重。</li>
<li>不适用于高维问题：在高维问题中，特征数量远远大于样本数量时，L1
正则化可能不太适用。由于参数空间的维度过高，L1
正则化可能无法准确地选择特征，导致过拟合或选择不稳定的特征子集。</li>
</ul>
<h2 id="l2正则化">l2正则化</h2>
<p><span class="math display">\[
l2=\frac{1}{2}\lambda||\vec{w}||_2^2=\sum_i|w_i|^2
\]</span></p>
<p>l2正则化会使部分特征<strong>趋近于0</strong>，也就达到正则化的目的了。</p>
<p>此外，l1正则化和l2正则化也可以联合使用，这种形式也被称为“<strong>Elastic网络正则化</strong>”。</p>
<p>假设参数分布是正态分布</p>
<h2 id="dropout">Dropout</h2>
<p>在训练的时候让一定量的神经元失活，在该epoch中不参与网络训练</p>
<h3
id="dropout训练出的参数需要乘以keep-prib使用">dropout训练出的参数需要乘以keep-prib使用</h3>
<p>因为神经元预测的时候就不应该随机丢弃，一种”补偿“的方案就是每个dropout训练出的神经元的权重都乘以一个<strong>p</strong>，这样在“总体上”使得<strong>测试数据</strong>和<strong>训练数据</strong>是大致一样的。保证<strong>测试</strong>的时候把这个神经元的权重乘以<strong>p</strong>可以得到<strong>同样的期望</strong>。比如一个神经元的输出是<strong>x</strong>，那么在训练的时候它有<strong>p</strong>的概率参与训练，<strong>(1-p)</strong>的概率丢弃，那么它输出的期望是<img
src="https://www.nowcoder.com/equation?tex=p%20%5Ctimes%20x%2B(1-p)%20%5Ctimes%200%20%3D%20p%20%5Ctimes%20x&amp;preview=true"
alt="img" />。因此<strong>测试</strong>的时候把这个神经元的权重乘以<strong>p</strong>可以得到<strong>同样的期望</strong>。</p>
<p>注：目前主流是采用inverted dropout替代dropout，inverted
dropout不需要乘以keep-prib。它的做法是在训练阶段对执行了dropout操作的层，其输出激活值要<strong>除以keep_prib</strong>，而测试的模型不用再做任何改动。除以（1-p），让期望与不dropout相同。</p>
<h2 id="早停">早停</h2>
<p>每一个epoch训练结束后使用<strong>验证集</strong>验证模型效果，画出训练曲线，这样就可以判断是否过拟合了。当发现网络有点过拟合了，当然就是“<strong>早停</strong>”了，可以直接停止训练了。</p>
<h2 id="扩充数据集">扩充数据集</h2>
<p>Augmentation，增加变化增加多样性</p>
<h3 id="数据增强方法">数据增强方法</h3>
<p>数据集越大，网络泛化性能越好，所以努力扩充数据集，通过平移、翻转、旋转、放缩、随机截取、加噪声、色彩抖动等等方式。</p>
<h2 id="bnbatch-normalization">BN（Batch Normalization）</h2>
<p>目的：用于解决深度网络<strong>梯度消失</strong>和<strong>梯度爆炸</strong>的问题，加速网络收敛速度。</p>
<p>批规范化，即在模型每次随机梯度下降训练时，通过mini-batch来对每一层的输出做<strong>规范化操作</strong>，使得结果（各个维度）的<strong>均值为0</strong>，<strong>方差为1</strong>，然后在进行尺度变换和偏移。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230316191341497.png"
alt="image-20230316191341497" />
<figcaption aria-hidden="true">image-20230316191341497</figcaption>
</figure>
<p>m是mini-batch中的数据个数。前面的散步是对input数据进行白化操作（线性），<strong>最后的“尺度变换和偏移”操作是为了让BN能够在线性和非线性之间做一个权衡</strong>，而这个偏移的参数是神经网络在训练时学出来的。</p>
<p>经过BN操作，网络每一层的输出小值被“拉大”，大值被“缩小”，所以就有效避免了梯度消失和梯度爆炸。<strong>总而言之，BN是一个可学习、有参数（γ、β）的网络层</strong>。</p>
<h3 id="尺度变换和偏移的作用">尺度变换和偏移的作用：</h3>
<p>归一会影响到本层网络A所学习到的特征（比如网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，如果强制把它给归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于这一层网络所学习到的特征分布<strong>被搞坏</strong>了）</p>
<p>于是<strong>BN</strong>最后的“<strong>尺度变换和偏移</strong>”操作，让我们的网络可以学习恢复出原始网络所要学习的特征分布（衡量线性和非线性）</p>
<h3 id="bn训练和测试有什么不同">BN训练和测试有什么不同</h3>
<p>训练时，均值和方差针对一个<strong>Batch</strong>。</p>
<p>测试时，均值和方差针对<strong>整个数据集</strong>而言。因此，在训练过程中除了正常的前向传播和反向求导之外，我们还要记录<strong>每一个Batch的均值和方差</strong>。</p>
<h3 id="bn和ln的差别">BN和LN的差别</h3>
<p>LN：Layer
Normalization，LN是“横”着来的，<strong>对一个样本，经过同一层的所有神经元</strong>做<strong>归一化</strong>。LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；LN不依赖于batch的大小和输入sequence的深度，因此可以用于<strong>batchsize为1</strong>和RNN中对边长的输入sequence的normalize操作。</p>
<p>BN：Batch
Normalization，BN是“竖”着来的，<strong>经过一个神经元的所有样本</strong>做<strong>归一化</strong>，所以与<strong>batch
size</strong>有关系。</p>
<p>二者提出的目的都是为了加快模型收敛，减少训练时间。</p>
<h3 id="如何同时使用bn和dropout">如何同时使用BN和dropout</h3>
<p>同时使用BN和Dropout会出现方差偏移的现象，原因：</p>
<p>使用Dropout：训练的时候以<strong>概率p</strong>
drop了一些节点，比如dropout设置为0.5，隐藏层共有6个节点，那训练的时候有3个节点的值被丢弃，而测试的时候这6个节点都被保留下来，这就导致了<strong>训练</strong>和<strong>测试</strong>的时候以该层节点为输入的下一层的神经网络节点获取的<strong>期望</strong>会有量级上的差异。为了解决这个问题，在训练时对当前dropout层的输出数据<strong>除以（1-p）</strong>，之后再输入到下一层的神经元节点，以作为失活神经元的补偿，以使得在训练时和测试时每一层的输入有大致相同的期望。</p>
<p>但是这样使得神经元输入期望大致相同，但是方差不一样，而BN是通过均值方差计算的，所以会导致输出不正确。</p>
<p>解决方法：</p>
<ol type="1">
<li>只<strong>在所有BN层的后面采用dropout层</strong>。</li>
<li>dropout原文提出了一种高斯dropout，论文再进一步对高斯dropout进行扩展，提出了一个<strong>均匀分布Dropout</strong>，这样做带来了一个好处就是这个形式的Dropout（又称为“Uout”）对方差的偏移的敏感度降低了</li>
</ol>
<h2 id="bagging-和bootstrap">Bagging 和Bootstrap？</h2>
<p><strong>Bootstrap</strong>是一种抽样方法，即随机抽取数据并将其放回。如一次抽取一个样本，然后放回样本集中，下次可能再抽取这个样本。接着将每轮未抽取的数据合并形成<strong>袋外数据集</strong>（Out
of Bag, OOB），用于模型中的测试集。</p>
<p><strong>Bagging算法</strong>使用<strong>Bootstrap方法</strong>从原始样本集中随机抽取样本。共提取K个轮次，得到K个独立的训练集，元素可以重复。用K个训练集训练K个模型。分类问题以结果中的多个值投票作为最终结果，回归问题以平均值作为最终结果。结果采用投票法，避免了决策树的过拟合问题。</p>
<p><strong>Boosting</strong>是为每个训练样本设置一个权重，在下一轮分类中，误分类的样本权重较大，即每轮样本相同，但样本权重不同；对于分类器来说，分类误差小的分类器权重较大，反之则小。</p>
<p><strong>采用模型融合的方式也可以避免过拟合</strong>。</p>
<h2 id="参数共享">参数共享</h2>
<p>参数共享是一种在神经网络中常用的正则化方法，特别适用于卷积神经网络（CNN）。通过在神经网络的不同层之间共享参数，可以减少模型的参数数量，提高模型的效率和泛化能力。</p>
<h2 id="数据标准化">数据标准化</h2>
<p>数据标准化是对数据进行预处理的一种方式，将数据按特征进行缩放，使得每个特征的均值为
0，标准差为
1。这有助于使不同特征之间的尺度一致，提高模型的收敛速度和性能。</p>
<ul>
<li>z-score</li>
<li>min-max</li>
</ul>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
</search>
