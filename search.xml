<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>KG-review</title>
    <url>/2023/03/03/KG-Rec/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<iframe height="800" width="60%" src="https://xmind.works/share/JMcp4hCx" frameborder="0" allowfullscreen>
</iframe>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>KGAT</title>
    <url>/2023/02/24/KGAT/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>利用KG作为辅助信息，并将KG与user-item graph 整合为一个图</p>
<h2 id="background-1">Background</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224155340260.png"
alt="image-20230224155340260" />
<figcaption aria-hidden="true">image-20230224155340260</figcaption>
</figure>
<p><strong>Previous model:</strong></p>
<p>CF: behaviorally similar users would exhibit similar preference on
items.</p>
<p><strong>focus on the histories of similar users who also watched
<span class="math inline">\(i1\)</span>, i.e., <span
class="math inline">\(u4\)</span> and <span
class="math inline">\(u5\)</span>;</strong></p>
<p>SL: transform side information into a generic feature vector,
together with user ID and item ID, and feed them into a supervised
learning (SL) model to predict the score.</p>
<p><strong>emphasize the similar items with the attribute <span
class="math inline">\(e1\)</span>, i.e.$ i2$.</strong></p>
<p><strong>current problem:</strong></p>
<p>existing SL methods fail to unify them, and ignore other
relationships in the graph:</p>
<ol type="1">
<li>the users in the yellow circle who watched other movies directed by
the same person <span class="math inline">\(e_1\)</span>.</li>
<li>the items in the grey circle that share other common relations with
<span class="math inline">\(e_1\)</span>.</li>
</ol>
<h2 id="user-item-bipartite-graph-g_1">User-Item Bipartite Graph: <span
class="math inline">\(G_1\)</span></h2>
<p><span class="math display">\[
\{(u,y_{ui},i)|u\in U, i\in I\}
\]</span> <span class="math inline">\(U\)</span>: user sets</p>
<p><span class="math inline">\(I\)</span>: item sets</p>
<p><span class="math inline">\(y_{ui}\)</span>: if user <span
class="math inline">\(u\)</span> interacts with item <span
class="math inline">\(i\)</span> <span
class="math inline">\(y_{ui}\)</span>=, else <span
class="math inline">\(y_{ui}\)</span>=0.</p>
<h2 id="knowledge-graph-g2">Knowledge Graph <span
class="math inline">\(G2\)</span></h2>
<p><span class="math display">\[
\{(h,r,t)|h,t\in E, r\in R\}
\]</span></p>
<p><span class="math inline">\(t\)</span> there is a relationship <span
class="math inline">\(r\)</span> from head entity <em>h</em> to tail
entity <span class="math inline">\(t\)</span>.</p>
<h2 id="ckg-combination-of-g1-and-g2"><span
class="math inline">\(CKG\)</span>: Combination of <span
class="math inline">\(G1\)</span> and <span
class="math inline">\(G2\)</span></h2>
<ol type="1">
<li>represent each user-item behavior as a triplet $ (u,
Interact,i)<span class="math inline">\(, where\)</span> y^{ui}$ =
1.</li>
<li>we establish a set of item-entity alignments</li>
</ol>
<p><span class="math display">\[
A = \{(i, e)|i ∈ I, e ∈ E \}
\]</span></p>
<ol start="3" type="1">
<li>based on the item-entity alignment set, the user-item graph can be
integrated with KG as a unified graph.</li>
</ol>
<p><span class="math display">\[
G = \{(h,r,t)|h,t ∈ E^′,r ∈R^′\}
\]</span></p>
<p><span class="math display">\[
E^′ = E ∪ U
\]</span></p>
<p><span class="math display">\[
R^′ = R ∪ {Interact}
\]</span></p>
<h1 id="methodology">Methodology</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222185609261.png"
alt="image-20230222185609261" />
<figcaption aria-hidden="true">image-20230222185609261</figcaption>
</figure>
<p>KGAT has three main components:</p>
<ol type="1">
<li>Embedding layer</li>
<li>Attentive embedding propagation layer</li>
<li>prediction layer</li>
</ol>
<h2 id="embedding-layer">Embedding layer</h2>
<p>Using <strong>TransR</strong> to calculate embedding</p>
<p><strong>Assumption</strong>: if a triplet (h,r,t) exist in the graph,
<span class="math display">\[
e^r_h+e_r\approx e_t^r
\]</span> Herein, <span class="math inline">\(e^h\)</span>, <span
class="math inline">\(e^t\)</span> ∈ <span
class="math inline">\(R^d\)</span> and <span
class="math inline">\(e^r\)</span> ∈ <span
class="math inline">\(R^k\)</span>are the embedding for <em>h</em>,
<em>t</em>, and <em>r</em>; and <span
class="math inline">\(e^r_h\)</span>, <span
class="math inline">\(e^r_t\)</span> are the projected representations
of <span class="math inline">\(e^h\)</span>, <span
class="math inline">\(e^t\)</span> in the relation <em>r</em>’s
space.</p>
<p><strong>Plausibility score</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222193700417.png"
alt="image-20230222193700417" />
<figcaption aria-hidden="true">image-20230222193700417</figcaption>
</figure>
<p><span class="math inline">\(W_r ∈ R^{k\times d}\)</span> is the
transformation matrix of relation <em>r</em>, which projects entities
from the <em>d</em>-dimension entity space into the <em>k</em> dimension
relation space.</p>
<p>A lower score suggests that the triplet is more likely to be
true.</p>
<p><strong>Loss</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222195306105.png"
alt="image-20230222195306105" />
<figcaption aria-hidden="true">image-20230222195306105</figcaption>
</figure>
<p><span class="math inline">\(\{(h,r,t,t^′ )|(h,r,t) \in G, (h,r,t^′ )
\notin G\}\)</span>, <span class="math inline">\((h,r,t^′ )\)</span> is
a negative sample constructed by replacing one entity in a valid triplet
randomly.</p>
<p><em>σ</em>(·): sigmoid function, ——》将分数映射再0-1区间，归一化</p>
<p>？？？？？？？？？？？why this layer model working as a
regularizer</p>
<h2 id="attentive-embedding-propagation-layersupon-gcn">Attentive
Embedding Propagation Layers(upon GCN)</h2>
<h3 id="first-order-propagation">First-order propagation</h3>
<p>和之前模型不同，这个的propagation layer encode了<span
class="math inline">\(e_r\)</span>.</p>
<p>For entity h, the information propagating from neighbor is :</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222201217039.png"
alt="image-20230222201217039" />
<figcaption aria-hidden="true">image-20230222201217039</figcaption>
</figure>
<p><span class="math inline">\(π(h,r,t)\)</span>: to controls the decay
factor on each propagation on edge (<em>h</em>,<em>r</em>,<em>t</em>),
indicating how much information is propagated</p>
<p>from <em>t</em> to <em>h</em> conditioned to relation <em>r</em>.</p>
<p>For <span class="math inline">\(π(h,r,t)\)</span>, we use attention
mechanism:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222204949423.png"
alt="image-20230222204949423" />
<figcaption aria-hidden="true">image-20230222204949423</figcaption>
</figure>
<p>This makes the attention score dependent on the distance between
<span class="math inline">\(e^h\)</span> and <span
class="math inline">\(e^t\)</span> in the relation <em>r</em>’s
space.</p>
<p>这里，tanh用于增加非线性因素；但不缺定是否有归一化作用？？？？？归一化就可以把这个function的大小集中在角度上，但是这样<span
class="math inline">\(e^h_t\)</span>也没有归一化，到时候看看输出参数</p>
<p>and than use softmax to normalize(no need to use as<span
class="math inline">\(\frac1{|N_t |}\)</span><span
class="math inline">\(\frac1{|N_t ||N_h |}\)</span>)</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222234256082.png"
alt="image-20230222234256082" />
<figcaption aria-hidden="true">image-20230222234256082</figcaption>
</figure>
<p>The final part is aggregation, threre are three choices:</p>
<ol type="1">
<li>GCN aggregator</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222235616598.png"
alt="image-20230222235616598" />
<figcaption aria-hidden="true">image-20230222235616598</figcaption>
</figure>
<ol start="2" type="1">
<li>GraphSage aggregator</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222235806956.png"
alt="image-20230222235806956" />
<figcaption aria-hidden="true">image-20230222235806956</figcaption>
</figure>
<ol start="3" type="1">
<li>Bi-Interaction aggregator</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223000647293.png"
alt="image-20230223000647293" />
<figcaption aria-hidden="true">image-20230223000647293</figcaption>
</figure>
<h3 id="multi-layer-propagation">Multi-layer propagation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223000834658.png"
alt="image-20230223000834658" />
<figcaption aria-hidden="true">image-20230223000834658</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223000850960.png"
alt="image-20230223000850960" />
<figcaption aria-hidden="true">image-20230223000850960</figcaption>
</figure>
<h2 id="model-prediction">Model Prediction</h2>
<p>multi-layers combination and inner product</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223001515690.png"
alt="image-20230223001515690" />
<figcaption aria-hidden="true">image-20230223001515690</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223001526127.png"
alt="image-20230223001526127" />
<figcaption aria-hidden="true">image-20230223001526127</figcaption>
</figure>
<h2 id="optimizazion">Optimizazion</h2>
<h3 id="loss">loss</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223002144083.png"
alt="image-20230223002144083" />
<figcaption aria-hidden="true">image-20230223002144083</figcaption>
</figure>
<p><span class="math inline">\(L_{cf}\)</span> is BPR Loss</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223002439536.png"
alt="image-20230223002439536" />
<figcaption aria-hidden="true">image-20230223002439536</figcaption>
</figure>
<p><span class="math inline">\(L_{kg}\)</span> is loss forTranR .</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222195306105.png"
alt="image-20230222195306105" />
<figcaption aria-hidden="true">image-20230222195306105</figcaption>
</figure>
<h3 id="optimizer">Optimizer</h3>
<p>Adam</p>
<h3 id="updata-method">updata method</h3>
<p>we update the embeddings for all nodes;</p>
<p>hereafter, we sample a batch of (<em>u</em>,<em>i</em>, <em>j</em>)
randomly, retrieve their representations after <em>L</em> steps of
propagation, and then update model parameters by using the gradients of
the prediction loss.</p>
<p>在同一个epoch中，先把所以数据扔进tranR训练，得到loss（此时不更新参数）</p>
<p>然后sample算BPR LOSS</p>
<h1 id="experiments">EXPERIMENTS</h1>
<h2 id="rq1-performance-comparison">RQ1: Performance Comparison</h2>
<ol type="1">
<li>regular dataset</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223004843914.png"
alt="image-20230223004843914" />
<figcaption aria-hidden="true">image-20230223004843914</figcaption>
</figure>
<ol start="2" type="1">
<li><p>Sparsity Levels</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223005253034.png"
alt="image-20230223005253034" />
<figcaption aria-hidden="true">image-20230223005253034</figcaption>
</figure></li>
</ol>
<p>KGAT outperforms the other models in most cases, especially on the
two sparsest user groups.</p>
<p>说明KGAT能够缓解稀疏性影响</p>
<h2 id="rq2study-of-kgat">RQ2：Study of KGAT</h2>
<ol type="1">
<li>study of layer influence and effect of aggregators</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223010038345.png"
alt="image-20230223010038345" />
<figcaption aria-hidden="true">image-20230223010038345</figcaption>
</figure>
<ol start="2" type="1">
<li><p>cut attention layer and TransR layer</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223010347815.png"
alt="image-20230223010347815" />
<figcaption aria-hidden="true">image-20230223010347815</figcaption>
</figure></li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>KGCN</title>
    <url>/2023/03/02/KGCN/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<h2 id="cf-questions">CF Questions</h2>
<ol type="1">
<li>sparsity</li>
<li>cold start</li>
</ol>
<h2 id="kg-benefits">KG Benefits</h2>
<ol type="1">
<li>The rich semantic relatedness among items in a KG can help explore
their latent connections and improve the <em>precision</em> of
results;</li>
<li>The various types of relations in a KG are helpful for extending a
user’s interests reasonably and increasing the <em>diversity</em> of
recommended items;</li>
<li>KG connects a user’s historically-liked and recommended items,
thereby bringing <em>explainability</em> to recommender systems.</li>
</ol>
<h2 id="previous-kg-method">Previous KG Method</h2>
<h3 id="knowledge-graph-embedding">Knowledge graph embedding</h3>
<p>Example: TransE [1] and TransR [12] assume <em>head</em> +
<em>relation</em> = <em>tail</em>, which focus on modeling rigorous
semantic relatedness</p>
<p>Problem: KGE methods are more suitable for in-graph applications such
as KG completion and link prediction rather than the recommendation
system.</p>
<h3 id="path-base-method">Path-base Method</h3>
<p>Example: PER, FMG</p>
<p>problem: Labor sensitivity</p>
<h2 id="ripple-net">Ripple Net</h2>
<p>problem:</p>
<ol type="1">
<li>the importance of relations is weakly characterized in RippleNet,
because the relation <strong>R</strong> can hardly be trained to capture
the sense of importance in the quadratic form <strong>v</strong>
⊤<strong>Rh</strong> (<strong>v</strong> and <strong>h</strong> are
embedding vectors of two entities).</li>
<li>The size of ripple set may go unpredictably with the increase of the
size of KG, which incurs heavy computation and storage overhead.</li>
</ol>
<h2 id="solution-kgcn">Solution: KGCN</h2>
<ol type="1">
<li>Propagation and aggregation mechanism.</li>
<li>Attention mechanism.</li>
<li>sample a fixed-size neighborhood to control compute cost.</li>
</ol>
<h1 id="model">Model</h1>
<h2 id="single-layer">Single layer</h2>
<p>Consider a pair(u,v)</p>
<h3 id="overall-of-single-layer">Overall of single layer</h3>
<p>!!!Propagation only use for updating of item's vector</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114017500.png"
alt="image-20230302114017500" />
<figcaption aria-hidden="true">image-20230302114017500</figcaption>
</figure>
<h3 id="propagation">Propagation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302111400335.png"
alt="image-20230302111400335" />
<figcaption aria-hidden="true">image-20230302111400335</figcaption>
</figure>
<p><span class="math inline">\(N(v)\)</span> is the neighbor set of
v</p>
<p>e$ is the embedding of entity(parameter to train)</p>
<p><span class="math inline">\(\pi^u_{r_{v,e}}\)</span> is attention
weight</p>
<p><span class="math inline">\(r_{v,e}\)</span> represent the relation
of v and e</p>
<h3 id="attention">Attention</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302112401231.png"
alt="image-20230302112401231" />
<figcaption aria-hidden="true">image-20230302112401231</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302112315690.png"
alt="image-20230302112315690" />
<figcaption aria-hidden="true">image-20230302112315690</figcaption>
</figure>
<p><span class="math inline">\(g : R^d ×R^d → R\)</span> (e.g., inner
product:内积能计算相似度)to compute the score between a user and a
relation</p>
<p><span class="math inline">\(u\in R\)</span>, <span
class="math inline">\(r\in R\)</span> : embedding of user and
relation(parameter to train)</p>
<p><span class="math inline">\(\pi^u_{r_{v,e}}\)</span>characterizes the
importance of relation <em>r</em> to user <em>u</em>. E.g. example, a
user may have more interests in the movies that share the same “star"
with his historically liked ones, while another user may be more
concerned about the “genre" of movies.</p>
<p>!!!!!!!!!!个性化！！！用户对不同关系重视程度不同！！</p>
<p>所以KGCN不用propagation更新用户的原因是否是因为希望user的embedding能专注于提取个性化信息（提高用户和重要relation的相似度），但是这样是否会让user和item没那么好聚类？</p>
<h3 id="sample-the-number-of-neighbors">Sample the number of
neighbors</h3>
<p>limit the neighbor number in K(can be config)</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302113734945.png"
alt="image-20230302113734945" />
<figcaption aria-hidden="true">image-20230302113734945</figcaption>
</figure>
<p>S(<em>v</em>) is also called the (single layer) <em>receptive
field</em> of entity <em>v</em></p>
<p>example K=2:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302113907523.png"
alt="image-20230302113907523" />
<figcaption aria-hidden="true">image-20230302113907523</figcaption>
</figure>
<h3 id="aggregation">aggregation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114047762.png"
alt="image-20230302114047762" />
<figcaption aria-hidden="true">image-20230302114047762</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114057869.png"
alt="image-20230302114057869" />
<figcaption aria-hidden="true">image-20230302114057869</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114106975.png"
alt="image-20230302114106975" />
<figcaption aria-hidden="true">image-20230302114106975</figcaption>
</figure>
<p><span class="math inline">\(\sigma\)</span> is ReLU</p>
<h2 id="multi-layer">Multi layer</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/9C6A6E4346910DDDD43EBB55E1A2FE6C.png"
alt="9C6A6E4346910DDDD43EBB55E1A2FE6C" />
<figcaption
aria-hidden="true">9C6A6E4346910DDDD43EBB55E1A2FE6C</figcaption>
</figure>
<p>First we consider the Receptive-Field:</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/1739B546615C1AEDC69F7A3514E2E458.png" alt="1739B546615C1AEDC69F7A3514E2E458" style="zoom:67%;" /></p>
<p>We first update eneities in M[0] by using propagation and aggregation
to get the high-hop neighbor information.</p>
<p>And then gradually narrow it down, and finally focus on v.</p>
<p>Note that we have only one user in one pair, every relations will
calculate the score with this user embedding</p>
<h2 id="predict">Predict</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114437063.png"
alt="image-20230302114437063" />
<figcaption aria-hidden="true">image-20230302114437063</figcaption>
</figure>
<h2 id="loss-function">Loss function</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302120154981.png"
alt="image-20230302120154981" />
<figcaption aria-hidden="true">image-20230302120154981</figcaption>
</figure>
<p><span class="math inline">\(J\)</span> is cross-entropy loss</p>
<p><em>P</em> is a negative sampling distribution, and <span
class="math inline">\(T_u\)</span> is the number of negative samples for
user <em>u</em>. In this paper,</p>
<h1 id="experiment">Experiment</h1>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>LightGCN</title>
    <url>/2023/02/24/LightGCN/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>question: why concentrated to sum</p>
<h2 id="main-contributes">Main contributes</h2>
<ol type="1">
<li><p>We empirically show that two common designs in GCN, feature
transformation and nonlinear activation, have no positive effect on the
effectiveness of collaborative filtering.</p>
<p>GCN is originally proposed for node classification on the attributed
graph, where each node has rich attributes as input features; whereas in
the user-item interaction graph for CF, each node (user or item) is only
described by a one-hot ID, which has no concrete semantics besides being
an identifier.</p></li>
<li><p>Propose LightGCN.</p></li>
</ol>
<h1 id="analyze-about-ngcf">Analyze about NGCF</h1>
<h2 id="brief">Brief</h2>
<p>完全想不起来的话建议先看NGCF的笔记</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213192715250.png"
alt="image-20230213192715250" />
<figcaption aria-hidden="true">image-20230213192715250</figcaption>
</figure>
<p>## Some experiment</p>
<h3 id="method">Method</h3>
<p>Using ablation studies, implement three simplified variants of
NGCF:</p>
<ol type="1">
<li>NGCF-f: which removes the feature transformation matrices <span
class="math inline">\(W1\)</span> and <span
class="math inline">\(W2\)</span>.</li>
<li>NGCF-n: which removes the non-linear activation function $ σ$.</li>
<li>NGCF-fn: which removes both the feature transformation matrices and
non-linear activation function.</li>
</ol>
<p><strong>Note</strong>: Since the core of GCN is to refine embeddings
by propagation, we are more interested in the embedding quality under
the same embedding size. Thus, we change the way of obtaining final
embedding from concatenation (i.e., <span
class="math inline">\(e_u^*=e_u^{(0)}\|e_u^{(1)}\|...\|e_u^{(L)}\)</span>)
to sum(i.e., <span
class="math inline">\(e_u^*=e_u^{(0)}+e_u^{(1)}+...+e_u^{(L)}\)</span>).</p>
<p>This change has little effect on NGCF’s performance but makes the
following ablation studies more indicative of the embedding quality
refined by GCN.</p>
<h3 id="result">Result</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213193937873.png"
alt="image-20230213193937873" />
<figcaption aria-hidden="true">image-20230213193937873</figcaption>
</figure>
<ol type="1">
<li>Adding feature transformation imposes negative effect on NGCF, since
removing it in both models of NGCF and NGCF-n improves the performance
significantly;</li>
<li>Adding nonlinear activation affects slightly when feature
transformation is included, but it imposes negative effect when feature
transformation is disabled.</li>
<li>As a whole, feature transformation and nonlinear activation impose
rather negative effect on NGCF, since by removing them simultaneously,
NGCF-fn demonstrates large improvements over NGCF.</li>
</ol>
<h3 id="conclusion">Conclusion</h3>
<p>The deterioration of NGCF stems from the training
difficulty(underfitting), rather than overfitting, because:</p>
<ol type="1">
<li><p>Such lower training loss of NGCF-fn successfully transfers to
better recommendation accuracy.</p></li>
<li><p>NGCF is more powerful and complex, but it demonstrates higher
training loss and worse generalization performance than NGCF-f.</p></li>
</ol>
<h1 id="model-of-lightgcn">Model of LightGCN</h1>
<p>Consisting four parts:</p>
<ol type="1">
<li>initialize users and items embedding.</li>
<li>Light Graph Convolution (LGC)</li>
<li>Layer Combination</li>
<li>Model Prediction</li>
</ol>
<h2 id="light-graph-convolution-lgc">Light Graph Convolution (LGC)</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213194939594.png"
alt="image-20230213194939594" />
<figcaption aria-hidden="true">image-20230213194939594</figcaption>
</figure>
<p>$ $： symmetric normalization, which can avoid the scale of
embeddings increasing with graph convolution operations. Here can use
other normalization, but symmetric normalization has good
performance.</p>
<p><strong>Note</strong>: Without self-connection, because the layer
combination operation of LightGCN captures the same effect as
self-connections.</p>
<h2 id="layer-combination">Layer Combination</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213195607414.png"
alt="image-20230213195607414" />
<figcaption aria-hidden="true">image-20230213195607414</figcaption>
</figure>
<p><span class="math inline">\(α_k\)</span>can be treated as a
hyperparameter to be tuned manually, or as a model parameter, and
setting <span class="math inline">\(α_k\)</span> uniformly as <span
class="math inline">\(1/(K + 1)\)</span> leads to good performance in
general.</p>
<p>This is probably because the training data does not contain
sufficient signal to learn good α that can generalize to unknown
data.</p>
<p>The reason of using the Layer Combination:</p>
<ol type="1">
<li>With the increasing of the number of layers, the embeddings will be
over-smoothed [27]. Thus simply using the last layer is
problematic.</li>
<li>The embeddings at different layers capture different semantics.</li>
<li>Combining embeddings at different layers with weighted sum captures
the effect of graph convolution with self-connections, an important
trick in GCNs.</li>
</ol>
<h2 id="model-prediction">Model Prediction</h2>
<p>inner product</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213200915280.png"
alt="image-20230213200915280" />
<figcaption aria-hidden="true">image-20230213200915280</figcaption>
</figure>
<h2 id="matrix-form">Matrix form</h2>
<p>Similar to NGCF, and there are some explanations in detail in NGCF
note.</p>
<p>Light Graph Convolution:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201201896.png"
alt="image-20230213201201896" />
<figcaption aria-hidden="true">image-20230213201201896</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201107785.png"
alt="image-20230213201107785" />
<figcaption aria-hidden="true">image-20230213201107785</figcaption>
</figure>
<p>Layer combination:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201221159.png"
alt="image-20230213201221159" />
<figcaption aria-hidden="true">image-20230213201221159</figcaption>
</figure>
<h1 id="analyze-about-lightgcn">Analyze about LightGCN</h1>
<h2 id="relation-with-sgcn">Relation with SGCN</h2>
<p><strong>Purpose</strong>: by doing layer combination, LightGCN
subsumes the effect of self-connection thus there is no need for
LightGCN to add self-connection in adjacency matrix.</p>
<p>SGCN: a recent linear GCN model that integrates self-connection into
graph convolution.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201725744.png"
alt="image-20230213201725744" />
<figcaption aria-hidden="true">image-20230213201725744</figcaption>
</figure>
<p>In the following analysis, we omit the <span class="math inline">\((D
+ I)^{-\frac{1}{2}}\)</span> terms for simplicity, since they only
re-scale embeddings.</p>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230213212117430.png"
alt="image-20230213212117430" />
<figcaption aria-hidden="true">image-20230213212117430</figcaption>
</figure>
<p>The above derivation shows that, inserting self-connection into A and
propagating embeddings on it, is essentially equivalent to a weighted
sum of the embeddings propagated at each LGC layer.</p>
<p>because <span class="math inline">\(AE^{(0)}=E^{(1)}\)</span>...<span
class="math inline">\(A^KE^{(0)}=E^{(K)}\)</span></p>
<h2 id="relation-with-appnp">Relation with APPNP</h2>
<p><strong>Purpose</strong>: shows the underlying equivalence between
LightGCN and APPNP, thus our LightGCN enjoys the sames benefits in
propagating long-range with controllable overs-moothing.</p>
<p>APPNP: a recent GCN variant that addresses over-smoothing. APPNP
complements each propagation layer with the starting features.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213212642134.png"
alt="image-20230213212642134" />
<figcaption aria-hidden="true">image-20230213212642134</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213212845456.png"
alt="image-20230213212845456" />
<figcaption aria-hidden="true">image-20230213212845456</figcaption>
</figure>
<p>also equivalent to a weighted sum of the embeddings propagated at
each LGC layer.</p>
<h2 id="second-order-embedding-smoothness">Second-Order Embedding
Smoothness</h2>
<p><strong>Purpose</strong>: providing more insights into the working
mechanism of LightGCN.</p>
<p>below is influence from2-order neighbor to target node.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213213500081.png"
alt="image-20230213213500081" />
<figcaption aria-hidden="true">image-20230213213500081</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213213521206.png"
alt="image-20230213213521206" />
<figcaption aria-hidden="true">image-20230213213521206</figcaption>
</figure>
<p><strong>conclusion</strong>: the influence of a second-order neighbor
v on u is determined by</p>
<ol type="1">
<li>the number of co-interacted items, the more the larger.</li>
<li>the popularity of the co-interacted items, the less popularity
(i.e., more indicative of user personalized preference) the larger</li>
<li>the activity of v, the less active the larger.</li>
</ol>
<h1 id="model-train">Model Train</h1>
<h2 id="loss-function">Loss function</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213213949666.png"
alt="image-20230213213949666" />
<figcaption aria-hidden="true">image-20230213213949666</figcaption>
</figure>
<h2 id="optimizer-adam">Optimizer: Adam</h2>
<h2 id="no-dropout-strategy">No dropout strategy</h2>
<p>The reason is that we do not have feature transformation weight
matrices in LightGCN, thus enforcing L2 regularization on the embedding
layer is sufficient to prevent overfitting.</p>
<h1 id="experiment">Experiment</h1>
<h2 id="compared-with-ngcf">compared with NGCF</h2>
<ol type="1">
<li>LightGCN performs better than NGCF and NGCF-fn, as NGCF-fn still
contains more useless operations than LightGCN.</li>
<li>Increasing the number of layers can improve performance, but the
benefits diminish. Increasing the layer number from 0 to 1 leads to the
largest performance gain, and using a layer number of 3 leads to
satisfactory performance in most cases.</li>
<li>LightGCN consistently obtains lower training loss, which indicates
that LightGCN fits the training data better than NGCF. Moreover, the
lower training loss successfully transfers to better testing accuracy,
indicating the strong generalization power of LightGCN. In contrast, the
higher training loss and lower testing accuracy of NGCF reflect the
practical difficulty to train such a heavy model it well.</li>
</ol>
<h2 id="ablation-and-effectiveness-analyses">Ablation and Effectiveness
Analyses</h2>
<h3 id="impact-of-layer-combination">Impact of Layer Combination</h3>
<h4 id="using-models">Using models:</h4>
<ol type="1">
<li>LightGCN</li>
<li>LightGCN-single: does not use layer combination</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215151920378.png"
alt="image-20230215151920378" />
<figcaption aria-hidden="true">image-20230215151920378</figcaption>
</figure>
<h4 id="conclusion-1">Conclusion</h4>
<ol type="1">
<li>Focusing on LightGCN-single, we find that its performance first
improves and then drops when the layer number increases from 1 to 4.
This indicates that smoothing a node’s embedding with its first-order
and secondorder neighbors is very useful for CF, but will suffer from
oversmoothing issues when higher-order neighbors are used.</li>
<li>Focusing on LightGCN, we find that its performance gradually
improves with the increasing of layers even using 4 layers. This
justifies the effectiveness of layer combination for addressing
over-smoothing.</li>
<li>we find that LightGCN consistently outperforms LightGCN-single on
Gowalla, but not on AmazonBook and Yelp2018. There are two reason:
<ol type="1">
<li>LightGCN-single is special case of LightGCN that sets αK to 1 and
other αk to 0;</li>
<li>we do not tune the <span class="math inline">\(αk\)</span> and
simply set it as <span class="math inline">\(\frac{1}{K+1}\)</span>
uniformly for LightGCN.</li>
</ol></li>
</ol>
<h3 id="impact-of-symmetric-sqrt-normalization">Impact of Symmetric Sqrt
Normalization</h3>
<h4 id="setting">Setting:</h4>
<ol type="1">
<li>LightGCN-L: normalization only at the left side (i.e., the target
node’s coefficient).</li>
<li>LightGCN-R: the right side (i.e., the neighbor node’s
coefficient).</li>
<li>LightGCN-L1: use L1 normalization( i.e., removing the square
root).</li>
<li>LightGCN-L1-L: use L1 normalization only on the left side.</li>
<li>LightGCN-L1-R: use L1 normalization only on the right side.</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215153734701.png"
alt="image-20230215153734701" />
<figcaption aria-hidden="true">image-20230215153734701</figcaption>
</figure>
<h4 id="conclusion-2">Conclusion</h4>
<ol type="1">
<li>The best setting in general is using sqrt normalization at both
sides (i.e., the current design of LightGCN). Removing either side will
drop the performance largely.</li>
<li>The second best setting is using L1 normalization at the left side
only (i.e., LightGCN-L1-L). This is equivalent to normalize the
adjacency matrix as a stochastic matrix by the
in-degree(norm后矩阵无对称性).</li>
<li>Normalizing symmetrically on two sides is helpful for the sqrt
normalization, but will degrade the performance of L1
normalization.</li>
</ol>
<h3 id="analysis-of-embedding-smoothness">Analysis of Embedding
Smoothness</h3>
<p><strong>Object</strong>: Making sure such
smoothing（有点像聚类的感觉） of embeddings is the key reason of
LightGCN’s effectiveness.</p>
<p><strong>Method</strong>: we first define the smoothness of user
embeddings as(用于衡量2-order
neighbor的embedding差别大小，是否合理聚类的感觉):</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215160123446.png"
alt="image-20230215160123446" />
<figcaption aria-hidden="true">image-20230215160123446</figcaption>
</figure>
<p>where the L2 norm on embeddings is used to eliminate the impact of
the embedding’s scale.</p>
<p><strong>result</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215160328103.png"
alt="image-20230215160328103" />
<figcaption aria-hidden="true">image-20230215160328103</figcaption>
</figure>
<p><strong>Conclusion</strong>: the smoothness loss of LightGCN-single
is much lower than that of MF.</p>
<p>This indicates that by conducting light graph convolution, the
embeddings become smoother and more suitable for recommendation.</p>
<h2 id="hyper-parameter-studies">Hyper-parameter Studies</h2>
<p><strong>object</strong>: Ensure the L2 regularization coefficient
<span class="math inline">\(λ\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215161009450.png"
alt="image-20230215161009450" />
<figcaption aria-hidden="true">image-20230215161009450</figcaption>
</figure>
<p><strong>Conclusion</strong>:</p>
<ol type="1">
<li>LightGCN is relatively insensitive to λ.</li>
<li>Even when λ sets to 0, LightGCN is better than NGCF, which
additionally uses dropout to prevent overfitting. This shows that
LightGCN is less prone to overfitting</li>
<li>When λ is larger than 1e−3, the performance drops quickly, which
indicates that too strong regularization will negatively affect model
normal training and is not encouraged.</li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title>NGCF</title>
    <url>/2023/02/24/NGCF/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>question: example(the Laplacian)</p>
<h2 id="some-definition">Some Definition</h2>
<ol type="1">
<li><p>Recommendation system: Estimate how likely a user will adopt an
item based on the historical interaction like purchase and
click.</p></li>
<li><p>Collaborative filtering(CF): behaviorally similar users would
exhibit similar preference on items.</p>
<p>CF consists of</p>
<ol type="1">
<li><p>embedding: transforms users and items into vectorized
representations. e.g. matrix factorization(MF),deep learning
function...</p></li>
<li><p>interaction modeling: reconstructs historical interactions based
on the embeddings. e.g. inner product, neural function...</p></li>
</ol></li>
<li><p>collaborative signal: signal latent in user-item
interactions</p></li>
</ol>
<h2 id="existing-problem">Existing Problem</h2>
<p>The current embedding process of CF doesn't encode a collaborative
signal. Most of them focus on the descriptive feature(e.g. user id,
attributes). When the embeddings are insufficient in capturing CF, the
methods have to rely on the interaction function to make up for the
deficiency of suboptimal embeddings</p>
<h2 id="main-contribute">Main contribute</h2>
<ol type="1">
<li><p>Highlight the critical importance of explicitly exploiting the
collaborative signal in the embedding function of model-based CF
methods.</p></li>
<li><p>Propose NGCF, a new recommendation framework based on a graph
neural network, which explicitly encodes the collaborative signal in the
form of high-order connectivities by performing embedding
propagation.</p></li>
</ol>
<h1 id="model">Model</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211111222966.png"
alt="image-20230211111222966" />
<figcaption aria-hidden="true">image-20230211111222966</figcaption>
</figure>
<p>There are three components in the framework:</p>
<ol type="1">
<li>Embedding layer: offers and initialization of user embeddings and
item embeddings;</li>
<li>Multiple embedding propagation layers: refine the embeddings by
injecting high-order connectivity relations;</li>
<li>Prediction layer: aggregates the refined embeddings from different
propagation layers and outputs the affinity score of a user-item
pair.</li>
</ol>
<h2 id="embedding-layer">Embedding layer</h2>
<p>Just initializing user embeddings and item embeddings by using ID or
other features.</p>
<p>Get user embedding <span class="math inline">\(e_i\)</span> and item
embedding <span class="math inline">\(e_u\)</span>.</p>
<h2 id="multiple-embedding-propagation-layers">Multiple Embedding
Propagation Layers</h2>
<h3 id="one-layer-propagation">One layer propagation</h3>
<p>It consists of two parts: Message Construction and Message
aggregation.</p>
<h4 id="message-construction">Message Construction</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211112521161.png"
alt="image-20230211112521161" />
<figcaption aria-hidden="true">image-20230211112521161</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211111736136.png"
alt="image-20230211111736136" />
<figcaption aria-hidden="true">image-20230211111736136</figcaption>
</figure>
<p><span class="math inline">\(m_{u&lt;-i}\)</span>: the result of the
message construction module. It is a message embedding that will be used
to update the target node.</p>
<p><span class="math inline">\(e_i\)</span>: Embedding of neighbor
item.</p>
<p><strong>meaning</strong> : encode neighbor item's feature.</p>
<p><span class="math inline">\(e_i⊙e_u\)</span> : element-wise product
of <span class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_u\)</span>.</p>
<p><strong>meaning</strong>: encodes the interaction between <span
class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_u\)</span> into the message and makes the
message dependent on the affinity between <span
class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_j\)</span>.</p>
<p><span class="math inline">\(W_1\)</span>, <span
class="math inline">\(W_2\)</span>: trainable weight matrices， the
shape is (<span class="math inline">\(d&#39;\)</span>, <span
class="math inline">\(d\)</span>), while <span
class="math inline">\(d\)</span> is the size of the initial embedding,
<span class="math inline">\(d&#39;\)</span> is the size of
transformation size.</p>
<p><span class="math inline">\(P_{ui}\)</span>: to control the decay
factor on each propagation on edge (u, i). Here, we set <span
class="math inline">\(P_{ui}\)</span> as <strong>Laplacian norm</strong>
$ $, $ N_u$, $ N_i$ is the first-hot neighbors of user u and item i.
(就是拉普拉斯矩阵归一化！！<span
class="math inline">\(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\)</span>)</p>
<p><strong>meaning</strong> -From the viewpoint of representation
learning: <span class="math inline">\(P_{ui}\)</span> reflects how much
the historical item contributes to the user preference.</p>
<p>From the viewpoint of the message passing: <span
class="math inline">\(P_{ui}\)</span> can be interpreted as a discount
factor, considering the messages being propagated should decay with the
path length.</p>
<h4 id="message-aggregation">Message Aggregation</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211151741633.png"
alt="image-20230211151741633" />
<figcaption aria-hidden="true">image-20230211151741633</figcaption>
</figure>
<p><span class="math inline">\(e_u^{(1)}\)</span>: the representation of
user u after 1 propagation layer.</p>
<p><span class="math inline">\(m_{u&lt;-u}\)</span>: self-connection of
u. Here is <span class="math inline">\(W1e_u\)</span>.</p>
<p><strong>meaning</strong>: retain information of original feature.</p>
<p><span class="math inline">\(m_{u&lt;-i}\)</span>： neighbor node
propagation.</p>
<h3 id="high-order-propagation">High-order propagation</h3>
<h4 id="formulate-form">Formulate Form</h4>
<p>By stacking l-embedding propagation layers, a user (and an item) is
capable of receiving the messages propagated from its l-hop neighbors.
The formulates are similar to one-layer propagation.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212105956664.png"
alt="image-20230212105956664" />
<figcaption aria-hidden="true">image-20230212105956664</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212110019741.png"
alt="image-20230212110019741" />
<figcaption aria-hidden="true">image-20230212110019741</figcaption>
</figure>
<h4 id="matrix-form">Matrix Form</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212110725475.png"
alt="image-20230212110725475" />
<figcaption aria-hidden="true">image-20230212110725475</figcaption>
</figure>
<p><span class="math inline">\(E^{(l)}\)</span> : the representations
for users and items obtained after l-layers propagation. Shape is
(N+M,d)</p>
<p>L: Laplacian matrix for the user-item graph.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212111719667.png"
alt="image-20230212111719667" />
<figcaption aria-hidden="true">image-20230212111719667</figcaption>
</figure>
<p>D is the diagonal degree matrix. where <span
class="math inline">\(D_{tt}=\vert N_t\vert\)</span> meaning the
<code>D[t][t]</code> is the number of neighbors' node. The shape is
(N+M, N+M), because there are totally n+m node(including user and
item)</p>
<p>A is the adjacency matrix. The shape of R is (N, M), while the shape
of A is (N+M, N+M).</p>
<p>some extra knowledge: <a
href="https://zhuanlan.zhihu.com/p/362416124/">理解拉普拉斯矩阵</a></p>
<p>I: identity matrix</p>
<h5 id="a-simple-example-for-matrix-form">A simple example for matrix
form:</h5>
<p>Suppose we have 2 users (A, B), 3 items(C, D, E), N=2 and M=3.</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/D9B00E7DDF74FF18B83E42668335328A.png" alt="D9B00E7DDF74FF18B83E42668335328A" style="zoom: 25%;" /></p>
<p>Let consider this part: <span
class="math inline">\((L+I)E^{(l-1)}W^{(l)}\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/81DBE0096BF060771E3355F2E6A34151.png"
alt="81DBE0096BF060771E3355F2E6A34151" />
<figcaption
aria-hidden="true">81DBE0096BF060771E3355F2E6A34151</figcaption>
</figure>
<p>After calculating <span
class="math inline">\((L+I)E^{(l-1)}\)</span>, we get information on
self-connection and neighbor-propagation (after the Laplacian norm), and
then we can multiply the trainable parameter W1(MLP).</p>
<p>拉普拉斯矩阵归一化的不成熟小理解：</p>
<p>①target node由n个邻居点做贡献，为了避免邻居越多，target
node的value越大的情况，首先除<span
class="math inline">\(\frac{1}{\sqrt{N_n}}\)</span>,
大概也可以理解为邻居越多，每个邻居对其造成的影响越小</p>
<p>②只做一次norm影响对称性，所以为了保持对称性在做一次<span
class="math inline">\(\frac{1}{\sqrt{N_t}}\)</span>,可以理解为neighbor
node有多少邻居对他给到每个邻居的权重有影响，是否能理解为邻居越多说明这个node能提供的信息更普通没价值（例如所有用户购买了水，对推荐系统来说，水能提供的信息就没那么有用）</p>
<p>x class UV_Aggregator(nn.Module):    """   item and user aggregator:
for aggregating embeddings of neighbors (item/user aggreagator).   """​  
 def <strong>init</strong>(self, v2e, r2e, u2e, embed_dim, cuda="cpu",
uv=True):        ...​    def forward(self, nodes, history_uv, history_r):
       # create a container for result, shpe of embed_matrix is
(batchsize,embed_dim)        embed_matrix = torch.empty(len(history_uv),
self.embed_dim, dtype=torch.float).to(self.device)​        # deal with
each single item nodes' neighbors        for i in
range(len(history_uv)):            history = history_uv[i]          
 num_histroy_item = len(history)            tmp_label = history_r[i]​    
       # e_uv : turn neighbors(user node) id to embedding            #
uv_rep : turn single node(item node) to embedding            if self.uv
== True:                # user component                e_uv =
self.v2e.weight[history]                uv_rep =
self.u2e.weight[nodes[i]]            else:                # item
component                e_uv = self.u2e.weight[history]              
 uv_rep = self.v2e.weight[nodes[i]]​            # get rating score
embedding            e_r = self.r2e.weight[tmp_label]            #
concatenated rating and neighbor, and than through two layers mlp to get
fjt            x = torch.cat((e_uv, e_r), 1)            x =
F.relu(self.w_r1(x))​            o_history = F.relu(self.w_r2(x))        
   # calculate neighbor attention and fjt*weight to finish aggregation  
         att_w = self.att(o_history, uv_rep, num_histroy_item)          
 att_history = torch.mm(o_history.t(), att_w)            att_history =
att_history.t()​            embed_matrix[i] = att_history        # result
(batchsize, embed_dim)        to_feats = embed_matrix        return
to_featspython</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/8EE43A6D961CA0F0145CD44C62B9F9BE.png"
alt="8EE43A6D961CA0F0145CD44C62B9F9BE" />
<figcaption
aria-hidden="true">8EE43A6D961CA0F0145CD44C62B9F9BE</figcaption>
</figure>
<p>We get information on the interaction between <span
class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_u\)</span> (after the Laplacian norm), and then
we can multiply the trainable parameter W2(MLP).</p>
<p>Add two parts and through LeakyRelu, we get user or item embedding
after l-layers propagation.</p>
<h2 id="model-prediction">Model Prediction</h2>
<p>Just concatenate all propagation layers' output embedding, and use
inner product to estimate the user's preference towards the target
item.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212173756733.png"
alt="image-20230212173756733" />
<figcaption aria-hidden="true">image-20230212173756733</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212173813003.png"
alt="image-20230212173813003" />
<figcaption aria-hidden="true">image-20230212173813003</figcaption>
</figure>
<h1 id="optimization">Optimization</h1>
<h2 id="loss">Loss</h2>
<p>BPR Loss: assumes that the observed interactions, which are more
reflective of a user’s preferences, should be assigned higher prediction
values than unobserved ones.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212212248890.png"
alt="image-20230212212248890" />
<figcaption aria-hidden="true">image-20230212212248890</figcaption>
</figure>
<h2 id="optimizer-adam">Optimizer: Adam</h2>
<h2 id="model-size">Model Size</h2>
<p>In NGCF, only W1 and W2 in the propagation layer need to be trained,
so has <span class="math inline">\(2Ld_ld_{l-1}\)</span> more
parameters, while L is always smaller than 5 and <span
class="math inline">\(d\)</span> is set as the embedding size(e.g. 64)
which is also small.</p>
<h2 id="message-and-node-dropout">Message and Node Dropout</h2>
<ol type="1">
<li><p><strong>Message dropout</strong>: randomly drops out the outgoing
messages (equal to dropout edge).</p>
<p><strong>meaning</strong>: endows the representations more robustness
against the presence or absence of single connections between users and
items.</p>
<p><strong>example</strong>: For the <span
class="math inline">\(l-th\)</span> propagation layer, we drop out the
messages being propagated, with a probability <span
class="math inline">\(p1\)</span>.</p></li>
<li><p><strong>Node dropout</strong>: randomly blocks a particular node
and discards all its outgoing messages.</p>
<p><strong>meaning</strong>: focuses on reducing the influences of
particular users or items.</p>
<p><strong>example</strong>: For the <span
class="math inline">\(l-th\)</span> propagation layer, we randomly drop
<span class="math inline">\((M + N)p2\)</span> nodes of the Laplacian
matrix, where <span class="math inline">\(p2\)</span> is the dropout
ratio.</p></li>
</ol>
<p>区别：对于message
dropout，计算时node的邻居数、拉普拉斯norm都是正常的，就是更新embedding的时候遗漏了信息，作用是提高一下鲁棒性和容错性；对于Node
dropout，直接在拉普拉斯矩阵中屏蔽若干个node，可能影响临界点数、归一化数值等，在矩阵运算时候就有影响，作用是希望模型不要过于依赖某些特定邻接点，没了部分点依然能正常运行。</p>
<h1 id="experiment">Experiment</h1>
<h2 id="conclusions-from-comparing-with-other-models">Conclusions from
comparing with other models</h2>
<ol type="1">
<li>The inner product is insufficient to capture the complex relations
between users and items.</li>
<li>Nonlinear feature interactions between users and items are
important</li>
<li>Neighbor information can improve embedding learning, and using the
attention mechanism is better than using equal and heuristic
weight.</li>
<li>Considering high-order connectivity or neighbor is better than only
considering first-order neighbor.</li>
<li>that exploiting high-order connectivity greatly facilitates
representation learning for inactive users, as the collaborative signal
can be effectively captured. And the embedding propagation is beneficial
to relatively inactive users.</li>
</ol>
<h2 id="study-for-ngcf">Study for NGCF</h2>
<p>....</p>
<h2 id="effect-of-high-order-connectivity">Effect of High-order
Connectivity</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212225247958.png"
alt="image-20230212225247958" />
<figcaption aria-hidden="true">image-20230212225247958</figcaption>
</figure>
<ol type="1">
<li>the representations of NGCF-3 exhibit discernible clustering,
meaning that the points with the same colors (<em>i.e.,</em> the items
consumed by the same users) tend to form the clusters.</li>
<li>when stacking three embedding propagation layers, the embeddings of
their historical items tend to be closer. It qualitatively verifies that
the proposed embedding propagation layer is capable of injecting the
explicit collaborative signal (via NGCF-3) into the
representations.</li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title>GraphRec</title>
    <url>/2023/02/24/Note_for_GraphRec/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="graphrec">GraphRec</h1>
<h1 id="graphrec-feature">GraphRec feature</h1>
<ol type="1">
<li><p>Can capture both interactions and opinions in user-item
graph.</p></li>
<li><p>Consider different strengths of social relations.</p></li>
<li><p>Use attention mechanism.</p></li>
</ol>
<h1 id="overall-architecture">Overall architecture</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/Snipaste_2023-02-02_15-10-34.png"
alt="Snipaste_2023-02-02_15-10-34" />
<figcaption aria-hidden="true">Snipaste_2023-02-02_15-10-34</figcaption>
</figure>
<h2 id="three-import-module">Three import module:</h2>
<ol type="1">
<li><p>User Modeling: used to compute User Latent Factor(vector
containing many useful information)</p></li>
<li><p>Item Modeling: used to compute Item Latent Factor.</p></li>
<li><p>Rating Prediction: used to predict the item which user would like
to interact with.</p></li>
</ol>
<h1 id="source-code-analyses">Source code analyses</h1>
<h2 id="data">Data</h2>
<h3 id="what-kind-of-datas-we-use"><strong>What kind of datas we
use?</strong></h3>
<ol type="1">
<li><p>User-Item graph: record interation(e.g. purchase) and
opinion(e.g. five star rating) between user and item</p></li>
<li><p>User-User social graph: relationship between user and
user</p></li>
</ol>
<h3 id="how-to-represent-these-datas-in-code"><strong>How to represent
these datas in code?</strong></h3>
<h4 id="user-item-graph">User-Item graph:</h4>
<ol type="1">
<li>history_u_lists, history_ur_lists: user's purchased history (item
set in training set), and his/her rating score (dict)</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history_u_list = &#123;</span><br><span class="line">    user_id1:[item_id1, item_id2, item_id3...],</span><br><span class="line">    user_id2:[item_id4...],</span><br><span class="line">&#125;</span><br><span class="line">history_ur_list = &#123;</span><br><span class="line">    user_id1:[rating_score_u1i1, rating_score_u1i2, rating_score_u1i3...],</span><br><span class="line">    user_id2:[rating_score_u2i4...],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">e.g.</span><br><span class="line">history_u_list = &#123;</span><br><span class="line">    <span class="number">681</span>: [<span class="number">0</span>, <span class="number">156</span>], </span><br><span class="line">    <span class="number">81</span>: [<span class="number">1</span>, <span class="number">41</span>, <span class="number">90</span>]&#125;</span><br><span class="line">history_ur_list = &#123;</span><br><span class="line">    <span class="number">681</span>: [<span class="number">5</span>,<span class="number">4</span>],</span><br><span class="line">    <span class="number">81</span>: [<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>]&#125;</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>history_v_lists, history_vr_lists: user set (in training set) who
have interacted with the item, and rating score (dict). Similar with
history_u_lists, history_ur_lists but key is item id and value is user
id.</li>
</ol>
<h4 id="user-user-socal-graph">User-User socal graph</h4>
<ol type="1">
<li>social_adj_lists: user's connected neighborhoods</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">social_adj_lists = &#123;</span><br><span class="line">    user_id1:[user_id2, user_id3, user_id4...],</span><br><span class="line">    user_id2:[user_id1...],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="other">other</h4>
<ol type="1">
<li>train_u, train_v, train_r: used for model training, one by one based
on index (user, item, rating)</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_u = [user_id1, user_id2,....]</span><br><span class="line">train_v = [item_id34, item_id1,...]</span><br><span class="line">train_r = [rating_socre_u1i34, rating_socre_u2i1]</span><br><span class="line"><span class="built_in">len</span>(train_u) = <span class="built_in">len</span>(train_v) = <span class="built_in">len</span>(train_r)</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li><p>test_u, test_v, test_r: similar with training datas</p></li>
<li><p>ratings_list: rating value from 0.5 to 4.0 (8 opinion embeddings)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;2.0: 0, 1.0: 1, 3.0: 2, 4.0: 3, 2.5: 4, 3.5: 5, 1.5: 6, 0.5: 7&#125;</span><br></pre></td></tr></table></figure></p></li>
</ol>
<h3 id="how-to-pre-process-data"><strong>How to pre-process
data?</strong></h3>
<p>use <code>torch.utils.data.TensorDataset</code> and
<code>torch.utils.data.DataLoader</code> generate
<code>training_dataset</code> and <code>testing_dataset</code> (user,
item, rating)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">support batchsize = <span class="number">5</span></span><br><span class="line">[tensor([<span class="number">409</span>,  <span class="number">88</span>, <span class="number">134</span>, <span class="number">298</span>, <span class="number">340</span>]),                             <span class="comment">#user id</span></span><br><span class="line">tensor([<span class="number">1221</span>,  <span class="number">761</span>,   <span class="number">39</span>,  <span class="number">145</span>,    <span class="number">0</span>]),                         <span class="comment">#item id</span></span><br><span class="line">tensor([<span class="number">1.0000</span>, <span class="number">2.0000</span>, <span class="number">3.5000</span>, <span class="number">0.5000</span>, <span class="number">1.5000</span>, <span class="number">3.5000</span>])        <span class="comment">#rating score</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h2 id="model">Model</h2>
<h3 id="init">Init</h3>
<p>Translate user_id, item_id and rating_id to low-dimension vector,
just random initize, the weight of embedding layers will be trained.</p>
<p>After translate we get</p>
<pre><code>qj-embedding of item vj, 
pi-embedding of user ui, 
er-embedding of rating.</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">u2e = nn.Embedding(num_users, embed_dim).to(device)</span><br><span class="line">v2e = nn.Embedding(num_items, embed_dim).to(device)</span><br><span class="line">r2e = nn.Embedding(num_ratings, embed_dim).to(device)</span><br><span class="line"><span class="built_in">print</span>(u2e, v2e, r2e)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;Output</span></span><br><span class="line"><span class="string">Embedding(705, 64) </span></span><br><span class="line"><span class="string">Embedding(1941, 64) </span></span><br><span class="line"><span class="string">Embedding(8, 64)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>So that, we can easily get embedding through U2e, V2e and r2e.</p>
<h3 id="overall-architecture-1">Overall architecture</h3>
<figure>
<img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/GraphRec.jpg"
alt="GraphRec" />
<figcaption aria-hidden="true">GraphRec</figcaption>
</figure>
<p>GraphRec consist of User Modeling, Item Modeling and Rating
Prediction. The forward code of GraphRec is as follow:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GraphRec</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_u, enc_v_history, r2e</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes_u, nodes_v</span>):</span><br><span class="line">        <span class="comment"># nodes_u : [128] 128(batchsize) user id</span></span><br><span class="line">        <span class="comment"># nodes_v : [128] 128(batchsize) item id</span></span><br><span class="line">        <span class="comment"># self.enc_u is the User Modeling part(including Item Aggregation and Social Aggregation )</span></span><br><span class="line">        <span class="comment"># self.enc_v_history is the Item Modeling part(User Aggregation)</span></span><br><span class="line">        embeds_u = self.enc_u(nodes_u)</span><br><span class="line">        embeds_v = self.enc_v_history(nodes_v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># After aggregation information, forward two layer MLP， and get the Latent vector of user and item</span></span><br><span class="line">        x_u = F.relu(self.bn1(self.w_ur1(embeds_u)))</span><br><span class="line">        x_u = F.dropout(x_u, training=self.training)</span><br><span class="line">        x_u = self.w_ur2(x_u)</span><br><span class="line">        x_v = F.relu(self.bn2(self.w_vr1(embeds_v)))</span><br><span class="line">        x_v = F.dropout(x_v, training=self.training)</span><br><span class="line">        x_v = self.w_vr2(x_v)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># concatenated user vector and item vector, use three layer MLP to predict</span></span><br><span class="line">        x_uv = torch.cat((x_u, x_v), <span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.bn3(self.w_uv1(x_uv)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = F.relu(self.bn4(self.w_uv2(x)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        scores = self.w_uv3(x)</span><br><span class="line">        <span class="keyword">return</span> scores.squeeze()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, nodes_u, nodes_v, labels_list</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>full code of GraphRec class</p>
<details>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GraphRec</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_u, enc_v_history, r2e</span>):</span><br><span class="line">        <span class="built_in">super</span>(GraphRec, self).__init__()</span><br><span class="line">        self.enc_u = enc_u</span><br><span class="line">        self.enc_v_history = enc_v_history</span><br><span class="line">        self.embed_dim = enc_u.embed_dim</span><br><span class="line"></span><br><span class="line">        self.w_ur1 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_ur2 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_vr1 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_vr2 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_uv1 = nn.Linear(self.embed_dim * <span class="number">2</span>, self.embed_dim)</span><br><span class="line">        self.w_uv2 = nn.Linear(self.embed_dim, <span class="number">16</span>)</span><br><span class="line">        self.w_uv3 = nn.Linear(<span class="number">16</span>, <span class="number">1</span>)</span><br><span class="line">        self.r2e = r2e</span><br><span class="line">        self.bn1 = nn.BatchNorm1d(self.embed_dim, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(self.embed_dim, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(self.embed_dim, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.bn4 = nn.BatchNorm1d(<span class="number">16</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes_u, nodes_v</span>):</span><br><span class="line">        embeds_u = self.enc_u(nodes_u)</span><br><span class="line">        embeds_v = self.enc_v_history(nodes_v)</span><br><span class="line"></span><br><span class="line">        x_u = F.relu(self.bn1(self.w_ur1(embeds_u)))</span><br><span class="line">        x_u = F.dropout(x_u, training=self.training)</span><br><span class="line">        x_u = self.w_ur2(x_u)</span><br><span class="line">        x_v = F.relu(self.bn2(self.w_vr1(embeds_v)))</span><br><span class="line">        x_v = F.dropout(x_v, training=self.training)</span><br><span class="line">        x_v = self.w_vr2(x_v)</span><br><span class="line"></span><br><span class="line">        x_uv = torch.cat((x_u, x_v), <span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.bn3(self.w_uv1(x_uv)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = F.relu(self.bn4(self.w_uv2(x)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        scores = self.w_uv3(x)</span><br><span class="line">        <span class="keyword">return</span> scores.squeeze()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, nodes_u, nodes_v, labels_list</span>):</span><br><span class="line">        scores = self.forward(nodes_u, nodes_v)</span><br><span class="line">        <span class="keyword">return</span> self.criterion(scores, labels_list)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</details>
<h3 id="user-modeling">User Modeling</h3>
<p>It contain Item Aggregation and Social Aggregation</p>
<p>在这里本质上是先做了一层Item
Aggregation之后，用得到的结果再做一层Social Aggregation 所以这里的Item
Aggregation，本质上是Social Aggregation中的self-connection</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Social_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, social_adj_lists, aggregator, base_model=<span class="literal">None</span>, cuda=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># to_neighs is a list which element is list recording social neighbor node, and len(list) is batchsize,</span></span><br><span class="line">        to_neighs = []</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            to_neighs.append(self.social_adj_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Social aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, to_neighs)  <span class="comment"># user-user network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Item aggregation</span></span><br><span class="line">        self_feats = self.features(torch.LongTensor(nodes.cpu().numpy())).to(self.device)</span><br><span class="line">        self_feats = self_feats.t()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        <span class="comment"># Concatenate Item Aggregation and Social Aggregation, and through one layer MLP</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<p>full code of User Modeling</p>
<details>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Social_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, social_adj_lists, aggregator, base_model=<span class="literal">None</span>, cuda=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Social_Encoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.features = features</span><br><span class="line">        self.social_adj_lists = social_adj_lists</span><br><span class="line">        self.aggregator = aggregator</span><br><span class="line">        <span class="keyword">if</span> base_model != <span class="literal">None</span>:</span><br><span class="line">            self.base_model = base_model</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.device = cuda</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">2</span> * self.embed_dim, self.embed_dim)  <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># to_neighs is a list which element is list recording social neighbor node, and len(list) is batchsize,</span></span><br><span class="line">        to_neighs = []</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            to_neighs.append(self.social_adj_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Item aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, to_neighs)  <span class="comment"># user-user network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Social aggregation</span></span><br><span class="line">        self_feats = self.features(torch.LongTensor(nodes.cpu().numpy())).to(self.device)</span><br><span class="line">        self_feats = self_feats.t()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        <span class="comment"># Concatenate Item Aggregation and Social Aggregation, and through one layer MLP</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
</details>
<h4 id="item-aggregation">Item Aggregation</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, history_uv_lists, history_r_lists, aggregator, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        tmp_history_uv = []</span><br><span class="line">        tmp_history_r = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">#get nodes(batch) neighbors</span></span><br><span class="line">        <span class="comment">#tmp_history_uv is a list which len is 128,while it&#x27;s element is also a list meaning that the each node&#x27;s(in batch) neighbor item id list</span></span><br><span class="line">        <span class="comment">#tmp_history_r is similar with tmp_history_uv, but record the rating score instead of item id</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            tmp_history_uv.append(self.history_uv_lists[<span class="built_in">int</span>(node)])</span><br><span class="line">            tmp_history_r.append(self.history_r_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># after neigh aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  <span class="comment"># user-item network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># id to embedding (features : u2e)</span></span><br><span class="line">        self_feats = self.features.weight[nodes]</span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<p>And the <code>self.aggregator</code> in neigh aggregation is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Aggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    item and user aggregator: for aggregating embeddings of neighbors (item/user aggreagator).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, v2e, r2e, u2e, embed_dim, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, history_uv, history_r</span>):</span><br><span class="line">        <span class="comment"># create a container for result, shpe of embed_matrix is (batchsize,embed_dim)</span></span><br><span class="line">        embed_matrix = torch.empty(<span class="built_in">len</span>(history_uv), self.embed_dim, dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># deal with each single nodes&#x27; neighbors</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(history_uv)):</span><br><span class="line">            history = history_uv[i]</span><br><span class="line">            num_histroy_item = <span class="built_in">len</span>(history)</span><br><span class="line">            tmp_label = history_r[i]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># e_uv : turn neighbors id to embedding</span></span><br><span class="line">            <span class="comment"># uv_rep : turn single node to embedding</span></span><br><span class="line">            <span class="keyword">if</span> self.uv == <span class="literal">True</span>:</span><br><span class="line">                <span class="comment"># user component</span></span><br><span class="line">                e_uv = self.v2e.weight[history]</span><br><span class="line">                uv_rep = self.u2e.weight[nodes[i]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># item component</span></span><br><span class="line">                e_uv = self.u2e.weight[history]</span><br><span class="line">                uv_rep = self.v2e.weight[nodes[i]]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># get rating score embedding</span></span><br><span class="line">            e_r = self.r2e.weight[tmp_label]</span><br><span class="line">            <span class="comment"># concatenated rating and neighbor, and than through two layers mlp to get xia</span></span><br><span class="line">            x = torch.cat((e_uv, e_r), <span class="number">1</span>)</span><br><span class="line">            x = F.relu(self.w_r1(x))</span><br><span class="line"></span><br><span class="line">            o_history = F.relu(self.w_r2(x))</span><br><span class="line">            <span class="comment"># calculate neighbor attention and xia*weight to finish aggregation</span></span><br><span class="line">            att_w = self.att(o_history, uv_rep, num_histroy_item)</span><br><span class="line">            att_history = torch.mm(o_history.t(), att_w)</span><br><span class="line">            att_history = att_history.t()</span><br><span class="line"></span><br><span class="line">            embed_matrix[i] = att_history</span><br><span class="line">        <span class="comment"># result (batchsize, embed_dim)</span></span><br><span class="line">        to_feats = embed_matrix</span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
<p>While <code>self.att</code> is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dims</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, node1, u_rep, num_neighs</span>):</span><br><span class="line">        <span class="comment"># pi</span></span><br><span class="line">        uv_reps = u_rep.repeat(num_neighs, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># concatenated neighbot and pi</span></span><br><span class="line">        x = torch.cat((node1, uv_reps), <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># through 3 layers MLP</span></span><br><span class="line">        x = F.relu(self.att1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = F.relu(self.att2(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.att3(x)</span><br><span class="line">        <span class="comment"># get weights</span></span><br><span class="line">        att = F.softmax(x, dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> att</span><br></pre></td></tr></table></figure>
<h4 id="social-aggregation">Social Aggregation</h4>
<p>use the result of Item Aggregation and pi as input</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Social_Aggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Social Aggregator: for aggregating embeddings of social neighbors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, u2e, embed_dim, cuda=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, to_neighs</span>):</span><br><span class="line">        <span class="comment">#return a uninitialize matrix as result container, which shape is (batchsize, embed_dim)</span></span><br><span class="line">        embed_matrix = torch.empty(<span class="built_in">len</span>(nodes), self.embed_dim, dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nodes)):</span><br><span class="line">            <span class="comment"># get social graph neighbor</span></span><br><span class="line">            tmp_adj = to_neighs[i]</span><br><span class="line">            num_neighs = <span class="built_in">len</span>(tmp_adj)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># fase : can use user embedding instead of result of item aggregation to improve speed</span></span><br><span class="line">            <span class="comment"># e_u = self.u2e.weight[list(tmp_adj)] # fast: user embedding </span></span><br><span class="line">            <span class="comment"># slow: item-space user latent factor (item aggregation)</span></span><br><span class="line">            feature_neigbhors = self.features(torch.LongTensor(<span class="built_in">list</span>(tmp_adj)).to(self.device))</span><br><span class="line">            e_u = torch.t(feature_neigbhors)</span><br><span class="line"></span><br><span class="line">            u_rep = self.u2e.weight[nodes[i]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># concatenated node embedding and neigbor vector (result of item aggregation) </span></span><br><span class="line">            <span class="comment"># and than through MLPs and Softmax to calculate weights</span></span><br><span class="line">            att_w = self.att(e_u, u_rep, num_neighs)</span><br><span class="line">            <span class="comment"># weight*neighbor vector</span></span><br><span class="line">            att_history = torch.mm(e_u.t(), att_w).t()</span><br><span class="line">            embed_matrix[i] = att_history</span><br><span class="line">        to_feats = embed_matrix</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
<h3 id="item-modeling">Item Modeling</h3>
<p>Similar with the Item Aggregation of User Modeling</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, history_uv_lists, history_r_lists, aggregator, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        tmp_history_uv = []</span><br><span class="line">        tmp_history_r = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">#get nodes(batch) neighbors of item</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            tmp_history_uv.append(self.history_uv_lists[<span class="built_in">int</span>(node)])</span><br><span class="line">            tmp_history_r.append(self.history_r_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># after neigh aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  <span class="comment"># user-item network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># id to embedding (features : v2e)</span></span><br><span class="line">        self_feats = self.features.weight[nodes]</span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<p>And the <code>self.aggregator</code> in neigh aggregation is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Aggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    item and user aggregator: for aggregating embeddings of neighbors (item/user aggreagator).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, v2e, r2e, u2e, embed_dim, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, history_uv, history_r</span>):</span><br><span class="line">        <span class="comment"># create a container for result, shpe of embed_matrix is (batchsize,embed_dim)</span></span><br><span class="line">        embed_matrix = torch.empty(<span class="built_in">len</span>(history_uv), self.embed_dim, dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># deal with each single item nodes&#x27; neighbors</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(history_uv)):</span><br><span class="line">            history = history_uv[i]</span><br><span class="line">            num_histroy_item = <span class="built_in">len</span>(history)</span><br><span class="line">            tmp_label = history_r[i]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># e_uv : turn neighbors(user node) id to embedding</span></span><br><span class="line">            <span class="comment"># uv_rep : turn single node(item node) to embedding</span></span><br><span class="line">            <span class="keyword">if</span> self.uv == <span class="literal">True</span>:</span><br><span class="line">                <span class="comment"># user component</span></span><br><span class="line">                e_uv = self.v2e.weight[history]</span><br><span class="line">                uv_rep = self.u2e.weight[nodes[i]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># item component</span></span><br><span class="line">                e_uv = self.u2e.weight[history]</span><br><span class="line">                uv_rep = self.v2e.weight[nodes[i]]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># get rating score embedding</span></span><br><span class="line">            e_r = self.r2e.weight[tmp_label]</span><br><span class="line">            <span class="comment"># concatenated rating and neighbor, and than through two layers mlp to get fjt</span></span><br><span class="line">            x = torch.cat((e_uv, e_r), <span class="number">1</span>)</span><br><span class="line">            x = F.relu(self.w_r1(x))</span><br><span class="line"></span><br><span class="line">            o_history = F.relu(self.w_r2(x))</span><br><span class="line">            <span class="comment"># calculate neighbor attention and fjt*weight to finish aggregation</span></span><br><span class="line">            att_w = self.att(o_history, uv_rep, num_histroy_item)</span><br><span class="line">            att_history = torch.mm(o_history.t(), att_w)</span><br><span class="line">            att_history = att_history.t()</span><br><span class="line"></span><br><span class="line">            embed_matrix[i] = att_history</span><br><span class="line">        <span class="comment"># result (batchsize, embed_dim)</span></span><br><span class="line">        to_feats = embed_matrix</span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>SocialRec</category>
      </categories>
  </entry>
  <entry>
    <title>王树森推荐系统公开课</title>
    <url>/2023/03/13/Recommendation-WangShusen/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="基本概念">基本概念</h1>
<h2 id="指标">指标</h2>
<h3 id="消费指标">消费指标</h3>
<p>点击率=点击次数/曝光次数</p>
<p>点赞量=点赞次数/点击次数</p>
<p>收藏率=收藏次数/点击次数</p>
<p>转发率=转发次数/点击次数</p>
<p>阅读完成率=滑动到底次数/点击次数<span class="math inline">\(\times
f(笔记长度)\)</span></p>
<h3 id="北极星指标">北极星指标</h3>
<p>用户规模：日活用户数（DAU），月活用户数（MAU）</p>
<p>消费：人均使用推荐时长、人均阅读笔记数量</p>
<p>发布： 发布渗透率、人均发布量</p>
<h2 id="推荐系统链路">推荐系统链路</h2>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230313220835272.png"
alt="image-20230313220835272" />
<figcaption aria-hidden="true">image-20230313220835272</figcaption>
</figure>
<ol type="1">
<li>召回：快速从海量数据中取回几千个用户可能感兴趣的物品。</li>
<li>粗排：用小规模的模型的神经网络给召回的物品打分，然后做截断，选出分数最高的几百个物品。</li>
<li>精排：
用大规模神经网络给粗排选中的几百个物品打分，可以做截断，也可以不做截断。</li>
<li>重排：
对精排结果做多样性抽样，得到几十个物品，然后用规则调整物品的排序。</li>
</ol>
<h2 id="实验流程">实验流程</h2>
<h3
id="概要03推荐系统的ab测试没仔细看以后补">概要03推荐系统的AB测试：没仔细看以后补</h3>
<h1 id="召回">召回</h1>
<h2 id="基于物品的协同过滤-itemcf">基于物品的协同过滤 ItemCF</h2>
<p>基本思想：如果用户喜欢item1,而item1与item2相似，那么用户很可能喜欢item2.</p>
<h3 id="基本结构">基本结构：</h3>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230313223949470.png"
alt="image-20230313223949470" />
<figcaption aria-hidden="true">image-20230313223949470</figcaption>
</figure>
<p>我们从用户历史互动知道用户对<span
class="math inline">\(item_j\)</span>，感兴趣利用下面公式计算对候选物品的兴趣分数
<span class="math display">\[
\sum_jlike(user,item_j)\times sim(item_j,item)
\]</span> 在这个例子中，用户对候选item的兴趣是：<span
class="math inline">\(2\times
0.1+1\times0.4+4\times0.2+3\times0.6=3.2\)</span>,我们计算所有item的分数，然后返回分数最高的若干个item</p>
<h4 id="计算item相似度">计算item相似度</h4>
<p>可以通过与item交互过的用户重合度计算item相似度（其中一种方法，也可以用KG）</p>
<ol type="1">
<li>方法1：不考虑用户对物品的喜欢程度</li>
</ol>
<p><span class="math display">\[
sim(i_1,i_2) = \frac{|W1 \cap W2|}{\sqrt[2]{|W1|\cdot |W2|}}
\]</span></p>
<p>其中，喜欢物品<span class="math inline">\(i_1\)</span>的用户记作<span
class="math inline">\(W_1\)</span>,喜欢物品<span
class="math inline">\(i_2\)</span>的用户记作<span
class="math inline">\(W_2\)</span>.</p>
<ol start="2" type="1">
<li><p>方法2： 考虑用户对物品的喜欢程度,使用余弦相似度！</p>
<p>把每个item用向量表示 <span class="math display">\[
i_1=[like(u_1,i_1),like(u_2,i_1),\cdots ,like(u_n,i_1)] \space u_n\in W
\]</span></p>
<p><span class="math display">\[
i_2=[like(u_1,i_2),like(u_2,i_2),\cdots ,like(u_n,i_2)] \space u_n\in W
\]</span></p>
<p><span class="math display">\[
W=W_1\cup W_2
\]</span></p>
<p>我们使用余弦相似度计算： <span class="math display">\[
similarity=cos(\theta) = \frac{A\cdot B}{||A||\space ||B||}
\]</span> 如果有用户k只喜欢其中一个物品:只喜欢<span
class="math inline">\(i_1\)</span>不喜欢<span
class="math inline">\(i_2\)</span>,那么<span
class="math inline">\(i_2[k]=0\)</span>，所以点乘后第k项为0，所以点乘只与同时喜欢<span
class="math inline">\(i_1,i_2\)</span>的用户有关系，如下面公式 <span
class="math display">\[
sim(i_1,i_2) = \frac{\sum_{v\in V}like(v,i_i)\cdot
like(v,i_2)}{\sqrt[2]{\sum_{u_1\in
W_1}like^2(u_1,i_1)}\sqrt[2]{\sum_{u_2\in W_2}like^2(u_2,i_2)}}
\]</span></p></li>
</ol>
<h3 id="运作基本流程">运作基本流程</h3>
<ol type="1">
<li><p>实现做离线计算，预先计算两个索引：</p>
<ol type="1">
<li><p>“user2item”：记录每个用户最近点击交互过的n个物品ID（lastN）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># example 不一定是公司真实的保存方式</span></span><br><span class="line">user2item=&#123;</span><br><span class="line">    <span class="string">&#x27;u1&#x27;</span>:[[i1,like(u1,i1)],[i2,like(u1,i2)],...,[<span class="keyword">in</span>,like(u1,<span class="keyword">in</span>)]]</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>"item2item":计算物品之间两两相似度，记录每个物品最相似的k个物品。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">item2item=&#123;</span><br><span class="line">	#target item:[[similar item, similarity score]...]</span><br><span class="line">	&#x27;i1&#x27;:[[i2,0.9],[i6,0.88]...]</span><br><span class="line">	&#x27;i2&#x27;:...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol></li>
<li><p>线上做召回</p>
<ol type="1">
<li>给定用户ID，通过“user2item”找到用户近期感兴趣的物品列表(last-n)</li>
<li>对于last-n列表中每个物品，通过“item2item"找到top-k相似物品。现在有1个user，n个互动物品，nxk个候选物品。</li>
<li>计算候选物品兴趣分数</li>
<li>返回分数最高的100个物品作为推荐结果</li>
</ol></li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>BasicTurtorial</category>
      </categories>
  </entry>
  <entry>
    <title>RippleNet</title>
    <url>/2023/03/02/RippleNet/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p><strong>CF</strong>: sparsity, cold start</p>
<p><strong>KG-benefit</strong>:</p>
<ol type="1">
<li>KG introduces semantic relatedness among items, which can help find
their latent connections and improve the <em>precision</em> of
recommended items;</li>
<li>KG consists of relations with various types, which is helpful for
extending a user’s interests reasonably and increasing the
<em>diversity</em> of recommended items;</li>
<li>KG connects a user’s historical records and the recommended ones,
thereby bringing <em>explainability</em> to recommender systems.</li>
</ol>
<p><strong>Existing KG model</strong>:</p>
<ol type="1">
<li><strong>embedding-based method</strong>: DKN, CKE, SHINE, but more
suitable for in-graph applications</li>
<li><strong>path-based method</strong>: rely heavily on manually
designed meta-paths</li>
</ol>
<p>so the author proposes RippleNet:</p>
<ol type="1">
<li>combine embedding-based and path-based() methods
<ol type="1">
<li>RippleNet incorporates the KGE methods into recommendation naturally
by preference propagation;<br />
</li>
<li>RippleNet can automatically discover possible paths from an item in
a user’s history to a candidate item.</li>
</ol></li>
</ol>
<h1 id="method">Method</h1>
<p>专注于挖掘KG中用户感兴趣的实体！！</p>
<h2 id="input">Input</h2>
<p>interaction matrix <strong>Y</strong> <em>and knowledge graph</em>
<strong>G</strong></p>
<h2 id="some-definition">Some definition</h2>
<h3 id="relevant-entity">Relevant entity</h3>
<p>the set of <strong>k</strong>-hop relevant entities for user
<strong>u</strong> is defined as</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302155703683.png"
alt="image-20230302155703683" />
<figcaption aria-hidden="true">image-20230302155703683</figcaption>
</figure>
<p><span class="math inline">\(\varepsilon_u^0=V_u =
\{v|y_{uv}=1\}\)</span> is the items which the user interacts with, and
they can link with entities in knowledge graph</p>
<p>can be seen as the seed set of user u in
KG(就是user如何参与到KG中)</p>
<h3 id="ripple-set">Ripple set</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302155653732.png"
alt="image-20230302155653732" />
<figcaption aria-hidden="true">image-20230302155653732</figcaption>
</figure>
<h2 id="model">Model</h2>
<h3 id="first-layer-propagation">First layer propagation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302163505799.png"
alt="image-20230302163505799" />
<figcaption aria-hidden="true">image-20230302163505799</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302165453023.png"
alt="image-20230302165453023" />
<figcaption aria-hidden="true">image-20230302165453023</figcaption>
</figure>
<p>v: embedding of item. Item embedding can incorporate one-hot ID ,
attributes of an item, based on the application scenario.</p>
<p>r: embedding of relation between head entity and tail entity.</p>
<p>h: embedding of head entity.</p>
<p>t: embedding of tail entity.</p>
<p>attention weight <span class="math inline">\(p_i\)</span> can be
regarded as the similarity of item <strong>v</strong> and the entity
<span class="math inline">\(h_i\)</span> measured in the space of
relation <span class="math inline">\(r_i\)</span>.</p>
<p><span class="math inline">\(r_i\)</span> is important, since an
item-entity pair may have different similarities when measured by
different relations</p>
<h3 id="multi-layer">Multi-layer</h3>
<p>the second layer just replace v with <span
class="math inline">\(o_u^1\)</span></p>
<p><span class="math display">\[
p_i = softmax(o_u^{1T}R_ih_i) =
\frac{exp(o_u^{1T}T_ih_i)}{\sum_{(h,r,t)\in S_u^2}exp(o_u^{1T}Rh)}
\]</span></p>
<p><span class="math display">\[
o_u^2 = \sum_{(h_i,r_i,t_i)\in S_u^2}p_it_i
\]</span></p>
<p>and third layer replace <span class="math inline">\(o_u^1\)</span>
with <span class="math inline">\(o_u^2\)</span></p>
<p>while</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302165932080.png"
alt="image-20230302165932080" />
<figcaption aria-hidden="true">image-20230302165932080</figcaption>
</figure>
<h3 id="predict">predict</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302170052985.png"
alt="image-20230302170052985" />
<figcaption aria-hidden="true">image-20230302170052985</figcaption>
</figure>
<h3 id="whole-process">Whole process</h3>
<p><strong>Propagation only used in KG-graph</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/6C506EFAADC22D9AC38B07273F102601.png"
alt="6C506EFAADC22D9AC38B07273F102601" />
<figcaption
aria-hidden="true">6C506EFAADC22D9AC38B07273F102601</figcaption>
</figure>
<p>模型不断扩散，不断获取更高层数neighbor的信息，最后通过加在一起汇总</p>
<p>所以与曾经互动过的item有关系的实体信息（KG信息）汇总为user
embedding，最后再与没互动过的item计算估计互动概率，</p>
<p>所以是否能理解为user汇总的KG信息</p>
<h3 id="loss-function还没想明白">Loss Function（还没想明白）</h3>
<p>别人的笔记：：</p>
<p>这里的分成三个部分：分别是预测分数的交叉熵损失，知识图谱特征表示的损失，参数正则化的损失：</p>
<p>预测部分的损失很好理解，就是用户和该item之间的预测值和真实值的loss</p>
<p>知识图谱特征表示的损失：我们在计算每个阶段的加权求和时上面说了，假设前提是hR=t，这是假设，所以我们需要设一个loss让模型学习，学习的内容就是hR和t之间计算相似度后，预测0,1是否相似</p>
<p>l2正则化损失：每一个hop中h，r，t分别和自己相乘后，求和再求均值得到一个值，即为该loss（这里我理解的不是很深，有了解的可以评论区说说）</p>
<h1 id="experiment">Experiment</h1>
<h1 id="other">Other</h1>
<ol type="1">
<li><p>ripple set 可能太大，</p>
<p>在RippleNet中，我们可以对固定大小的邻居集进行采样，而不是使用完整的纹波集来进一步减少计算开销。</p></li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>Unsolve problem</title>
    <url>/2023/03/02/Unsolve_question/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<p>KGCN Note</p>
<p>about ripple net, why</p>
<p>the relation <strong>R</strong> can hardly be trained to capture the
sense of importance in the quadratic form <strong>v</strong>
⊤<strong>Rh</strong> ??</p>
<p>about attention：</p>
<p>所以KGCN不用propagation更新用户的原因是否是因为希望user的embedding能专注于提取个性化信息，但是这样是否会让user和item没那么好聚类？</p>
<ol start="2" type="1">
<li><p>RippleNet</p>
<p>然后将Rh和v相乘并删除上一步增加的维度得到样本v和实体h在关系R的空间中的相似度？？为啥是R空间</p>
<p>为什么KGCN又不能</p></li>
</ol>
<p>问老师</p>
<p>1.如果训练的太慢怎么办</p>
<p>2.是否要做CF</p>
<p>3.对于先进技术用于推荐系统</p>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>常见激活函数</title>
    <url>/2023/02/28/activate_function/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="常见激活函数">常见激活函数</h1>
<p>激活函数作用：加入非线性因素</p>
<h2 id="sigmoid">Sigmoid</h2>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1+exp(-x)}
\]</span></p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228224935533.png" alt="image-20230228224935533" style="zoom:40%;" /></p>
<p>输出的值范围在[0,1]之间。但是<code>sigmoid</code>型函数的输出存在<strong>均值不为0</strong>的情况，并且存在<strong>梯度消失的问题</strong>，在深层网络中被其他激活函数替代。在<strong>逻辑回归</strong>中使用的该激活函数用于输出<strong>分类</strong>。</p>
<h3 id="求导公式">求导公式</h3>
<p>链式法则</p>
<h3 id="梯度消失原因">梯度消失原因：</h3>
<p><span class="math display">\[
\sigma&#39;(x) = \sigma\space \cdot (1-\sigma)
\]</span></p>
<ol type="1">
<li>sigmoid函数两边的斜率趋向0，很难继续学习</li>
<li>sigmoid导数两个部分都小于1，在深层神经网络中，靠前layer参数会因为后面多层sigmoid导数叠加（链式法则）导致更新的特别慢。</li>
</ol>
<h3 id="缺点解决办法">缺点解决办法</h3>
<ol type="1">
<li>在深层网络中被其他激活函数替代。如<code>ReLU(x)</code>、<code>Leaky ReLU(x)</code>等</li>
<li>在分类问题中，sigmoid做激活函数时，使用交叉熵损失函数替代均方误差损失函数。</li>
<li>采用正确的权重初始化方法（让初始化的数据尽量不要落在梯度消失区域）</li>
<li>加入BN层（同上，避免数据落入梯度消失区）</li>
<li>分层训练权重</li>
</ol>
<h2 id="tanh">tanh</h2>
<p><span class="math display">\[
tanh(x) = \frac{e^x-e^{(-x)}}{e^x+e^{(-x)}} =\frac{e^{2x}-1}{e^{2x}+1}=
2 \cdot sigmoid(2x)-1
\]</span></p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194241514.png" alt="image-20230307194241514" style="zoom:67%;" /></p>
<p><code>tanh(x)</code>型函数可以解决<code>sigmoid</code>型函数的<strong>期望（均值）不为0</strong>的情况。函数输出范围为(-1,+1)。但<code>tanh(x)</code>型函数依然存在<strong>梯度消失的问题</strong>。</p>
<p>在LSTM中使用了<code>tanh(x)</code>型函数。</p>
<h2 id="relu">Relu</h2>
<p><code>ReLU(x)</code>型函数可以有效避免<strong>梯度消失的问题</strong>，公式如下：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228222815687.png"
alt="image-20230228222815687" />
<figcaption aria-hidden="true">image-20230228222815687</figcaption>
</figure>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194352199.png" alt="image-20230307194352199" style="zoom:67%;" /></p>
<p><code>ReLU(x)</code>型函数的缺点是<strong>负值成为“死区”</strong>，神经网络无法再对其进行响应。Alex-Net使用了<code>ReLU(x)</code>型函数。当我们训练深层神经网络时，最好使用<code>ReLU(x)</code>型函数而不是<code>sigmoid(x)</code>型函数。</p>
<p>ReLU梯度稳定，值还比sigmoid大，所以<strong>可以加快网络训练</strong>。</p>
<p>但是要注意，我们在输入图像时就要注意，应该使用Min-Max归一化，而不能使用Z-score归一化。（避免进入死区）</p>
<h3 id="在0点不可导">在0点不可导</h3>
<p>人为将梯度规定为0（源码就是这么写的）</p>
<h2 id="relu6">Relu6</h2>
<p>Relu的正值输出是[0，无穷大]，但计算机内存优先，所以限定relu最大值为6</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194457525.png" alt="image-20230307194457525" style="zoom:67%;" /></p>
<h2 id="leakyrelu">LeakyRelu</h2>
<p>为<strong>负值增加了一个斜率</strong>，缓解了“死区”现象，公式如下：</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194659126.png" alt="image-20230307194659126" style="zoom:67%;" /></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228222900735.png"
alt="image-20230228222900735" />
<figcaption aria-hidden="true">image-20230228222900735</figcaption>
</figure>
<p><code>Leaky ReLU(x)</code>型函数缺点是，<strong>超参数a（阿尔法）合适的值不好设定</strong>。当我们想让神经网络能够学到负值信息，那么使用该激活函数。</p>
<h2 id="p-relu-参数化relu">P-Relu 参数化Relu</h2>
<p>数化ReLU（P-ReLU）。参数化ReLU为了解决超参数a（阿尔法）合适的值不好设定的问题，干脆将这个参数也融入模型的整体训练过程中。也使用误差反向传播和随机梯度下降的方法更新参数。</p>
<h2 id="r-relu-随机化relu">R-Relu 随机化Relu</h2>
<p>就是超参数a（阿尔法）随机化，<strong>让不同的层自己学习不同的超参数</strong>，但随机化的超参数的分布符合均值分布或高斯分布。</p>
<h2 id="mish激活函数">Mish激活函数</h2>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194824945.png" alt="image-20230307194824945" style="zoom:67%;" />
<span class="math display">\[
Mish(x) = x\cdot tanh(log(1+e^x))
\]</span></p>
<p>在负值中，允许有一定的梯度流入。</p>
<h2 id="elu指数化线性单元">ELU指数化线性单元</h2>
<p>也是为了解决死区问题，公式如下：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194918301.png"
alt="image-20230307194918301" />
<figcaption aria-hidden="true">image-20230307194918301</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228224119801.png"
alt="image-20230228224119801" />
<figcaption aria-hidden="true">image-20230228224119801</figcaption>
</figure>
<p>缺点是<strong>指数计算量大</strong>。</p>
<h2 id="maxout">Maxout</h2>
<p>就是用一个MLP层作为激活函数。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307195003327.png"
alt="image-20230307195003327" />
<figcaption aria-hidden="true">image-20230307195003327</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307195013184.png"
alt="image-20230307195013184" />
<figcaption aria-hidden="true">image-20230307195013184</figcaption>
</figure>
<p>与常规的激活函数不同，<strong>Maxout</strong>是一个可以学习的<strong>分段线性函数</strong>。其原理是，任何ReLU及其变体等激活函数都可以看成分段的线性函数，而Maxout加入的一层神经元正是一个可以学习参数的分段线性函数。</p>
<p>优点是其拟合能力很强，理论上可以拟合任意的凸函数。缺点是参数量激增！在Network-in-Network中使用的该激活函数。</p>
<h1 id="softmax求导">Softmax求导</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307195822249.png"
alt="image-20230307195822249" />
<figcaption aria-hidden="true">image-20230307195822249</figcaption>
</figure>
<p>要结合交叉熵loss函数考虑</p>
<p><span class="math inline">\(\frac{dL}{dz}=\frac{dL}{da}\cdot
\frac{da}{dz}\)</span></p>
<p>假设第j个类别是正确的，<span
class="math inline">\(y_j=1\)</span>,其它为0</p>
<p><span class="math inline">\(L = -\sum_{i=1}^ny_iln(a_i)\)</span></p>
<p><span class="math inline">\(\frac{dL}{da} =
-y_iln(a_j)=-ln(a_j)\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307200559541.png"
alt="image-20230307200559541" />
<figcaption aria-hidden="true">image-20230307200559541</figcaption>
</figure>
<p>所以最终Loss只跟label类别有关</p>
<p>所以当i=j：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307201705230.png"
alt="image-20230307201705230" />
<figcaption aria-hidden="true">image-20230307201705230</figcaption>
</figure>
<p>当i!=j:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307201737792.png"
alt="image-20230307201737792" />
<figcaption aria-hidden="true">image-20230307201737792</figcaption>
</figure>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>常见损失函数</title>
    <url>/2023/02/28/loss_function/</url>
    <content><![CDATA[<p>常见损失函数及常见问题</p>
<span id="more"></span>
<h1 id="常见损失函数">常见损失函数</h1>
<p><strong>损失函数</strong>用来评价模型的<strong>预测值</strong>和<strong>真实值</strong>不一样的程度，在模型正常拟合的情况下，损失函数值越低，模型的性能越好。不同的模型用的损失函数一般也不一样。</p>
<p><strong>损失函数</strong>分为<strong>经验风险损失函数</strong>和<strong>结构风险损失函数</strong>。经验风险损失函数指<strong>预测结果</strong>和<strong>实际结果</strong>的差值，结构风险损失函数是指<strong>经验风险损失函数</strong>加上<strong>正则项</strong>。</p>
<h2 id="常用">常用</h2>
<h3 id="用于回归">用于<strong>回归</strong>：</h3>
<h4 id="绝对值损失函数">绝对值损失函数</h4>
<p><span class="math display">\[
L(Y,f(x)) = |Y-f(x)|
\]</span></p>
<h4 id="平方损失函数">平方损失函数</h4>
<p><span class="math display">\[
L(Y,f(x)) = (Y-f(x))^2
\]</span></p>
<p>对n个数据求平方损失后加和求平均叫<strong>均方误差MSE</strong>，常在<strong>线性回归</strong>使用
<span class="math display">\[
\frac{1}{N}\sum_n(Y-f(x))^2
\]</span></p>
<h3 id="用于分类">用于分类</h3>
<h4 id="损失函数zero-one-loss">0-1损失函数（zero-one loss）</h4>
<p><span class="math display">\[
L(Y,f(x)) = \left\{
\begin{array}{rcl}
1   &amp;   &amp;{Y!=f(x)}\\
0   &amp;   &amp;{Y=f(x)}
\end{array} \right.
\]</span></p>
<p>非黑即白，过于严格，用的很少，比如<strong>感知机</strong>用。</p>
<p>可通过设置阈值放宽条件 <span class="math display">\[
L(Y,f(x)) = \left\{
\begin{array}{rcl}
1   &amp;   &amp;{|Y-f(x)&gt;=T}\\
0   &amp;   &amp;{|Y-f(x)&lt;T}
\end{array} \right.
\]</span></p>
<h4 id="对数损失函数log-loss">对数损失函数（log loss）</h4>
<p><span class="math display">\[
L(Y,P(Y|X)) = -logP(Y|X)
\]</span></p>
<p>Y为真实分类，<span
class="math inline">\(P(Y|X)\)</span>为X条件下分类为Y的概率。用于最大似然估计，等价于交叉熵损失函数</p>
<p>加负号原因：习惯在模型更准确的情况下，loss函数越小</p>
<p>加log原因：这和最大（极大）似然估计有关，对数损失是用于最大似然估计的。</p>
<p><strong>最大似然估计</strong>：<strong>利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值</strong>。</p>
<p>我们假定一组参数（<span
class="math inline">\(\Theta\)</span>）在一堆数据（样本结果<span
class="math inline">\(x_1,x_2...\)</span>）下的<strong>似然值</strong>为<code>P(θ|x1,x2,...,xn)=P(x1|θ)*P(x2|θ)*...*P(xn|θ)</code>，可以看出来，似然值等于每一条数据在这组参数下的条件概率<strong>之积</strong>。求概率是<strong>乘性</strong>，而求损失是<strong>加性</strong>，所以才需要借助log（对数）来<strong>转积为和</strong>，另一方面也是为了简化运算。</p>
<p>对数损失在<strong>逻辑回归</strong>和<strong>多分类任务</strong>上广泛使用。交叉熵损失函数的标准型就是对数损失函数，本质没有区别。</p>
<h4 id="交叉熵损失函数">交叉熵损失函数</h4>
<p>双分类： <span class="math display">\[
L(Y,f(x)) = -[Ylnf(x)+(1-y)ln(1-f(x))]
\]</span> 多分类： <span class="math display">\[
L(Y,f(x)) = -Ylnf(x)
\]</span></p>
<h4 id="合页损失函数hinge-loss">合页损失函数(hinge loss)</h4>
<p><span class="math display">\[
L(Y,f(x)) = max(0, 1-Y\cdot f(x))
\]</span></p>
<p>SVM就是使用的合页损失，还加上了正则项。公式意义是，当样本被正确分类且函数间隔大于1时，合页损失是0，否则损失是<span
class="math inline">\(1-Y\cdot f(x)\)</span>.</p>
<p>SVM中<span class="math inline">\(Y\cdot
f(x)\)</span>为函数间隔，对于函数间隔：</p>
<ol type="1">
<li><p>正负</p>
<p>当样本被正确分类时，<span class="math inline">\(Y\cdot
f(x)&gt;0\)</span>；当样本被错误分类时，<span
class="math inline">\(Y\cdot f(x)&lt;0\)</span>。</p></li>
<li><p>大小</p>
<p><span class="math inline">\(Y\cdot
f(x)\)</span>的绝对值代表样本距离决策边界的远近程度。<span
class="math inline">\(Y\cdot
f(x)\)</span>的绝对值越大，表示样本距离决策边界越远。因此，我们可以知道：</p></li>
</ol>
<p>​ 当<span class="math inline">\(Y\cdot f(x)&gt;0\)</span>时，<span
class="math inline">\(Y\cdot
f(x)\)</span>的绝对值越大表示决策边界对样本的区分度越好</p>
<p>​ 当<span class="math inline">\(Y\cdot f(x)&lt;0\)</span>时，<span
class="math inline">\(Y\cdot
f(x)\)</span>的绝对值越大表示决策边界对样本的区分度越差</p>
<h4 id="指数损失函数exponential-loss">指数损失函数(exponential
loss)</h4>
<p><span class="math display">\[
L(Y,f(x)) = exp(-Y\cdot f(x)) = \frac{exp(f(x))}{exp(Y)}
\]</span></p>
<p>常用于AdaBoost算法，</p>
<p><strong>那么为什么AdaBoost算法使用指数损失函数，而不使用其他损失函数呢？</strong></p>
<p>这是因为，当<strong>前向分步算法的损失函数是指数损失函数</strong>时，其学习的具体操作等价于AdaBoost算法的学习过程。</p>
<h3 id="用于分割">用于分割</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230221201538734.png"
alt="image-20230221201538734" />
<figcaption aria-hidden="true">image-20230221201538734</figcaption>
</figure>
<h3 id="用于检测">用于检测</h3>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230228205041367.png"
alt="image-20230228205041367" />
<figcaption aria-hidden="true">image-20230228205041367</figcaption>
</figure>
<h1 id="常见损失函数问题">常见<strong>损失函数问题</strong></h1>
<h2 id="交叉熵相关">交叉熵相关</h2>
<h3
id="交叉熵函数与最大似然函数的联系和区别">交叉熵函数与最大似然函数的联系和区别？</h3>
<p><strong>区别</strong>：</p>
<p><strong>交叉熵函数</strong>使用来描述模型预测值和真实值的差距大小，越大代表越不相近；</p>
<p><strong>极大似然</strong>就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！即“模型已定，参数未知”</p>
<p><strong>联系</strong>：</p>
<p><strong>交叉熵函数</strong>可以由<strong>最大似然函数</strong>在<strong>伯努利分布</strong>的条件下推导出来，或者说<strong>最小化交叉熵函数</strong>的本质就是<strong>对数似然函数的最大化</strong>。</p>
<p><img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/IMG_0115(20230224-203706).PNG" /></p>
<h3
id="在用sigmoid作为激活函数的时候为什么要用交叉熵损失函数而不用均方误差损失函数">在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？</h3>
<p>另一个问法其实是在分类问题中为什么不用均方误差做损失函数。</p>
<ol type="1">
<li><p><strong>sigmoid</strong>作为激活函数的时候，如果采用<strong>均方误差损失函数</strong>，那么这是一个<strong>非凸优化</strong>问题，不宜求解。而采用<strong>交叉熵损失函数</strong>依然是一个<strong>凸优化</strong>问题，更容易优化求解。（凸优化问题中局部最优解同时也是全局最优解）。而且<span
class="math inline">\(\frac{dL}{dW}\)</span>中，有地方为0，如果参数刚好导致<span
class="math inline">\(\frac{dL}{dW}\)</span>为0，参数就不会更新。</p>
<p><img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224210154928.png" /></p></li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224211215148.png"
alt="image-20230224211215148" />
<figcaption aria-hidden="true">image-20230224211215148</figcaption>
</figure>
<ol start="2" type="1">
<li>因为<strong>交叉熵损失函数</strong>可以<strong>完美解决平方损失函数权重更新过慢</strong>的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。</li>
</ol>
<p>​ 方损失函数权重更新过慢原因：</p>
<p>​ 梯度更新公式为：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224211408952.png"
alt="image-20230224211408952" />
<figcaption aria-hidden="true">image-20230224211408952</figcaption>
</figure>
<p>这里a是预测值，y是实际值</p>
<p>有<span
class="math inline">\(\sigma&#39;(z)\)</span>这一项而sigmoid函数两端梯度很小，导致参数更新缓慢。</p>
<p>而交叉熵函数不会有这个问题虽然有<span
class="math inline">\(\sigma(z)\)</span>但没有<span
class="math inline">\(\sigma&#39;(z)\)</span>,求导detail如下：</p>
<details>
<img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224211928602.png" >
</details>
<h3 id="交叉熵和均分函数区别">交叉熵和均分函数区别</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224212504307.png"
alt="image-20230224212504307" />
<figcaption aria-hidden="true">image-20230224212504307</figcaption>
</figure>
<h3 id="如何推导出交叉熵函数">如何推导出交叉熵函数</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224215208050.png"
alt="image-20230224215208050" />
<figcaption aria-hidden="true">image-20230224215208050</figcaption>
</figure>
<h3 id="为什么交叉熵函数有log项">为什么交叉熵函数有log项</h3>
<p>第一种：因为是公式推导出来的，比如第六题的推导，推导出来的有log项。</p>
<p>第二种：通过最大似然估计的方式求得交叉熵公式，这个时候引入log项。这是因为似然函数（概率）是乘性的，而loss函数是加性的，所以需要引入log项“<strong>转积为和</strong>”。而且也是为了<strong>简化运算</strong>。</p>
<h3 id="交叉熵的设计思想">交叉熵的设计思想</h3>
<p><strong>交叉熵函数</strong>的本质是对数函数。</p>
<p><strong>交叉熵函数</strong>使用来描述模型预测值和真实值的差距大小，越大代表越不相近。</p>
<p><strong>交叉熵损失函数</strong>可以<strong>完美解决平方损失函数权重更新过慢</strong>的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。</p>
<p>对数损失在<strong>逻辑回归</strong>和<strong>多分类任务</strong>上广泛使用。交叉熵损失函数的标准型就是对数损失函数，本质没有区别。</p>
<h2 id="cv相关">CV相关</h2>
<h3 id="yolo损失函数">Yolo损失函数</h3>
<p>Yolo是用于模板检测的模型</p>
<p>Yolo的损失函数由四部分组成：</p>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823639975/8B2446F6E2BC3932829E4B801BDBDF05"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测的中心坐标做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823762782/488A1D20613F3E03B97A925F2C63D9AF"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测边界框的宽高做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823867484/DE22034D2077B5200B2C5440D47249FC"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测的类别做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823980181/9CE8A218F55F619B9EAEBCDFCFBF6446"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测的置信度做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618824075778/79B60A7E11ACBF428FD0510200949CFC"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>我们发现每一项loss的计算都是L2
loss（平方差），即使是分类问题也是。所以说yolo是把<strong>分类</strong>问题转为了<strong>回归</strong>问题。</p>
<h3 id="iou与miou计算">IOU与MIOU计算</h3>
<p>IOU（Intersection over Union），交集占并集的大小。</p>
<figure>
<img
src="https://www.nowcoder.com/equation?tex=%0A%20%20IOU%3DJaccard%20%3D%5Cfrac%7B%7CA%5Ccap%20B%7C%7D%20%7B%7CA%5Ccup%20B%7C%7D%3D%5Cfrac%7B%7CA%5Ccap%20B%7C%7D%20%7B%7CA%7C%2B%7CB%7C-%7CA%5Ccap%20B%7C%7D%20%5C%5C%0A%20%20%5Ctag%7B.%7D%0A%20%20&amp;preview=true"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>mIOU一般都是基于类进行计算的，将每一类的IOU计算之后累加，再进行平均，得到的就是mIOU。</p>
<h2 id="其它">其它</h2>
<h3 id="kl散度">KL散度</h3>
<p>相对熵（relative
entropy），又被称为Kullback-Leibler散度（Kullback-Leibler
divergence）或信息散度（information
divergence），是<strong>两个概率分布（probability
distribution）间差异的非对称性度量</strong>
。在信息理论中，<strong>相对熵等价于两个概率分布的信息熵（Shannon
entropy）的差值</strong>。</p>
<p>设<img
src="https://www.nowcoder.com/equation?tex=P(x)&amp;preview=true"
alt="img" />，<img
src="https://www.nowcoder.com/equation?tex=Q(x)&amp;preview=true"
alt="img" />是随机变量<img
src="https://www.nowcoder.com/equation?tex=X&amp;preview=true"
alt="img" />上的两个概率分布，则在离散和连续随机变量的情形下，相对熵的定义分别为：</p>
<figure>
<img
src="https://www.nowcoder.com/equation?tex=%0AKL(P%7C%7CQ)%3D%5Csum%7BP(x)log%20%5Cfrac%7BP(x)%7D%7BQ(x)%7D%7D%20%5C%5C%0AKL(P%7C%7CQ)%3D%5Cint%7BP(x)log%20%5Cfrac%7BP(x)%7D%7BQ(x)%7Ddx%7D%20%5C%5C%0A%5Ctag%7B.%7D%0A&amp;preview=true"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><img
src="https://www.nowcoder.com/equation?tex=Q(x)&amp;preview=true"
alt="img" />为<strong>理论概率分布</strong>，<img
src="https://www.nowcoder.com/equation?tex=P(x)&amp;preview=true"
alt="img" />为模型<strong>预测概率分布</strong>，而KL就是度量这两个分布的差异性，当然差异越小越好，所以KL也可以用作损失函数。</p>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>常见优化函数</title>
    <url>/2023/03/07/optimizer/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="常见优化函数">常见优化函数</h1>
<h2 id="梯度下降gd">梯度下降GD</h2>
<p><strong>梯度下降的核心思想：负梯度方向是使函数值下降最快的方向</strong></p>
<h3 id="批次梯度下降bgd">批次梯度下降BGD</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307202943629.png"
alt="image-20230307202943629" />
<figcaption aria-hidden="true">image-20230307202943629</figcaption>
</figure>
<p><strong>优点</strong>：在梯度下降法中，因为每次都遍历了完整的训练集，<strong>其能保证结果为全局最优</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307204257470.png"
alt="image-20230307204257470" />
<figcaption aria-hidden="true">image-20230307204257470</figcaption>
</figure>
<p><strong>缺点</strong>：我们需要对于每个参数求偏导，且在对每个参数求偏导的过程中还需要对训练集遍历一次，当训练集（m）很大时，计算费时</p>
<p><strong>解决方法</strong>：使用minibatch去更新</p>
<h3 id="随机梯度下降">随机梯度下降</h3>
<p>为了解决BGD耗时过长，它是利用单个样本的损失函数对θ求偏导得到对应的梯度，来更新θ，更新过程如下：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307204324179.png"
alt="image-20230307204324179" />
<figcaption aria-hidden="true">image-20230307204324179</figcaption>
</figure>
<p>速度快，但受抽样影响大，<strong>噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307204553630.png"
alt="image-20230307204553630" />
<figcaption aria-hidden="true">image-20230307204553630</figcaption>
</figure>
<p>因为每一次迭代的梯度受抽样的影响比较大，学习率需要逐渐减少，否则模型很难收敛。在实际操作中，一般采用线性衰减：
<span class="math display">\[
\eta_k=(1-\alpha)\eta_0+\alpha\eta_{\tau}
\]</span></p>
<p><span class="math display">\[
\alpha=\frac{k}{\tau}
\]</span></p>
<p><span class="math inline">\(\eta_0\)</span>:初始学习率</p>
<p><span class="math inline">\(\eta_{\tau}\)</span>：
最后一次迭代的学习率</p>
<p><span class="math inline">\(\tau\)</span>：自然迭代次数</p>
<p><span class="math inline">\(\eta_{\tau}\)</span>设为<span
class="math inline">\(\eta_0\)</span>的1%，k一般设为100的倍数。</p>
<p><strong>优点</strong>：收敛速度快</p>
<p><strong>缺点</strong>：</p>
<ol type="1">
<li><p>训练不稳定：噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p></li>
<li><p>选择适当的学习率可能很困难。
太小的学习率会导致收敛性缓慢，而学习速度太大可能会妨碍收敛，并导致损失函数在最小点波动。</p></li>
<li><p>无法逃脱鞍点</p></li>
</ol>
<details>
<p>在数学中，鞍点或极小值点是函数图形表面上的一个点，其正交方向上的斜率(导数)均为零(临界点)，但不是函数的局
部极值。一句话概括就是：一个不是局部极值点的驻点称为鞍点。
*驻点：函数在一点处的一阶导数为零。
<img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307205942585.png"></p>
<h3 id="min-batch-小批量梯度下降mbgd">min-batch 小批量梯度下降MBGD</h3>
<p><strong>算法的训练过程比较快，而且也要保证最终参数训练的准确率</strong></p>
<p>m表示一个批次的数据个数</p>
<h2 id="动量方法">动量方法</h2>
<h3 id="momentum随机梯度下降">Momentum随机梯度下降</h3>
<p>核心思想：Momentum借用了物理中的<strong>动量</strong>概念,即前一次的梯度也会参与运算。为了表示动量，引入了<strong>一阶动量</strong>m。<img
src="https://www.nowcoder.com/equation?tex=m&amp;preview=true"
alt="img" />是之前的梯度的累加,但是每回合都有一定的衰减。公式如下：
<span class="math display">\[
m_t=\beta m_{t-1}+(1-\beta)\cdot g_t
\]</span></p>
<p><span class="math display">\[
w_{t+1}=w_t-\eta \cdot m_t
\]</span></p>
<p><span class="math inline">\(g_t\)</span>：
为第t次计算的梯度（就是现在要算这次）</p>
<p><span class="math inline">\(m_{t-1}\)</span>: 为之前梯度的累加</p>
<p><span class="math inline">\(\beta\)</span>: 动量因子</p>
<p>所以当前权值的改变受上一次改变的影响，类似加上了<strong>惯性</strong>。</p>
<p>优点：momentum能够加速SGD收敛，抑制震荡。并且动量有机会逃脱局部极小值(鞍点)。</p>
<ol type="1">
<li>在梯度方向改变时，momentum能够降低参数更新速度，从而减少震荡；</li>
<li>在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛。</li>
</ol>
<h3 id="nesterov动量随机梯度下降法">Nesterov动量随机梯度下降法</h3>
<p>Nesterov是Momentum的变种。与Momentum唯一区别就是，计算梯度的不同。Nesterov动量中，先用当前的速度临时更新一遍参数，在用更新的临时参数计算梯度。</p>
<p>在momentum更新梯度时加入对当前梯度的校正，让梯度“多走一步”，可能跳出局部最优解：
<span class="math display">\[
w_t^*=\beta m_{t-1}+w_t
\]</span></p>
<p><span class="math display">\[
m_t=\beta m_{t-1}+(1-\beta)\cdot g_t
\]</span></p>
<p><span class="math display">\[
w_{t+1}=w_t-\eta \cdot m_t
\]</span></p>
<p>这里的<span class="math inline">\(g_t\)</span>用临时点<span
class="math inline">\(w_t^*\)</span>计算的</p>
<h2 id="更新学习率方法">更新学习率方法</h2>
<h3 id="adagrad">Adagrad</h3>
<p>引入<strong>二阶动量</strong>，根据训练轮数的不同，对学习率进行了动态调整：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307213914026.png"
alt="image-20230307213914026" />
<figcaption aria-hidden="true">image-20230307213914026</figcaption>
</figure>
<p><strong>缺点</strong>：仍然需要人为指定一个合适的全局学习率，同时网络训练到一定轮次后，分母上梯度累加过大使得学习率为0而导致训练提前结束。</p>
<h3 id="adadelta不是很懂">Adadelta(不是很懂)</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307215135905.png"
alt="image-20230307215135905" />
<figcaption aria-hidden="true">image-20230307215135905</figcaption>
</figure>
<h3 id="rmsprop">RMSProp</h3>
<p>AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSprop算法对Adagrad算法做了一点小小的修改，RMSprop使用指数衰减只保留过去给定窗口大小的梯度，使其能够在找到凸碗状结构后快速收敛。RMSProp法可以视为Adadelta法的一个特例，即依然使用全局学习率替换掉Adadelta法中的<span
class="math inline">\(s_t\)</span>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307215341546.png"
alt="image-20230307215341546" />
<figcaption aria-hidden="true">image-20230307215341546</figcaption>
</figure>
<p>推荐<span
class="math inline">\(\eta_{global}=1,\rho=0.9,\epsilon=10^{-6}\)</span></p>
<p>缺点：依然使用了全局学习率，需要根据实际情况来设定 优点：</p>
<ol type="1">
<li>分母不再是一味的增加，它会重点考虑距离它较近的梯度（指数衰减的效果）</li>
<li>只用了部分梯度加和而不是所有，这样避免了梯度累加过大使得学习率为0而导致训练提前结束。</li>
</ol>
<h3 id="adam">Adam</h3>
<p>https://zhuanlan.zhihu.com/p/377968342</p>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
</search>
