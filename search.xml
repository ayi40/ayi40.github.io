<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>算法</title>
    <url>/2023/02/14/Algorithm_example/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="动态规划">动态规划</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">有一排深渊法师，他们的护盾值用数组 a 表示；</span><br><span class="line">你可以对深渊法师这样破盾：</span><br><span class="line">1) 用重击破盾，每消耗1MP破1点护盾</span><br><span class="line">2) 如果有两个相邻的深渊法师他们的元素不同，且护盾都不为0，你可以对他们俩使用高天之歌，消耗xMP破所有护盾</span><br><span class="line">问最少消耗的MP？</span><br><span class="line">下面输入的 els 表示深渊法师的元素，I 表示冰，W 表示水，F 表示火</span><br><span class="line"></span><br><span class="line">输入：</span><br><span class="line">a = [4, 8, 10, 2, 15, 2]</span><br><span class="line">x = 9</span><br><span class="line">els = WIFFII</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i]=min(dp[i-1]+a[i],dp[i-2]+x) if 能使用高天之歌 else dp[i-1]+a[i]</span><br></pre></td></tr></table></figure>
<h1 id="找规律题目">找规律题目</h1>
<h2 id="找规律后拼接">找规律后拼接</h2>
<h3 id="构建长度为n的字符串">构建长度为n的字符串</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A希望你构造一个长度为n的数组，满足以下条件：</span><br><span class="line">1. 所有元素绝对值不大于3</span><br><span class="line">2. 相邻两个元素乘积小于0，且和不为0</span><br><span class="line">3. 所有元素之和=0</span><br><span class="line">input: 2 output:no answer</span><br><span class="line">input: 3 output: -1 2 -1</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">根据题目信息：</span><br><span class="line">1. 数组的元素在&#123;1,2,3&#125;里挑选</span><br><span class="line">2. 数组相邻元素一个为正一个为负，且互不相等，正号组和负号组可以互换。</span><br><span class="line">3. 正负号数值和相同，且可拼接！！！因为0+0=0</span><br><span class="line"></span><br><span class="line">考虑拼接条件：只要首尾元素不相同就可以拼接！！</span><br><span class="line"></span><br><span class="line">开始找规律以及能用于拼接的元素：</span><br><span class="line">input: 2 output:no answer</span><br><span class="line">input: 3 output: -1 3 -2（这个可以用于拼接，首位元素不同） -1 2 -1（这个不可以）</span><br><span class="line">input: 4 output: -1 2 -3 2（这个可以用于拼接，首位元素不同）</span><br><span class="line">input: 5 output: no answer</span><br><span class="line">我们发现所以大于5的数都可以由若干个3和若干个4相加得到，且3、4中有答案可以随意拼接，所以我们能利用3、4的答案拼接出n&gt;5的满足条件的数组</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="拼接mhy字符串">拼接mhy字符串</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">如果不能定义：字符串的“权重”是字符串中出现的字符种类数量，比如 mmm 权重为1，mmh 为 2，mhy 为 3</span><br><span class="line">输入：x, y, z 三个变量</span><br><span class="line">输出：一个长度为 x + y + z + 2 的字符串，这个字符串只能由 m h y 三种字符组成，这个字符串一共有 x + y + z 个长度为 3 的子串，其中有 x 个权重为 1 的子串，y 个权重为 2 的子串，z 个权重为 3 的子串。</span><br><span class="line">只用输出一种情况，若无法组成，输出-1</span><br><span class="line"></span><br><span class="line">范例输入：x = 2, y = 1, z = 1</span><br><span class="line">范例输出：mmmmhy</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 找规律，发现权重3字符串后只能接权重2或权重3的字符串，不能跟权重1；所以如果x,z&gt;0,y=0这种情况无解</span><br><span class="line">2. 我们可以先拼接权重3字符串（若有）mhymhy...</span><br><span class="line">3. 然后拼接权重2，mhymhyhyhy...；如果无权重三直接拼接权重2,hyhyh....</span><br><span class="line">4. 只剩下一个权重2没拼时，拼一个可以接权重1的权重2，mhymhyhyhyy</span><br><span class="line">5. 拼接权重1：mhymhyhyhyyyyyyy</span><br></pre></td></tr></table></figure>
<h2 id="找规律后计算">找规律后计算</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A拿到了3元无限长字符串&#123;1,2,3；4,5,6；7,8,9；....&#125;</span><br><span class="line">其中3的倍数后用；分割，其他用逗号</span><br><span class="line">求l个字符到r个字符之间有几个逗号和分号。</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 求l到r字符之间有几个逗号分号——》r前逗号分号-l前逗号分号</span><br><span class="line">2. 难点：数字为1-9时，每个数字只占1个字符，为10-99，每个数字就占两个字符了。</span><br><span class="line">	开始找规律！</span><br><span class="line">	1-9			每6个字符为一组&#123;1,2,3;&#125;			这样的组有3个</span><br><span class="line">	10-99		每9个字符为一组&#123;10,11,12;&#125;			这样的组有30个</span><br><span class="line">	100-999		每12个字符为一组&#123;100,111,112;&#125;		这样的组有300个</span><br><span class="line">	以此类推</span><br><span class="line">	</span><br></pre></td></tr></table></figure>
<h1 id="审题题">审题题</h1>
<h2 id="交换字母使字典序尽可能大">交换字母使字典序尽可能大</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">A拿到一个仅有小写字母组成的字符串，她准备进行恰好一次操作：交换连个相邻字母，在操作结束后使字符串的字典序尽可能大。</span><br><span class="line"></span><br><span class="line">input: ba</span><br><span class="line">output: ab</span><br><span class="line">2&lt;=len(input)&lt;=200000</span><br></pre></td></tr></table></figure>
<p>知识点：</p>
<ol type="1">
<li><p>字典序，就是按照字典排列顺序，英文字母按下面方式排列：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ABCDEFG HIJKLMN OPQRST UVWXYZ</span><br><span class="line">abcdefg hijklmn opqrst uvwxyz</span><br></pre></td></tr></table></figure></li>
<li><p>题目说的是<strong>恰好一次</strong>操作！！！！就算交换之后会让原来字符串字典序减小也需要进行操作！</p></li>
</ol>
<h1 id="寻找用例">寻找用例</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。</span><br><span class="line"></span><br><span class="line">计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。</span><br><span class="line"></span><br><span class="line">你可以认为每种硬币的数量是无限的。</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><p>找正例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：coins = [1, 2, 5], amount = 11</span><br><span class="line">输出：3 </span><br></pre></td></tr></table></figure></li>
<li><p>找负例</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：coins = [2], amount = 3</span><br><span class="line">输出：-1</span><br></pre></td></tr></table></figure></li>
<li><p>找边界条件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：coins = [1], amount = 0</span><br><span class="line">输出：0</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>DHT</title>
    <url>/2023/03/20/DHT/</url>
    <content><![CDATA[<p>DHT, which transforms the edges of a graph into the nodes of a
hypergraph.</p>
<p>ENGNN, use hypergraph after DHT to propagation</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>Before methods only capture edge information implicitly, e.g. used as
weight.</p>
<h1 id="contribute">Contribute</h1>
<ol type="1">
<li>propose DHT, Dual Hypergraph Transformation</li>
<li>propose a novel edge representation learning scheme ENGNN by using
DHT.</li>
<li>propose novel edge pooling methods.</li>
</ol>
<h1 id="method">Method</h1>
<h2 id="dht-how-to-transfer-graph-to-hypergraph">DHT： how to transfer
graph to hypergraph</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230320171020430.png"
alt="image-20230320171020430" />
<figcaption aria-hidden="true">image-20230320171020430</figcaption>
</figure>
<h3 id="step1-get-origin-graph-representation">Step1: Get origin graph
representation</h3>
<p>Firstly, we get the initial node feature and edge feature. <span
class="math display">\[
node \space  feature: X\in R^{n\times d}
\]</span></p>
<p><span class="math display">\[
edge\space feature: E\in R^{m\times d&#39;}
\]</span></p>
<p>Than we use an incidence matrix M rather than an adjacency matrix to
represent graph structure. <span class="math display">\[
incidence\space matrix: M\in \{0,1\}^{n\times m}
\]</span> So the origin graph is <span class="math display">\[
G=(X,M,E)
\]</span></p>
<h3 id="step-2-use-dht-to-get-hypergraph-g">Step 2: Use DHT to get
hypergraph <span class="math inline">\(G^*\)</span></h3>
<p>The hypergraph represent <span class="math display">\[
G^*=(X^*,M^*,E^*)
\]</span></p>
<p><span class="math display">\[
X^*=E
\]</span></p>
<p><span class="math display">\[
M^*=M^T
\]</span></p>
<p><span class="math display">\[
E^*=X
\]</span></p>
<p><span class="math display">\[
DHT:G=(X,M,E)-&gt;G^*=(E,M^T,X)
\]</span></p>
<p>While DHT is a bijective transformation: <span
class="math display">\[
DHT:G^*=(E,M^T,X)-&gt;G=(X,M,E)
\]</span></p>
<h2
id="ehgnn-an-edge-representation-learning-framework-using-dht">EHGNN: an
edge representation learning framework using DHT</h2>
<p><span class="math display">\[
E^{(l+1)}=ENGNN(X^{(l)},M,E^{(l)})=GNN(DHT(X^{(l)},M,E^{(l)}))
\]</span></p>
<p>So ENGNN consists of DHT and GNN, while GNN can be any GNN
function.</p>
<p>After ENGNN, EHGNN, <span class="math inline">\(E^{(L)}\)</span> is
returned to the original graph by applying DHT to dual hypergraph <span
class="math inline">\(G^∗\)</span>. Then, the remaining step is how to
make use of these edge-wise representations to finish the task.</p>
<h2 id="pooling">Pooling</h2>
<p>To be continue...</p>
<h1 id="advantage">Advantage</h1>
<h2 id="dht">DHT</h2>
<ol type="1">
<li>low time complexity</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230320165122363.png"
alt="image-20230320165122363" />
<figcaption aria-hidden="true">image-20230320165122363</figcaption>
</figure>
]]></content>
      <categories>
        <category>GNN</category>
        <category>EdgeLearning</category>
      </categories>
  </entry>
  <entry>
    <title>GeneralReading</title>
    <url>/2023/03/29/GeneralReading/</url>
    <content><![CDATA[<p>MKR,RKGE,HAGERec,entity2rec</p>
<span id="more"></span>
<h1 id="mkr">MKR</h1>
<h2 id="framework">Framework</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230329182212440.png"
alt="image-20230329182212440" />
<figcaption aria-hidden="true">image-20230329182212440</figcaption>
</figure>
<p>The framework of MKR is illustrated in Figure 1a.</p>
<p>MKR consists of three main components: recommendation module, KGE
module, and cross&amp;compress units.</p>
<ol type="1">
<li><p>The recommendation module on the left takes a user and an item as
input, and uses a multi-layer perceptron (MLP) and cross&amp;compress
units to extract short and dense features for the user and the item,
respectively. The extracted features are then fed into another MLP
together to output the predicted probability.</p></li>
<li><p>Similar to the left part, the KGE module in the right part also
uses multiple layers to extract features from the head and relation of a
knowledge triple, and outputs the representation of the predicted tail
under the supervision of a score function f and the real tail.</p></li>
<li><p>The recommendation module and the KGE module are bridged by
specially designed cross&amp;compress units. The proposed unit can
automatically learn high-order feature interactions of items in
recommender systems and entities in the knowledge graph.</p></li>
</ol>
<p>u: MLP to update</p>
<p>v: cross&amp;compress units</p>
<p>r: MLP to update</p>
<p>h: cross&amp;compress units</p>
<h2 id="loss-function">Loss function</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230329184018259.png"
alt="image-20230329184018259" />
<figcaption aria-hidden="true">image-20230329184018259</figcaption>
</figure>
<h1 id="rkge">RKGE</h1>
<p>RKGE first automatically mines all qualified paths between entity
pairs from the KG, which are then encoded via a batch of recurrent
networks, with each path modeled by a single recurrent network.</p>
<p>It then employs a pooling operation to discriminate the importance of
different paths for characterizing user preferences towards items.</p>
<h2 id="framework-1">framework</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230329205324109.png"
alt="image-20230329205324109" />
<figcaption aria-hidden="true">image-20230329205324109</figcaption>
</figure>
<h3 id="semantic-path-mining">Semantic Path Mining</h3>
<p>Strategy</p>
<ol type="1">
<li>We only consider user-to-item paths <span
class="math inline">\(P(u_i,v_j)\)</span> that connect <span
class="math inline">\(u, i\)</span> with all her rated items.</li>
<li>We enumerate paths with a length constraint.</li>
</ol>
<h3 id="encode-path">Encode path</h3>
<p>use recurrent networks</p>
<h4 id="embedding-layer">Embedding layer</h4>
<p>generate the embedding of entities</p>
<h4 id="attention-gated-hidden-layer">Attention-Gated Hidden Layer</h4>
<p>就是一个RNN网络的变种</p>
<h1 id="hagerec">HAGERec</h1>
<h2 id="framework-2">Framework</h2>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230330191130895.png"
alt="image-20230330191130895" />
<figcaption aria-hidden="true">image-20230330191130895</figcaption>
</figure>
<p>four components:</p>
<ol type="1">
<li><p>Flatten and embedding layer: flatten complex high-order relations
and embedding the entities and relations as vectors.</p></li>
<li><p>GCN learning layer: uses GCN model to propagate and update user’s
and item’s embedding via a bi-directional entity propagation
strategy</p></li>
<li><p>Interaction signals unit: preserves interaction signals structure
of an entity and its neighbor network to give a more complete picture
for user’s and item’s representation.</p></li>
<li><p>Prediction layer: utilizes the user’s and item’s aggregated
representation with prediction-level attention to output the predicted
score.</p></li>
</ol>
<h3 id="flatten-and-embedding">Flatten and embedding</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330193624700.png"
alt="image-20230330193624700" />
<figcaption aria-hidden="true">image-20230330193624700</figcaption>
</figure>
<p>flatten high-order connection to the path: <span
class="math inline">\(u\rightarrow^{r1}v\rightarrow^{r2}e_{u1}\rightarrow^{r3}e_{v3}\)</span></p>
<p>embedding: initialized embedding vectors.</p>
<h3 id="gcn-learning-unit">GCN learning unit</h3>
<p>user and item use the same propagation and aggregate strategy.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330200154040.png"
alt="image-20230330200154040" />
<figcaption aria-hidden="true">image-20230330200154040</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330200204043.png"
alt="image-20230330200204043" />
<figcaption aria-hidden="true">image-20230330200204043</figcaption>
</figure>
<p><span class="math inline">\(h^T, W, b\)</span>: learned
parameters</p>
<p>neighbor sample(only get fixed number neighbor): <span
class="math inline">\(\alpha_{e_v,e_{nv}}\)</span> would be regarded as
the similarity of each neighbor entity and central entity. Through this
evidence, those neighbors with lower similarity would be filtered.</p>
<h3 id="interaction-signals-unit">Interaction signals unit</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330201306937.png"
alt="image-20230330201306937" />
<figcaption aria-hidden="true">image-20230330201306937</figcaption>
</figure>
<p>区别：上面是相加，下面事相乘</p>
<p>so GCN unit + interaction unit =</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330201407720.png"
alt="image-20230330201407720" />
<figcaption aria-hidden="true">image-20230330201407720</figcaption>
</figure>
<h3 id="predict">Predict</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230330201630216.png"
alt="image-20230330201630216" />
<figcaption aria-hidden="true">image-20230330201630216</figcaption>
</figure>
<h1 id="entity2rec">Entity2rec</h1>
<h2 id="framework-3">Framework</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331170104462.png"
alt="image-20230331170104462" />
<figcaption aria-hidden="true">image-20230331170104462</figcaption>
</figure>
<h3 id="node2vec">node2vec</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331165454743.png"
alt="image-20230331165454743" />
<figcaption aria-hidden="true">image-20230331165454743</figcaption>
</figure>
<p>将图用random walk转化为word格式，用词袋模型计算vector。</p>
<h3 id="property-specific-knowledge-graph-embedding">Property-specific
knowledge graph embedding</h3>
<p>在node2vec基础上加上relation embedding，基于p子图在p空间上优化node
vector</p>
<p>maximize the dot product between vectors of the same neighborhood</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331170246301.png"
alt="image-20230331170246301" />
<figcaption aria-hidden="true">image-20230331170246301</figcaption>
</figure>
<p>Ze-negative sampling</p>
<p>N(e): neighbor of entity</p>
<h3 id="subgraph">subgraph</h3>
<h4 id="collaborative-content-subgraphs">Collaborative-content
subgraphs</h4>
<p>只保留单一relation，但连接性很差，对random walk效果不好</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331170405382.png"
alt="image-20230331170405382" />
<figcaption aria-hidden="true">image-20230331170405382</figcaption>
</figure>
<p>所有子图可以分成两张类型：feedback子图(user-item图)和其他子图</p>
<p>用下面方法来计算推荐分数：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331170819790.png"
alt="image-20230331170819790" />
<figcaption aria-hidden="true">image-20230331170819790</figcaption>
</figure>
<p>R+(u) denotes a set of items liked by the user u in the past.</p>
<p>s(x): similarity socre</p>
<h4 id="hybrid-subgraphs">Hybrid subgraphs</h4>
<p><span class="math inline">\(K_p^+=K_p \cup(u,feedback,i)\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331171032188.png"
alt="image-20230331171032188" />
<figcaption aria-hidden="true">image-20230331171032188</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230331171139039.png"
alt="image-20230331171139039" />
<figcaption aria-hidden="true">image-20230331171139039</figcaption>
</figure>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>KG-review</title>
    <url>/2023/03/03/KG-Rec/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<iframe height="800" width="60%" src="https://xmind.works/share/JMcp4hCx" frameborder="0" allowfullscreen>
</iframe>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>KGAT</title>
    <url>/2023/02/24/KGAT/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>利用KG作为辅助信息，并将KG与user-item graph 整合为一个图</p>
<h2 id="background-1">Background</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224155340260.png"
alt="image-20230224155340260" />
<figcaption aria-hidden="true">image-20230224155340260</figcaption>
</figure>
<p><strong>Previous model:</strong></p>
<p>CF: behaviorally similar users would exhibit similar preference on
items.</p>
<p><strong>focus on the histories of similar users who also watched
<span class="math inline">\(i1\)</span>, i.e., <span
class="math inline">\(u4\)</span> and <span
class="math inline">\(u5\)</span>;</strong></p>
<p>SL: transform side information into a generic feature vector,
together with user ID and item ID, and feed them into a supervised
learning (SL) model to predict the score.</p>
<p><strong>emphasize the similar items with the attribute <span
class="math inline">\(e1\)</span>, i.e.$ i2$.</strong></p>
<p><strong>current problem:</strong></p>
<p>existing SL methods fail to unify them, and ignore other
relationships in the graph:</p>
<ol type="1">
<li>the users in the yellow circle who watched other movies directed by
the same person <span class="math inline">\(e_1\)</span>.</li>
<li>the items in the grey circle that share other common relations with
<span class="math inline">\(e_1\)</span>.</li>
</ol>
<h2 id="user-item-bipartite-graph-g_1">User-Item Bipartite Graph: <span
class="math inline">\(G_1\)</span></h2>
<p><span class="math display">\[
\{(u,y_{ui},i)|u\in U, i\in I\}
\]</span> <span class="math inline">\(U\)</span>: user sets</p>
<p><span class="math inline">\(I\)</span>: item sets</p>
<p><span class="math inline">\(y_{ui}\)</span>: if user <span
class="math inline">\(u\)</span> interacts with item <span
class="math inline">\(i\)</span> <span
class="math inline">\(y_{ui}\)</span>=, else <span
class="math inline">\(y_{ui}\)</span>=0.</p>
<h2 id="knowledge-graph-g2">Knowledge Graph <span
class="math inline">\(G2\)</span></h2>
<p><span class="math display">\[
\{(h,r,t)|h,t\in E, r\in R\}
\]</span></p>
<p><span class="math inline">\(t\)</span> there is a relationship <span
class="math inline">\(r\)</span> from head entity <em>h</em> to tail
entity <span class="math inline">\(t\)</span>.</p>
<h2 id="ckg-combination-of-g1-and-g2"><span
class="math inline">\(CKG\)</span>: Combination of <span
class="math inline">\(G1\)</span> and <span
class="math inline">\(G2\)</span></h2>
<ol type="1">
<li>represent each user-item behavior as a triplet $ (u,
Interact,i)<span class="math inline">\(, where\)</span> y^{ui}$ =
1.</li>
<li>we establish a set of item-entity alignments</li>
</ol>
<p><span class="math display">\[
A = \{(i, e)|i ∈ I, e ∈ E \}
\]</span></p>
<ol start="3" type="1">
<li>based on the item-entity alignment set, the user-item graph can be
integrated with KG as a unified graph.</li>
</ol>
<p><span class="math display">\[
G = \{(h,r,t)|h,t ∈ E^′,r ∈R^′\}
\]</span></p>
<p><span class="math display">\[
E^′ = E ∪ U
\]</span></p>
<p><span class="math display">\[
R^′ = R ∪ {Interact}
\]</span></p>
<h1 id="methodology">Methodology</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222185609261.png"
alt="image-20230222185609261" />
<figcaption aria-hidden="true">image-20230222185609261</figcaption>
</figure>
<p>KGAT has three main components:</p>
<ol type="1">
<li>Embedding layer</li>
<li>Attentive embedding propagation layer</li>
<li>prediction layer</li>
</ol>
<h2 id="embedding-layer">Embedding layer</h2>
<p>Using <strong>TransR</strong> to calculate embedding</p>
<p><strong>Assumption</strong>: if a triplet (h,r,t) exist in the graph,
<span class="math display">\[
e^r_h+e_r\approx e_t^r
\]</span> Herein, <span class="math inline">\(e^h\)</span>, <span
class="math inline">\(e^t\)</span> ∈ <span
class="math inline">\(R^d\)</span> and <span
class="math inline">\(e^r\)</span> ∈ <span
class="math inline">\(R^k\)</span>are the embedding for <em>h</em>,
<em>t</em>, and <em>r</em>; and <span
class="math inline">\(e^r_h\)</span>, <span
class="math inline">\(e^r_t\)</span> are the projected representations
of <span class="math inline">\(e^h\)</span>, <span
class="math inline">\(e^t\)</span> in the relation <em>r</em>’s
space.</p>
<p><strong>Plausibility score</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222193700417.png"
alt="image-20230222193700417" />
<figcaption aria-hidden="true">image-20230222193700417</figcaption>
</figure>
<p><span class="math inline">\(W_r ∈ R^{k\times d}\)</span> is the
transformation matrix of relation <em>r</em>, which projects entities
from the <em>d</em>-dimension entity space into the <em>k</em> dimension
relation space.</p>
<p>A lower score suggests that the triplet is more likely to be
true.</p>
<p><strong>Loss</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222195306105.png"
alt="image-20230222195306105" />
<figcaption aria-hidden="true">image-20230222195306105</figcaption>
</figure>
<p><span class="math inline">\(\{(h,r,t,t^′ )|(h,r,t) \in G, (h,r,t^′ )
\notin G\}\)</span>, <span class="math inline">\((h,r,t^′ )\)</span> is
a negative sample constructed by replacing one entity in a valid triplet
randomly.</p>
<p><em>σ</em>(·): sigmoid function, ——》将分数映射再0-1区间，归一化</p>
<p>？？？？？？？？？？？why this layer model working as a
regularizer</p>
<h2 id="attentive-embedding-propagation-layersupon-gcn">Attentive
Embedding Propagation Layers(upon GCN)</h2>
<h3 id="first-order-propagation">First-order propagation</h3>
<p>和之前模型不同，这个的propagation layer encode了<span
class="math inline">\(e_r\)</span>.</p>
<p>For entity h, the information propagating from neighbor is :</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222201217039.png"
alt="image-20230222201217039" />
<figcaption aria-hidden="true">image-20230222201217039</figcaption>
</figure>
<p><span class="math inline">\(π(h,r,t)\)</span>: to controls the decay
factor on each propagation on edge (<em>h</em>,<em>r</em>,<em>t</em>),
indicating how much information is propagated</p>
<p>from <em>t</em> to <em>h</em> conditioned to relation <em>r</em>.</p>
<p>For <span class="math inline">\(π(h,r,t)\)</span>, we use attention
mechanism:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222204949423.png"
alt="image-20230222204949423" />
<figcaption aria-hidden="true">image-20230222204949423</figcaption>
</figure>
<p>This makes the attention score dependent on the distance between
<span class="math inline">\(e^h\)</span> and <span
class="math inline">\(e^t\)</span> in the relation <em>r</em>’s
space.</p>
<p>这里，tanh用于增加非线性因素；但不缺定是否有归一化作用？？？？？归一化就可以把这个function的大小集中在角度上，但是这样<span
class="math inline">\(e^h_t\)</span>也没有归一化，到时候看看输出参数</p>
<p>and than use softmax to normalize(no need to use as<span
class="math inline">\(\frac1{|N_t |}\)</span><span
class="math inline">\(\frac1{|N_t ||N_h |}\)</span>)</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222234256082.png"
alt="image-20230222234256082" />
<figcaption aria-hidden="true">image-20230222234256082</figcaption>
</figure>
<p>The final part is aggregation, threre are three choices:</p>
<ol type="1">
<li>GCN aggregator</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222235616598.png"
alt="image-20230222235616598" />
<figcaption aria-hidden="true">image-20230222235616598</figcaption>
</figure>
<ol start="2" type="1">
<li>GraphSage aggregator</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222235806956.png"
alt="image-20230222235806956" />
<figcaption aria-hidden="true">image-20230222235806956</figcaption>
</figure>
<ol start="3" type="1">
<li>Bi-Interaction aggregator</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223000647293.png"
alt="image-20230223000647293" />
<figcaption aria-hidden="true">image-20230223000647293</figcaption>
</figure>
<h3 id="multi-layer-propagation">Multi-layer propagation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223000834658.png"
alt="image-20230223000834658" />
<figcaption aria-hidden="true">image-20230223000834658</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223000850960.png"
alt="image-20230223000850960" />
<figcaption aria-hidden="true">image-20230223000850960</figcaption>
</figure>
<h2 id="model-prediction">Model Prediction</h2>
<p>multi-layers combination and inner product</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223001515690.png"
alt="image-20230223001515690" />
<figcaption aria-hidden="true">image-20230223001515690</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223001526127.png"
alt="image-20230223001526127" />
<figcaption aria-hidden="true">image-20230223001526127</figcaption>
</figure>
<h2 id="optimizazion">Optimizazion</h2>
<h3 id="loss">loss</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223002144083.png"
alt="image-20230223002144083" />
<figcaption aria-hidden="true">image-20230223002144083</figcaption>
</figure>
<p><span class="math inline">\(L_{cf}\)</span> is BPR Loss</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223002439536.png"
alt="image-20230223002439536" />
<figcaption aria-hidden="true">image-20230223002439536</figcaption>
</figure>
<p><span class="math inline">\(L_{kg}\)</span> is loss forTranR .</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230222195306105.png"
alt="image-20230222195306105" />
<figcaption aria-hidden="true">image-20230222195306105</figcaption>
</figure>
<h3 id="optimizer">Optimizer</h3>
<p>Adam</p>
<h3 id="updata-method">updata method</h3>
<p>we update the embeddings for all nodes;</p>
<p>hereafter, we sample a batch of (<em>u</em>,<em>i</em>, <em>j</em>)
randomly, retrieve their representations after <em>L</em> steps of
propagation, and then update model parameters by using the gradients of
the prediction loss.</p>
<p>在同一个epoch中，先把所以数据扔进tranR训练，得到loss（此时不更新参数）</p>
<p>然后sample算BPR LOSS</p>
<h1 id="experiments">EXPERIMENTS</h1>
<h2 id="rq1-performance-comparison">RQ1: Performance Comparison</h2>
<ol type="1">
<li>regular dataset</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223004843914.png"
alt="image-20230223004843914" />
<figcaption aria-hidden="true">image-20230223004843914</figcaption>
</figure>
<ol start="2" type="1">
<li><p>Sparsity Levels</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223005253034.png"
alt="image-20230223005253034" />
<figcaption aria-hidden="true">image-20230223005253034</figcaption>
</figure></li>
</ol>
<p>KGAT outperforms the other models in most cases, especially on the
two sparsest user groups.</p>
<p>说明KGAT能够缓解稀疏性影响</p>
<h2 id="rq2study-of-kgat">RQ2：Study of KGAT</h2>
<ol type="1">
<li>study of layer influence and effect of aggregators</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223010038345.png"
alt="image-20230223010038345" />
<figcaption aria-hidden="true">image-20230223010038345</figcaption>
</figure>
<ol start="2" type="1">
<li><p>cut attention layer and TransR layer</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230223010347815.png"
alt="image-20230223010347815" />
<figcaption aria-hidden="true">image-20230223010347815</figcaption>
</figure></li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>KGCN</title>
    <url>/2023/03/02/KGCN/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<h2 id="cf-questions">CF Questions</h2>
<ol type="1">
<li>sparsity</li>
<li>cold start</li>
</ol>
<h2 id="kg-benefits">KG Benefits</h2>
<ol type="1">
<li>The rich semantic relatedness among items in a KG can help explore
their latent connections and improve the <em>precision</em> of
results;</li>
<li>The various types of relations in a KG are helpful for extending a
user’s interests reasonably and increasing the <em>diversity</em> of
recommended items;</li>
<li>KG connects a user’s historically-liked and recommended items,
thereby bringing <em>explainability</em> to recommender systems.</li>
</ol>
<h2 id="previous-kg-method">Previous KG Method</h2>
<h3 id="knowledge-graph-embedding">Knowledge graph embedding</h3>
<p>Example: TransE [1] and TransR [12] assume <em>head</em> +
<em>relation</em> = <em>tail</em>, which focus on modeling rigorous
semantic relatedness</p>
<p>Problem: KGE methods are more suitable for in-graph applications such
as KG completion and link prediction rather than the recommendation
system.</p>
<h3 id="path-base-method">Path-base Method</h3>
<p>Example: PER, FMG</p>
<p>problem: Labor sensitivity</p>
<h2 id="ripple-net">Ripple Net</h2>
<p>problem:</p>
<ol type="1">
<li>the importance of relations is weakly characterized in RippleNet,
because the relation <strong>R</strong> can hardly be trained to capture
the sense of importance in the quadratic form <strong>v</strong>
⊤<strong>Rh</strong> (<strong>v</strong> and <strong>h</strong> are
embedding vectors of two entities).</li>
<li>The size of ripple set may go unpredictably with the increase of the
size of KG, which incurs heavy computation and storage overhead.</li>
</ol>
<h2 id="solution-kgcn">Solution: KGCN</h2>
<ol type="1">
<li>Propagation and aggregation mechanism.</li>
<li>Attention mechanism.</li>
<li>sample a fixed-size neighborhood to control compute cost.</li>
</ol>
<h1 id="model">Model</h1>
<h2 id="single-layer">Single layer</h2>
<p>Consider a pair(u,v)</p>
<h3 id="overall-of-single-layer">Overall of single layer</h3>
<p>!!!Propagation only use for updating of item's vector</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114017500.png"
alt="image-20230302114017500" />
<figcaption aria-hidden="true">image-20230302114017500</figcaption>
</figure>
<h3 id="propagation">Propagation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302111400335.png"
alt="image-20230302111400335" />
<figcaption aria-hidden="true">image-20230302111400335</figcaption>
</figure>
<p><span class="math inline">\(N(v)\)</span> is the neighbor set of
v</p>
<p>e$ is the embedding of entity(parameter to train)</p>
<p><span class="math inline">\(\pi^u_{r_{v,e}}\)</span> is attention
weight</p>
<p><span class="math inline">\(r_{v,e}\)</span> represent the relation
of v and e</p>
<h3 id="attention">Attention</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302112401231.png"
alt="image-20230302112401231" />
<figcaption aria-hidden="true">image-20230302112401231</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302112315690.png"
alt="image-20230302112315690" />
<figcaption aria-hidden="true">image-20230302112315690</figcaption>
</figure>
<p><span class="math inline">\(g : R^d ×R^d → R\)</span> (e.g., inner
product:内积能计算相似度)to compute the score between a user and a
relation</p>
<p><span class="math inline">\(u\in R\)</span>, <span
class="math inline">\(r\in R\)</span> : embedding of user and
relation(parameter to train)</p>
<p><span class="math inline">\(\pi^u_{r_{v,e}}\)</span>characterizes the
importance of relation <em>r</em> to user <em>u</em>. E.g. example, a
user may have more interests in the movies that share the same “star"
with his historically liked ones, while another user may be more
concerned about the “genre" of movies.</p>
<p>!!!!!!!!!!个性化！！！用户对不同关系重视程度不同！！</p>
<p>所以KGCN不用propagation更新用户的原因是否是因为希望user的embedding能专注于提取个性化信息（提高用户和重要relation的相似度），但是这样是否会让user和item没那么好聚类？</p>
<h3 id="sample-the-number-of-neighbors">Sample the number of
neighbors</h3>
<p>limit the neighbor number in K(can be config)</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302113734945.png"
alt="image-20230302113734945" />
<figcaption aria-hidden="true">image-20230302113734945</figcaption>
</figure>
<p>S(<em>v</em>) is also called the (single layer) <em>receptive
field</em> of entity <em>v</em></p>
<p>example K=2:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302113907523.png"
alt="image-20230302113907523" />
<figcaption aria-hidden="true">image-20230302113907523</figcaption>
</figure>
<h3 id="aggregation">aggregation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114047762.png"
alt="image-20230302114047762" />
<figcaption aria-hidden="true">image-20230302114047762</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114057869.png"
alt="image-20230302114057869" />
<figcaption aria-hidden="true">image-20230302114057869</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114106975.png"
alt="image-20230302114106975" />
<figcaption aria-hidden="true">image-20230302114106975</figcaption>
</figure>
<p><span class="math inline">\(\sigma\)</span> is ReLU</p>
<h2 id="multi-layer">Multi layer</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/9C6A6E4346910DDDD43EBB55E1A2FE6C.png"
alt="9C6A6E4346910DDDD43EBB55E1A2FE6C" />
<figcaption
aria-hidden="true">9C6A6E4346910DDDD43EBB55E1A2FE6C</figcaption>
</figure>
<p>First we consider the Receptive-Field:</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/1739B546615C1AEDC69F7A3514E2E458.png" alt="1739B546615C1AEDC69F7A3514E2E458" style="zoom:67%;" /></p>
<p>We first update eneities in M[0] by using propagation and aggregation
to get the high-hop neighbor information.</p>
<p>And then gradually narrow it down, and finally focus on v.</p>
<p>Note that we have only one user in one pair, every relations will
calculate the score with this user embedding</p>
<h2 id="predict">Predict</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302114437063.png"
alt="image-20230302114437063" />
<figcaption aria-hidden="true">image-20230302114437063</figcaption>
</figure>
<h2 id="loss-function">Loss function</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302120154981.png"
alt="image-20230302120154981" />
<figcaption aria-hidden="true">image-20230302120154981</figcaption>
</figure>
<p><span class="math inline">\(J\)</span> is cross-entropy loss</p>
<p><em>P</em> is a negative sampling distribution, and <span
class="math inline">\(T_u\)</span> is the number of negative samples for
user <em>u</em>. In this paper,</p>
<h1 id="experiment">Experiment</h1>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>LightGCN</title>
    <url>/2023/02/24/LightGCN/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>question: why concentrated to sum</p>
<h2 id="main-contributes">Main contributes</h2>
<ol type="1">
<li><p>We empirically show that two common designs in GCN, feature
transformation and nonlinear activation, have no positive effect on the
effectiveness of collaborative filtering.</p>
<p>GCN is originally proposed for node classification on the attributed
graph, where each node has rich attributes as input features; whereas in
the user-item interaction graph for CF, each node (user or item) is only
described by a one-hot ID, which has no concrete semantics besides being
an identifier.</p></li>
<li><p>Propose LightGCN.</p></li>
</ol>
<h1 id="analyze-about-ngcf">Analyze about NGCF</h1>
<h2 id="brief">Brief</h2>
<p>完全想不起来的话建议先看NGCF的笔记</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213192715250.png"
alt="image-20230213192715250" />
<figcaption aria-hidden="true">image-20230213192715250</figcaption>
</figure>
<p>## Some experiment</p>
<h3 id="method">Method</h3>
<p>Using ablation studies, implement three simplified variants of
NGCF:</p>
<ol type="1">
<li>NGCF-f: which removes the feature transformation matrices <span
class="math inline">\(W1\)</span> and <span
class="math inline">\(W2\)</span>.</li>
<li>NGCF-n: which removes the non-linear activation function $ σ$.</li>
<li>NGCF-fn: which removes both the feature transformation matrices and
non-linear activation function.</li>
</ol>
<p><strong>Note</strong>: Since the core of GCN is to refine embeddings
by propagation, we are more interested in the embedding quality under
the same embedding size. Thus, we change the way of obtaining final
embedding from concatenation (i.e., <span
class="math inline">\(e_u^*=e_u^{(0)}\|e_u^{(1)}\|...\|e_u^{(L)}\)</span>)
to sum(i.e., <span
class="math inline">\(e_u^*=e_u^{(0)}+e_u^{(1)}+...+e_u^{(L)}\)</span>).</p>
<p>This change has little effect on NGCF’s performance but makes the
following ablation studies more indicative of the embedding quality
refined by GCN.</p>
<h3 id="result">Result</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213193937873.png"
alt="image-20230213193937873" />
<figcaption aria-hidden="true">image-20230213193937873</figcaption>
</figure>
<ol type="1">
<li>Adding feature transformation imposes negative effect on NGCF, since
removing it in both models of NGCF and NGCF-n improves the performance
significantly;</li>
<li>Adding nonlinear activation affects slightly when feature
transformation is included, but it imposes negative effect when feature
transformation is disabled.</li>
<li>As a whole, feature transformation and nonlinear activation impose
rather negative effect on NGCF, since by removing them simultaneously,
NGCF-fn demonstrates large improvements over NGCF.</li>
</ol>
<h3 id="conclusion">Conclusion</h3>
<p>The deterioration of NGCF stems from the training
difficulty(underfitting), rather than overfitting, because:</p>
<ol type="1">
<li><p>Such lower training loss of NGCF-fn successfully transfers to
better recommendation accuracy.</p></li>
<li><p>NGCF is more powerful and complex, but it demonstrates higher
training loss and worse generalization performance than NGCF-f.</p></li>
</ol>
<h1 id="model-of-lightgcn">Model of LightGCN</h1>
<p>Consisting four parts:</p>
<ol type="1">
<li>initialize users and items embedding.</li>
<li>Light Graph Convolution (LGC)</li>
<li>Layer Combination</li>
<li>Model Prediction</li>
</ol>
<h2 id="light-graph-convolution-lgc">Light Graph Convolution (LGC)</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213194939594.png"
alt="image-20230213194939594" />
<figcaption aria-hidden="true">image-20230213194939594</figcaption>
</figure>
<p>$ $： symmetric normalization, which can avoid the scale of
embeddings increasing with graph convolution operations. Here can use
other normalization, but symmetric normalization has good
performance.</p>
<p><strong>Note</strong>: Without self-connection, because the layer
combination operation of LightGCN captures the same effect as
self-connections.</p>
<h2 id="layer-combination">Layer Combination</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213195607414.png"
alt="image-20230213195607414" />
<figcaption aria-hidden="true">image-20230213195607414</figcaption>
</figure>
<p><span class="math inline">\(α_k\)</span>can be treated as a
hyperparameter to be tuned manually, or as a model parameter, and
setting <span class="math inline">\(α_k\)</span> uniformly as <span
class="math inline">\(1/(K + 1)\)</span> leads to good performance in
general.</p>
<p>This is probably because the training data does not contain
sufficient signal to learn good α that can generalize to unknown
data.</p>
<p>The reason of using the Layer Combination:</p>
<ol type="1">
<li>With the increasing of the number of layers, the embeddings will be
over-smoothed [27]. Thus simply using the last layer is
problematic.</li>
<li>The embeddings at different layers capture different semantics.</li>
<li>Combining embeddings at different layers with weighted sum captures
the effect of graph convolution with self-connections, an important
trick in GCNs.</li>
</ol>
<h2 id="model-prediction">Model Prediction</h2>
<p>inner product</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213200915280.png"
alt="image-20230213200915280" />
<figcaption aria-hidden="true">image-20230213200915280</figcaption>
</figure>
<h2 id="matrix-form">Matrix form</h2>
<p>Similar to NGCF, and there are some explanations in detail in NGCF
note.</p>
<p>Light Graph Convolution:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201201896.png"
alt="image-20230213201201896" />
<figcaption aria-hidden="true">image-20230213201201896</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201107785.png"
alt="image-20230213201107785" />
<figcaption aria-hidden="true">image-20230213201107785</figcaption>
</figure>
<p>Layer combination:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201221159.png"
alt="image-20230213201221159" />
<figcaption aria-hidden="true">image-20230213201221159</figcaption>
</figure>
<h1 id="analyze-about-lightgcn">Analyze about LightGCN</h1>
<h2 id="relation-with-sgcn">Relation with SGCN</h2>
<p><strong>Purpose</strong>: by doing layer combination, LightGCN
subsumes the effect of self-connection thus there is no need for
LightGCN to add self-connection in adjacency matrix.</p>
<p>SGCN: a recent linear GCN model that integrates self-connection into
graph convolution.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213201725744.png"
alt="image-20230213201725744" />
<figcaption aria-hidden="true">image-20230213201725744</figcaption>
</figure>
<p>In the following analysis, we omit the <span class="math inline">\((D
+ I)^{-\frac{1}{2}}\)</span> terms for simplicity, since they only
re-scale embeddings.</p>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230213212117430.png"
alt="image-20230213212117430" />
<figcaption aria-hidden="true">image-20230213212117430</figcaption>
</figure>
<p>The above derivation shows that, inserting self-connection into A and
propagating embeddings on it, is essentially equivalent to a weighted
sum of the embeddings propagated at each LGC layer.</p>
<p>because <span class="math inline">\(AE^{(0)}=E^{(1)}\)</span>...<span
class="math inline">\(A^KE^{(0)}=E^{(K)}\)</span></p>
<h2 id="relation-with-appnp">Relation with APPNP</h2>
<p><strong>Purpose</strong>: shows the underlying equivalence between
LightGCN and APPNP, thus our LightGCN enjoys the sames benefits in
propagating long-range with controllable overs-moothing.</p>
<p>APPNP: a recent GCN variant that addresses over-smoothing. APPNP
complements each propagation layer with the starting features.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213212642134.png"
alt="image-20230213212642134" />
<figcaption aria-hidden="true">image-20230213212642134</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213212845456.png"
alt="image-20230213212845456" />
<figcaption aria-hidden="true">image-20230213212845456</figcaption>
</figure>
<p>also equivalent to a weighted sum of the embeddings propagated at
each LGC layer.</p>
<h2 id="second-order-embedding-smoothness">Second-Order Embedding
Smoothness</h2>
<p><strong>Purpose</strong>: providing more insights into the working
mechanism of LightGCN.</p>
<p>below is influence from2-order neighbor to target node.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213213500081.png"
alt="image-20230213213500081" />
<figcaption aria-hidden="true">image-20230213213500081</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213213521206.png"
alt="image-20230213213521206" />
<figcaption aria-hidden="true">image-20230213213521206</figcaption>
</figure>
<p><strong>conclusion</strong>: the influence of a second-order neighbor
v on u is determined by</p>
<ol type="1">
<li>the number of co-interacted items, the more the larger.</li>
<li>the popularity of the co-interacted items, the less popularity
(i.e., more indicative of user personalized preference) the larger</li>
<li>the activity of v, the less active the larger.</li>
</ol>
<h1 id="model-train">Model Train</h1>
<h2 id="loss-function">Loss function</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230213213949666.png"
alt="image-20230213213949666" />
<figcaption aria-hidden="true">image-20230213213949666</figcaption>
</figure>
<h2 id="optimizer-adam">Optimizer: Adam</h2>
<h2 id="no-dropout-strategy">No dropout strategy</h2>
<p>The reason is that we do not have feature transformation weight
matrices in LightGCN, thus enforcing L2 regularization on the embedding
layer is sufficient to prevent overfitting.</p>
<h1 id="experiment">Experiment</h1>
<h2 id="compared-with-ngcf">compared with NGCF</h2>
<ol type="1">
<li>LightGCN performs better than NGCF and NGCF-fn, as NGCF-fn still
contains more useless operations than LightGCN.</li>
<li>Increasing the number of layers can improve performance, but the
benefits diminish. Increasing the layer number from 0 to 1 leads to the
largest performance gain, and using a layer number of 3 leads to
satisfactory performance in most cases.</li>
<li>LightGCN consistently obtains lower training loss, which indicates
that LightGCN fits the training data better than NGCF. Moreover, the
lower training loss successfully transfers to better testing accuracy,
indicating the strong generalization power of LightGCN. In contrast, the
higher training loss and lower testing accuracy of NGCF reflect the
practical difficulty to train such a heavy model it well.</li>
</ol>
<h2 id="ablation-and-effectiveness-analyses">Ablation and Effectiveness
Analyses</h2>
<h3 id="impact-of-layer-combination">Impact of Layer Combination</h3>
<h4 id="using-models">Using models:</h4>
<ol type="1">
<li>LightGCN</li>
<li>LightGCN-single: does not use layer combination</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215151920378.png"
alt="image-20230215151920378" />
<figcaption aria-hidden="true">image-20230215151920378</figcaption>
</figure>
<h4 id="conclusion-1">Conclusion</h4>
<ol type="1">
<li>Focusing on LightGCN-single, we find that its performance first
improves and then drops when the layer number increases from 1 to 4.
This indicates that smoothing a node’s embedding with its first-order
and secondorder neighbors is very useful for CF, but will suffer from
oversmoothing issues when higher-order neighbors are used.</li>
<li>Focusing on LightGCN, we find that its performance gradually
improves with the increasing of layers even using 4 layers. This
justifies the effectiveness of layer combination for addressing
over-smoothing.</li>
<li>we find that LightGCN consistently outperforms LightGCN-single on
Gowalla, but not on AmazonBook and Yelp2018. There are two reason:
<ol type="1">
<li>LightGCN-single is special case of LightGCN that sets αK to 1 and
other αk to 0;</li>
<li>we do not tune the <span class="math inline">\(αk\)</span> and
simply set it as <span class="math inline">\(\frac{1}{K+1}\)</span>
uniformly for LightGCN.</li>
</ol></li>
</ol>
<h3 id="impact-of-symmetric-sqrt-normalization">Impact of Symmetric Sqrt
Normalization</h3>
<h4 id="setting">Setting:</h4>
<ol type="1">
<li>LightGCN-L: normalization only at the left side (i.e., the target
node’s coefficient).</li>
<li>LightGCN-R: the right side (i.e., the neighbor node’s
coefficient).</li>
<li>LightGCN-L1: use L1 normalization( i.e., removing the square
root).</li>
<li>LightGCN-L1-L: use L1 normalization only on the left side.</li>
<li>LightGCN-L1-R: use L1 normalization only on the right side.</li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215153734701.png"
alt="image-20230215153734701" />
<figcaption aria-hidden="true">image-20230215153734701</figcaption>
</figure>
<h4 id="conclusion-2">Conclusion</h4>
<ol type="1">
<li>The best setting in general is using sqrt normalization at both
sides (i.e., the current design of LightGCN). Removing either side will
drop the performance largely.</li>
<li>The second best setting is using L1 normalization at the left side
only (i.e., LightGCN-L1-L). This is equivalent to normalize the
adjacency matrix as a stochastic matrix by the
in-degree(norm后矩阵无对称性).</li>
<li>Normalizing symmetrically on two sides is helpful for the sqrt
normalization, but will degrade the performance of L1
normalization.</li>
</ol>
<h3 id="analysis-of-embedding-smoothness">Analysis of Embedding
Smoothness</h3>
<p><strong>Object</strong>: Making sure such
smoothing（有点像聚类的感觉） of embeddings is the key reason of
LightGCN’s effectiveness.</p>
<p><strong>Method</strong>: we first define the smoothness of user
embeddings as(用于衡量2-order
neighbor的embedding差别大小，是否合理聚类的感觉):</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215160123446.png"
alt="image-20230215160123446" />
<figcaption aria-hidden="true">image-20230215160123446</figcaption>
</figure>
<p>where the L2 norm on embeddings is used to eliminate the impact of
the embedding’s scale.</p>
<p><strong>result</strong>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215160328103.png"
alt="image-20230215160328103" />
<figcaption aria-hidden="true">image-20230215160328103</figcaption>
</figure>
<p><strong>Conclusion</strong>: the smoothness loss of LightGCN-single
is much lower than that of MF.</p>
<p>This indicates that by conducting light graph convolution, the
embeddings become smoother and more suitable for recommendation.</p>
<h2 id="hyper-parameter-studies">Hyper-parameter Studies</h2>
<p><strong>object</strong>: Ensure the L2 regularization coefficient
<span class="math inline">\(λ\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230215161009450.png"
alt="image-20230215161009450" />
<figcaption aria-hidden="true">image-20230215161009450</figcaption>
</figure>
<p><strong>Conclusion</strong>:</p>
<ol type="1">
<li>LightGCN is relatively insensitive to λ.</li>
<li>Even when λ sets to 0, LightGCN is better than NGCF, which
additionally uses dropout to prevent overfitting. This shows that
LightGCN is less prone to overfitting</li>
<li>When λ is larger than 1e−3, the performance drops quickly, which
indicates that too strong regularization will negatively affect model
normal training and is not encouraged.</li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title>NGCF</title>
    <url>/2023/02/24/NGCF/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p>question: example(the Laplacian)</p>
<h2 id="some-definition">Some Definition</h2>
<ol type="1">
<li><p>Recommendation system: Estimate how likely a user will adopt an
item based on the historical interaction like purchase and
click.</p></li>
<li><p>Collaborative filtering(CF): behaviorally similar users would
exhibit similar preference on items.</p>
<p>CF consists of</p>
<ol type="1">
<li><p>embedding: transforms users and items into vectorized
representations. e.g. matrix factorization(MF),deep learning
function...</p></li>
<li><p>interaction modeling: reconstructs historical interactions based
on the embeddings. e.g. inner product, neural function...</p></li>
</ol></li>
<li><p>collaborative signal: signal latent in user-item
interactions</p></li>
</ol>
<h2 id="existing-problem">Existing Problem</h2>
<p>The current embedding process of CF doesn't encode a collaborative
signal. Most of them focus on the descriptive feature(e.g. user id,
attributes). When the embeddings are insufficient in capturing CF, the
methods have to rely on the interaction function to make up for the
deficiency of suboptimal embeddings</p>
<h2 id="main-contribute">Main contribute</h2>
<ol type="1">
<li><p>Highlight the critical importance of explicitly exploiting the
collaborative signal in the embedding function of model-based CF
methods.</p></li>
<li><p>Propose NGCF, a new recommendation framework based on a graph
neural network, which explicitly encodes the collaborative signal in the
form of high-order connectivities by performing embedding
propagation.</p></li>
</ol>
<h1 id="model">Model</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211111222966.png"
alt="image-20230211111222966" />
<figcaption aria-hidden="true">image-20230211111222966</figcaption>
</figure>
<p>There are three components in the framework:</p>
<ol type="1">
<li>Embedding layer: offers and initialization of user embeddings and
item embeddings;</li>
<li>Multiple embedding propagation layers: refine the embeddings by
injecting high-order connectivity relations;</li>
<li>Prediction layer: aggregates the refined embeddings from different
propagation layers and outputs the affinity score of a user-item
pair.</li>
</ol>
<h2 id="embedding-layer">Embedding layer</h2>
<p>Just initializing user embeddings and item embeddings by using ID or
other features.</p>
<p>Get user embedding <span class="math inline">\(e_i\)</span> and item
embedding <span class="math inline">\(e_u\)</span>.</p>
<h2 id="multiple-embedding-propagation-layers">Multiple Embedding
Propagation Layers</h2>
<h3 id="one-layer-propagation">One layer propagation</h3>
<p>It consists of two parts: Message Construction and Message
aggregation.</p>
<h4 id="message-construction">Message Construction</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211112521161.png"
alt="image-20230211112521161" />
<figcaption aria-hidden="true">image-20230211112521161</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211111736136.png"
alt="image-20230211111736136" />
<figcaption aria-hidden="true">image-20230211111736136</figcaption>
</figure>
<p><span class="math inline">\(m_{u&lt;-i}\)</span>: the result of the
message construction module. It is a message embedding that will be used
to update the target node.</p>
<p><span class="math inline">\(e_i\)</span>: Embedding of neighbor
item.</p>
<p><strong>meaning</strong> : encode neighbor item's feature.</p>
<p><span class="math inline">\(e_i⊙e_u\)</span> : element-wise product
of <span class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_u\)</span>.</p>
<p><strong>meaning</strong>: encodes the interaction between <span
class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_u\)</span> into the message and makes the
message dependent on the affinity between <span
class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_j\)</span>.</p>
<p><span class="math inline">\(W_1\)</span>, <span
class="math inline">\(W_2\)</span>: trainable weight matrices， the
shape is (<span class="math inline">\(d&#39;\)</span>, <span
class="math inline">\(d\)</span>), while <span
class="math inline">\(d\)</span> is the size of the initial embedding,
<span class="math inline">\(d&#39;\)</span> is the size of
transformation size.</p>
<p><span class="math inline">\(P_{ui}\)</span>: to control the decay
factor on each propagation on edge (u, i). Here, we set <span
class="math inline">\(P_{ui}\)</span> as <strong>Laplacian norm</strong>
$ $, $ N_u$, $ N_i$ is the first-hot neighbors of user u and item i.
(就是拉普拉斯矩阵归一化！！<span
class="math inline">\(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\)</span>)</p>
<p><strong>meaning</strong> -From the viewpoint of representation
learning: <span class="math inline">\(P_{ui}\)</span> reflects how much
the historical item contributes to the user preference.</p>
<p>From the viewpoint of the message passing: <span
class="math inline">\(P_{ui}\)</span> can be interpreted as a discount
factor, considering the messages being propagated should decay with the
path length.</p>
<h4 id="message-aggregation">Message Aggregation</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230211151741633.png"
alt="image-20230211151741633" />
<figcaption aria-hidden="true">image-20230211151741633</figcaption>
</figure>
<p><span class="math inline">\(e_u^{(1)}\)</span>: the representation of
user u after 1 propagation layer.</p>
<p><span class="math inline">\(m_{u&lt;-u}\)</span>: self-connection of
u. Here is <span class="math inline">\(W1e_u\)</span>.</p>
<p><strong>meaning</strong>: retain information of original feature.</p>
<p><span class="math inline">\(m_{u&lt;-i}\)</span>： neighbor node
propagation.</p>
<h3 id="high-order-propagation">High-order propagation</h3>
<h4 id="formulate-form">Formulate Form</h4>
<p>By stacking l-embedding propagation layers, a user (and an item) is
capable of receiving the messages propagated from its l-hop neighbors.
The formulates are similar to one-layer propagation.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212105956664.png"
alt="image-20230212105956664" />
<figcaption aria-hidden="true">image-20230212105956664</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212110019741.png"
alt="image-20230212110019741" />
<figcaption aria-hidden="true">image-20230212110019741</figcaption>
</figure>
<h4 id="matrix-form">Matrix Form</h4>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212110725475.png"
alt="image-20230212110725475" />
<figcaption aria-hidden="true">image-20230212110725475</figcaption>
</figure>
<p><span class="math inline">\(E^{(l)}\)</span> : the representations
for users and items obtained after l-layers propagation. Shape is
(N+M,d)</p>
<p>L: Laplacian matrix for the user-item graph.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212111719667.png"
alt="image-20230212111719667" />
<figcaption aria-hidden="true">image-20230212111719667</figcaption>
</figure>
<p>D is the diagonal degree matrix. where <span
class="math inline">\(D_{tt}=\vert N_t\vert\)</span> meaning the
<code>D[t][t]</code> is the number of neighbors' node. The shape is
(N+M, N+M), because there are totally n+m node(including user and
item)</p>
<p>A is the adjacency matrix. The shape of R is (N, M), while the shape
of A is (N+M, N+M).</p>
<p>some extra knowledge: <a
href="https://zhuanlan.zhihu.com/p/362416124/">理解拉普拉斯矩阵</a></p>
<p>I: identity matrix</p>
<h5 id="a-simple-example-for-matrix-form">A simple example for matrix
form:</h5>
<p>Suppose we have 2 users (A, B), 3 items(C, D, E), N=2 and M=3.</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/D9B00E7DDF74FF18B83E42668335328A.png" alt="D9B00E7DDF74FF18B83E42668335328A" style="zoom: 25%;" /></p>
<p>Let consider this part: <span
class="math inline">\((L+I)E^{(l-1)}W^{(l)}\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/81DBE0096BF060771E3355F2E6A34151.png"
alt="81DBE0096BF060771E3355F2E6A34151" />
<figcaption
aria-hidden="true">81DBE0096BF060771E3355F2E6A34151</figcaption>
</figure>
<p>After calculating <span
class="math inline">\((L+I)E^{(l-1)}\)</span>, we get information on
self-connection and neighbor-propagation (after the Laplacian norm), and
then we can multiply the trainable parameter W1(MLP).</p>
<p>拉普拉斯矩阵归一化的不成熟小理解：</p>
<p>①target node由n个邻居点做贡献，为了避免邻居越多，target
node的value越大的情况，首先除<span
class="math inline">\(\frac{1}{\sqrt{N_n}}\)</span>,
大概也可以理解为邻居越多，每个邻居对其造成的影响越小</p>
<p>②只做一次norm影响对称性，所以为了保持对称性在做一次<span
class="math inline">\(\frac{1}{\sqrt{N_t}}\)</span>,可以理解为neighbor
node有多少邻居对他给到每个邻居的权重有影响，是否能理解为邻居越多说明这个node能提供的信息更普通没价值（例如所有用户购买了水，对推荐系统来说，水能提供的信息就没那么有用）</p>
<p>x class UV_Aggregator(nn.Module):    """   item and user aggregator:
for aggregating embeddings of neighbors (item/user aggreagator).   """​  
 def <strong>init</strong>(self, v2e, r2e, u2e, embed_dim, cuda="cpu",
uv=True):        ...​    def forward(self, nodes, history_uv, history_r):
       # create a container for result, shpe of embed_matrix is
(batchsize,embed_dim)        embed_matrix = torch.empty(len(history_uv),
self.embed_dim, dtype=torch.float).to(self.device)​        # deal with
each single item nodes' neighbors        for i in
range(len(history_uv)):            history = history_uv[i]          
 num_histroy_item = len(history)            tmp_label = history_r[i]​    
       # e_uv : turn neighbors(user node) id to embedding            #
uv_rep : turn single node(item node) to embedding            if self.uv
== True:                # user component                e_uv =
self.v2e.weight[history]                uv_rep =
self.u2e.weight[nodes[i]]            else:                # item
component                e_uv = self.u2e.weight[history]              
 uv_rep = self.v2e.weight[nodes[i]]​            # get rating score
embedding            e_r = self.r2e.weight[tmp_label]            #
concatenated rating and neighbor, and than through two layers mlp to get
fjt            x = torch.cat((e_uv, e_r), 1)            x =
F.relu(self.w_r1(x))​            o_history = F.relu(self.w_r2(x))        
   # calculate neighbor attention and fjt*weight to finish aggregation  
         att_w = self.att(o_history, uv_rep, num_histroy_item)          
 att_history = torch.mm(o_history.t(), att_w)            att_history =
att_history.t()​            embed_matrix[i] = att_history        # result
(batchsize, embed_dim)        to_feats = embed_matrix        return
to_featspython</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/8EE43A6D961CA0F0145CD44C62B9F9BE.png"
alt="8EE43A6D961CA0F0145CD44C62B9F9BE" />
<figcaption
aria-hidden="true">8EE43A6D961CA0F0145CD44C62B9F9BE</figcaption>
</figure>
<p>We get information on the interaction between <span
class="math inline">\(e_i\)</span> and <span
class="math inline">\(e_u\)</span> (after the Laplacian norm), and then
we can multiply the trainable parameter W2(MLP).</p>
<p>Add two parts and through LeakyRelu, we get user or item embedding
after l-layers propagation.</p>
<h2 id="model-prediction">Model Prediction</h2>
<p>Just concatenate all propagation layers' output embedding, and use
inner product to estimate the user's preference towards the target
item.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212173756733.png"
alt="image-20230212173756733" />
<figcaption aria-hidden="true">image-20230212173756733</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212173813003.png"
alt="image-20230212173813003" />
<figcaption aria-hidden="true">image-20230212173813003</figcaption>
</figure>
<h1 id="optimization">Optimization</h1>
<h2 id="loss">Loss</h2>
<p>BPR Loss: assumes that the observed interactions, which are more
reflective of a user’s preferences, should be assigned higher prediction
values than unobserved ones.</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212212248890.png"
alt="image-20230212212248890" />
<figcaption aria-hidden="true">image-20230212212248890</figcaption>
</figure>
<h2 id="optimizer-adam">Optimizer: Adam</h2>
<h2 id="model-size">Model Size</h2>
<p>In NGCF, only W1 and W2 in the propagation layer need to be trained,
so has <span class="math inline">\(2Ld_ld_{l-1}\)</span> more
parameters, while L is always smaller than 5 and <span
class="math inline">\(d\)</span> is set as the embedding size(e.g. 64)
which is also small.</p>
<h2 id="message-and-node-dropout">Message and Node Dropout</h2>
<ol type="1">
<li><p><strong>Message dropout</strong>: randomly drops out the outgoing
messages (equal to dropout edge).</p>
<p><strong>meaning</strong>: endows the representations more robustness
against the presence or absence of single connections between users and
items.</p>
<p><strong>example</strong>: For the <span
class="math inline">\(l-th\)</span> propagation layer, we drop out the
messages being propagated, with a probability <span
class="math inline">\(p1\)</span>.</p></li>
<li><p><strong>Node dropout</strong>: randomly blocks a particular node
and discards all its outgoing messages.</p>
<p><strong>meaning</strong>: focuses on reducing the influences of
particular users or items.</p>
<p><strong>example</strong>: For the <span
class="math inline">\(l-th\)</span> propagation layer, we randomly drop
<span class="math inline">\((M + N)p2\)</span> nodes of the Laplacian
matrix, where <span class="math inline">\(p2\)</span> is the dropout
ratio.</p></li>
</ol>
<p>区别：对于message
dropout，计算时node的邻居数、拉普拉斯norm都是正常的，就是更新embedding的时候遗漏了信息，作用是提高一下鲁棒性和容错性；对于Node
dropout，直接在拉普拉斯矩阵中屏蔽若干个node，可能影响临界点数、归一化数值等，在矩阵运算时候就有影响，作用是希望模型不要过于依赖某些特定邻接点，没了部分点依然能正常运行。</p>
<h1 id="experiment">Experiment</h1>
<h2 id="conclusions-from-comparing-with-other-models">Conclusions from
comparing with other models</h2>
<ol type="1">
<li>The inner product is insufficient to capture the complex relations
between users and items.</li>
<li>Nonlinear feature interactions between users and items are
important</li>
<li>Neighbor information can improve embedding learning, and using the
attention mechanism is better than using equal and heuristic
weight.</li>
<li>Considering high-order connectivity or neighbor is better than only
considering first-order neighbor.</li>
<li>that exploiting high-order connectivity greatly facilitates
representation learning for inactive users, as the collaborative signal
can be effectively captured. And the embedding propagation is beneficial
to relatively inactive users.</li>
</ol>
<h2 id="study-for-ngcf">Study for NGCF</h2>
<p>....</p>
<h2 id="effect-of-high-order-connectivity">Effect of High-order
Connectivity</h2>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230212225247958.png"
alt="image-20230212225247958" />
<figcaption aria-hidden="true">image-20230212225247958</figcaption>
</figure>
<ol type="1">
<li>the representations of NGCF-3 exhibit discernible clustering,
meaning that the points with the same colors (<em>i.e.,</em> the items
consumed by the same users) tend to form the clusters.</li>
<li>when stacking three embedding propagation layers, the embeddings of
their historical items tend to be closer. It qualitatively verifies that
the proposed embedding propagation layer is capable of injecting the
explicit collaborative signal (via NGCF-3) into the
representations.</li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
      </categories>
  </entry>
  <entry>
    <title>GraphRec</title>
    <url>/2023/02/24/Note_for_GraphRec/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="graphrec">GraphRec</h1>
<h1 id="graphrec-feature">GraphRec feature</h1>
<ol type="1">
<li><p>Can capture both interactions and opinions in user-item
graph.</p></li>
<li><p>Consider different strengths of social relations.</p></li>
<li><p>Use attention mechanism.</p></li>
</ol>
<h1 id="overall-architecture">Overall architecture</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/Snipaste_2023-02-02_15-10-34.png"
alt="Snipaste_2023-02-02_15-10-34" />
<figcaption aria-hidden="true">Snipaste_2023-02-02_15-10-34</figcaption>
</figure>
<h2 id="three-import-module">Three import module:</h2>
<ol type="1">
<li><p>User Modeling: used to compute User Latent Factor(vector
containing many useful information)</p></li>
<li><p>Item Modeling: used to compute Item Latent Factor.</p></li>
<li><p>Rating Prediction: used to predict the item which user would like
to interact with.</p></li>
</ol>
<h1 id="source-code-analyses">Source code analyses</h1>
<h2 id="data">Data</h2>
<h3 id="what-kind-of-datas-we-use"><strong>What kind of datas we
use?</strong></h3>
<ol type="1">
<li><p>User-Item graph: record interation(e.g. purchase) and
opinion(e.g. five star rating) between user and item</p></li>
<li><p>User-User social graph: relationship between user and
user</p></li>
</ol>
<h3 id="how-to-represent-these-datas-in-code"><strong>How to represent
these datas in code?</strong></h3>
<h4 id="user-item-graph">User-Item graph:</h4>
<ol type="1">
<li>history_u_lists, history_ur_lists: user's purchased history (item
set in training set), and his/her rating score (dict)</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history_u_list = &#123;</span><br><span class="line">    user_id1:[item_id1, item_id2, item_id3...],</span><br><span class="line">    user_id2:[item_id4...],</span><br><span class="line">&#125;</span><br><span class="line">history_ur_list = &#123;</span><br><span class="line">    user_id1:[rating_score_u1i1, rating_score_u1i2, rating_score_u1i3...],</span><br><span class="line">    user_id2:[rating_score_u2i4...],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">e.g.</span><br><span class="line">history_u_list = &#123;</span><br><span class="line">    <span class="number">681</span>: [<span class="number">0</span>, <span class="number">156</span>], </span><br><span class="line">    <span class="number">81</span>: [<span class="number">1</span>, <span class="number">41</span>, <span class="number">90</span>]&#125;</span><br><span class="line">history_ur_list = &#123;</span><br><span class="line">    <span class="number">681</span>: [<span class="number">5</span>,<span class="number">4</span>],</span><br><span class="line">    <span class="number">81</span>: [<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>]&#125;</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>history_v_lists, history_vr_lists: user set (in training set) who
have interacted with the item, and rating score (dict). Similar with
history_u_lists, history_ur_lists but key is item id and value is user
id.</li>
</ol>
<h4 id="user-user-socal-graph">User-User socal graph</h4>
<ol type="1">
<li>social_adj_lists: user's connected neighborhoods</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">social_adj_lists = &#123;</span><br><span class="line">    user_id1:[user_id2, user_id3, user_id4...],</span><br><span class="line">    user_id2:[user_id1...],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="other">other</h4>
<ol type="1">
<li>train_u, train_v, train_r: used for model training, one by one based
on index (user, item, rating)</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_u = [user_id1, user_id2,....]</span><br><span class="line">train_v = [item_id34, item_id1,...]</span><br><span class="line">train_r = [rating_socre_u1i34, rating_socre_u2i1]</span><br><span class="line"><span class="built_in">len</span>(train_u) = <span class="built_in">len</span>(train_v) = <span class="built_in">len</span>(train_r)</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li><p>test_u, test_v, test_r: similar with training datas</p></li>
<li><p>ratings_list: rating value from 0.5 to 4.0 (8 opinion embeddings)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;2.0: 0, 1.0: 1, 3.0: 2, 4.0: 3, 2.5: 4, 3.5: 5, 1.5: 6, 0.5: 7&#125;</span><br></pre></td></tr></table></figure></p></li>
</ol>
<h3 id="how-to-pre-process-data"><strong>How to pre-process
data?</strong></h3>
<p>use <code>torch.utils.data.TensorDataset</code> and
<code>torch.utils.data.DataLoader</code> generate
<code>training_dataset</code> and <code>testing_dataset</code> (user,
item, rating)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">support batchsize = <span class="number">5</span></span><br><span class="line">[tensor([<span class="number">409</span>,  <span class="number">88</span>, <span class="number">134</span>, <span class="number">298</span>, <span class="number">340</span>]),                             <span class="comment">#user id</span></span><br><span class="line">tensor([<span class="number">1221</span>,  <span class="number">761</span>,   <span class="number">39</span>,  <span class="number">145</span>,    <span class="number">0</span>]),                         <span class="comment">#item id</span></span><br><span class="line">tensor([<span class="number">1.0000</span>, <span class="number">2.0000</span>, <span class="number">3.5000</span>, <span class="number">0.5000</span>, <span class="number">1.5000</span>, <span class="number">3.5000</span>])        <span class="comment">#rating score</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h2 id="model">Model</h2>
<h3 id="init">Init</h3>
<p>Translate user_id, item_id and rating_id to low-dimension vector,
just random initize, the weight of embedding layers will be trained.</p>
<p>After translate we get</p>
<pre><code>qj-embedding of item vj, 
pi-embedding of user ui, 
er-embedding of rating.</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">u2e = nn.Embedding(num_users, embed_dim).to(device)</span><br><span class="line">v2e = nn.Embedding(num_items, embed_dim).to(device)</span><br><span class="line">r2e = nn.Embedding(num_ratings, embed_dim).to(device)</span><br><span class="line"><span class="built_in">print</span>(u2e, v2e, r2e)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;Output</span></span><br><span class="line"><span class="string">Embedding(705, 64) </span></span><br><span class="line"><span class="string">Embedding(1941, 64) </span></span><br><span class="line"><span class="string">Embedding(8, 64)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>So that, we can easily get embedding through U2e, V2e and r2e.</p>
<h3 id="overall-architecture-1">Overall architecture</h3>
<figure>
<img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/GraphRec.jpg"
alt="GraphRec" />
<figcaption aria-hidden="true">GraphRec</figcaption>
</figure>
<p>GraphRec consist of User Modeling, Item Modeling and Rating
Prediction. The forward code of GraphRec is as follow:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GraphRec</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_u, enc_v_history, r2e</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes_u, nodes_v</span>):</span><br><span class="line">        <span class="comment"># nodes_u : [128] 128(batchsize) user id</span></span><br><span class="line">        <span class="comment"># nodes_v : [128] 128(batchsize) item id</span></span><br><span class="line">        <span class="comment"># self.enc_u is the User Modeling part(including Item Aggregation and Social Aggregation )</span></span><br><span class="line">        <span class="comment"># self.enc_v_history is the Item Modeling part(User Aggregation)</span></span><br><span class="line">        embeds_u = self.enc_u(nodes_u)</span><br><span class="line">        embeds_v = self.enc_v_history(nodes_v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># After aggregation information, forward two layer MLP， and get the Latent vector of user and item</span></span><br><span class="line">        x_u = F.relu(self.bn1(self.w_ur1(embeds_u)))</span><br><span class="line">        x_u = F.dropout(x_u, training=self.training)</span><br><span class="line">        x_u = self.w_ur2(x_u)</span><br><span class="line">        x_v = F.relu(self.bn2(self.w_vr1(embeds_v)))</span><br><span class="line">        x_v = F.dropout(x_v, training=self.training)</span><br><span class="line">        x_v = self.w_vr2(x_v)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># concatenated user vector and item vector, use three layer MLP to predict</span></span><br><span class="line">        x_uv = torch.cat((x_u, x_v), <span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.bn3(self.w_uv1(x_uv)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = F.relu(self.bn4(self.w_uv2(x)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        scores = self.w_uv3(x)</span><br><span class="line">        <span class="keyword">return</span> scores.squeeze()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, nodes_u, nodes_v, labels_list</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>full code of GraphRec class</p>
<details>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GraphRec</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_u, enc_v_history, r2e</span>):</span><br><span class="line">        <span class="built_in">super</span>(GraphRec, self).__init__()</span><br><span class="line">        self.enc_u = enc_u</span><br><span class="line">        self.enc_v_history = enc_v_history</span><br><span class="line">        self.embed_dim = enc_u.embed_dim</span><br><span class="line"></span><br><span class="line">        self.w_ur1 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_ur2 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_vr1 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_vr2 = nn.Linear(self.embed_dim, self.embed_dim)</span><br><span class="line">        self.w_uv1 = nn.Linear(self.embed_dim * <span class="number">2</span>, self.embed_dim)</span><br><span class="line">        self.w_uv2 = nn.Linear(self.embed_dim, <span class="number">16</span>)</span><br><span class="line">        self.w_uv3 = nn.Linear(<span class="number">16</span>, <span class="number">1</span>)</span><br><span class="line">        self.r2e = r2e</span><br><span class="line">        self.bn1 = nn.BatchNorm1d(self.embed_dim, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(self.embed_dim, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(self.embed_dim, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.bn4 = nn.BatchNorm1d(<span class="number">16</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line">        self.criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes_u, nodes_v</span>):</span><br><span class="line">        embeds_u = self.enc_u(nodes_u)</span><br><span class="line">        embeds_v = self.enc_v_history(nodes_v)</span><br><span class="line"></span><br><span class="line">        x_u = F.relu(self.bn1(self.w_ur1(embeds_u)))</span><br><span class="line">        x_u = F.dropout(x_u, training=self.training)</span><br><span class="line">        x_u = self.w_ur2(x_u)</span><br><span class="line">        x_v = F.relu(self.bn2(self.w_vr1(embeds_v)))</span><br><span class="line">        x_v = F.dropout(x_v, training=self.training)</span><br><span class="line">        x_v = self.w_vr2(x_v)</span><br><span class="line"></span><br><span class="line">        x_uv = torch.cat((x_u, x_v), <span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.bn3(self.w_uv1(x_uv)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = F.relu(self.bn4(self.w_uv2(x)))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        scores = self.w_uv3(x)</span><br><span class="line">        <span class="keyword">return</span> scores.squeeze()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, nodes_u, nodes_v, labels_list</span>):</span><br><span class="line">        scores = self.forward(nodes_u, nodes_v)</span><br><span class="line">        <span class="keyword">return</span> self.criterion(scores, labels_list)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</details>
<h3 id="user-modeling">User Modeling</h3>
<p>It contain Item Aggregation and Social Aggregation</p>
<p>在这里本质上是先做了一层Item
Aggregation之后，用得到的结果再做一层Social Aggregation 所以这里的Item
Aggregation，本质上是Social Aggregation中的self-connection</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Social_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, social_adj_lists, aggregator, base_model=<span class="literal">None</span>, cuda=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># to_neighs is a list which element is list recording social neighbor node, and len(list) is batchsize,</span></span><br><span class="line">        to_neighs = []</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            to_neighs.append(self.social_adj_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Social aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, to_neighs)  <span class="comment"># user-user network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Item aggregation</span></span><br><span class="line">        self_feats = self.features(torch.LongTensor(nodes.cpu().numpy())).to(self.device)</span><br><span class="line">        self_feats = self_feats.t()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        <span class="comment"># Concatenate Item Aggregation and Social Aggregation, and through one layer MLP</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<p>full code of User Modeling</p>
<details>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Social_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, social_adj_lists, aggregator, base_model=<span class="literal">None</span>, cuda=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Social_Encoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.features = features</span><br><span class="line">        self.social_adj_lists = social_adj_lists</span><br><span class="line">        self.aggregator = aggregator</span><br><span class="line">        <span class="keyword">if</span> base_model != <span class="literal">None</span>:</span><br><span class="line">            self.base_model = base_model</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.device = cuda</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">2</span> * self.embed_dim, self.embed_dim)  <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># to_neighs is a list which element is list recording social neighbor node, and len(list) is batchsize,</span></span><br><span class="line">        to_neighs = []</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            to_neighs.append(self.social_adj_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Item aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, to_neighs)  <span class="comment"># user-user network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Social aggregation</span></span><br><span class="line">        self_feats = self.features(torch.LongTensor(nodes.cpu().numpy())).to(self.device)</span><br><span class="line">        self_feats = self_feats.t()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        <span class="comment"># Concatenate Item Aggregation and Social Aggregation, and through one layer MLP</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
</details>
<h4 id="item-aggregation">Item Aggregation</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, history_uv_lists, history_r_lists, aggregator, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        tmp_history_uv = []</span><br><span class="line">        tmp_history_r = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">#get nodes(batch) neighbors</span></span><br><span class="line">        <span class="comment">#tmp_history_uv is a list which len is 128,while it&#x27;s element is also a list meaning that the each node&#x27;s(in batch) neighbor item id list</span></span><br><span class="line">        <span class="comment">#tmp_history_r is similar with tmp_history_uv, but record the rating score instead of item id</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            tmp_history_uv.append(self.history_uv_lists[<span class="built_in">int</span>(node)])</span><br><span class="line">            tmp_history_r.append(self.history_r_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># after neigh aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  <span class="comment"># user-item network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># id to embedding (features : u2e)</span></span><br><span class="line">        self_feats = self.features.weight[nodes]</span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<p>And the <code>self.aggregator</code> in neigh aggregation is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Aggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    item and user aggregator: for aggregating embeddings of neighbors (item/user aggreagator).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, v2e, r2e, u2e, embed_dim, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, history_uv, history_r</span>):</span><br><span class="line">        <span class="comment"># create a container for result, shpe of embed_matrix is (batchsize,embed_dim)</span></span><br><span class="line">        embed_matrix = torch.empty(<span class="built_in">len</span>(history_uv), self.embed_dim, dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># deal with each single nodes&#x27; neighbors</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(history_uv)):</span><br><span class="line">            history = history_uv[i]</span><br><span class="line">            num_histroy_item = <span class="built_in">len</span>(history)</span><br><span class="line">            tmp_label = history_r[i]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># e_uv : turn neighbors id to embedding</span></span><br><span class="line">            <span class="comment"># uv_rep : turn single node to embedding</span></span><br><span class="line">            <span class="keyword">if</span> self.uv == <span class="literal">True</span>:</span><br><span class="line">                <span class="comment"># user component</span></span><br><span class="line">                e_uv = self.v2e.weight[history]</span><br><span class="line">                uv_rep = self.u2e.weight[nodes[i]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># item component</span></span><br><span class="line">                e_uv = self.u2e.weight[history]</span><br><span class="line">                uv_rep = self.v2e.weight[nodes[i]]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># get rating score embedding</span></span><br><span class="line">            e_r = self.r2e.weight[tmp_label]</span><br><span class="line">            <span class="comment"># concatenated rating and neighbor, and than through two layers mlp to get xia</span></span><br><span class="line">            x = torch.cat((e_uv, e_r), <span class="number">1</span>)</span><br><span class="line">            x = F.relu(self.w_r1(x))</span><br><span class="line"></span><br><span class="line">            o_history = F.relu(self.w_r2(x))</span><br><span class="line">            <span class="comment"># calculate neighbor attention and xia*weight to finish aggregation</span></span><br><span class="line">            att_w = self.att(o_history, uv_rep, num_histroy_item)</span><br><span class="line">            att_history = torch.mm(o_history.t(), att_w)</span><br><span class="line">            att_history = att_history.t()</span><br><span class="line"></span><br><span class="line">            embed_matrix[i] = att_history</span><br><span class="line">        <span class="comment"># result (batchsize, embed_dim)</span></span><br><span class="line">        to_feats = embed_matrix</span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
<p>While <code>self.att</code> is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dims</span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, node1, u_rep, num_neighs</span>):</span><br><span class="line">        <span class="comment"># pi</span></span><br><span class="line">        uv_reps = u_rep.repeat(num_neighs, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># concatenated neighbot and pi</span></span><br><span class="line">        x = torch.cat((node1, uv_reps), <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># through 3 layers MLP</span></span><br><span class="line">        x = F.relu(self.att1(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = F.relu(self.att2(x))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.att3(x)</span><br><span class="line">        <span class="comment"># get weights</span></span><br><span class="line">        att = F.softmax(x, dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> att</span><br></pre></td></tr></table></figure>
<h4 id="social-aggregation">Social Aggregation</h4>
<p>use the result of Item Aggregation and pi as input</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Social_Aggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Social Aggregator: for aggregating embeddings of social neighbors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, u2e, embed_dim, cuda=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, to_neighs</span>):</span><br><span class="line">        <span class="comment">#return a uninitialize matrix as result container, which shape is (batchsize, embed_dim)</span></span><br><span class="line">        embed_matrix = torch.empty(<span class="built_in">len</span>(nodes), self.embed_dim, dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nodes)):</span><br><span class="line">            <span class="comment"># get social graph neighbor</span></span><br><span class="line">            tmp_adj = to_neighs[i]</span><br><span class="line">            num_neighs = <span class="built_in">len</span>(tmp_adj)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># fase : can use user embedding instead of result of item aggregation to improve speed</span></span><br><span class="line">            <span class="comment"># e_u = self.u2e.weight[list(tmp_adj)] # fast: user embedding </span></span><br><span class="line">            <span class="comment"># slow: item-space user latent factor (item aggregation)</span></span><br><span class="line">            feature_neigbhors = self.features(torch.LongTensor(<span class="built_in">list</span>(tmp_adj)).to(self.device))</span><br><span class="line">            e_u = torch.t(feature_neigbhors)</span><br><span class="line"></span><br><span class="line">            u_rep = self.u2e.weight[nodes[i]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># concatenated node embedding and neigbor vector (result of item aggregation) </span></span><br><span class="line">            <span class="comment"># and than through MLPs and Softmax to calculate weights</span></span><br><span class="line">            att_w = self.att(e_u, u_rep, num_neighs)</span><br><span class="line">            <span class="comment"># weight*neighbor vector</span></span><br><span class="line">            att_history = torch.mm(e_u.t(), att_w).t()</span><br><span class="line">            embed_matrix[i] = att_history</span><br><span class="line">        to_feats = embed_matrix</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
<h3 id="item-modeling">Item Modeling</h3>
<p>Similar with the Item Aggregation of User Modeling</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Encoder</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, embed_dim, history_uv_lists, history_r_lists, aggregator, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        tmp_history_uv = []</span><br><span class="line">        tmp_history_r = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">#get nodes(batch) neighbors of item</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            tmp_history_uv.append(self.history_uv_lists[<span class="built_in">int</span>(node)])</span><br><span class="line">            tmp_history_r.append(self.history_r_lists[<span class="built_in">int</span>(node)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># after neigh aggregation</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  <span class="comment"># user-item network</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># id to embedding (features : v2e)</span></span><br><span class="line">        self_feats = self.features.weight[nodes]</span><br><span class="line">        <span class="comment"># self-connection could be considered.</span></span><br><span class="line">        combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        combined = F.relu(self.linear1(combined))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<p>And the <code>self.aggregator</code> in neigh aggregation is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UV_Aggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    item and user aggregator: for aggregating embeddings of neighbors (item/user aggreagator).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, v2e, r2e, u2e, embed_dim, cuda=<span class="string">&quot;cpu&quot;</span>, uv=<span class="literal">True</span></span>):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, history_uv, history_r</span>):</span><br><span class="line">        <span class="comment"># create a container for result, shpe of embed_matrix is (batchsize,embed_dim)</span></span><br><span class="line">        embed_matrix = torch.empty(<span class="built_in">len</span>(history_uv), self.embed_dim, dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># deal with each single item nodes&#x27; neighbors</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(history_uv)):</span><br><span class="line">            history = history_uv[i]</span><br><span class="line">            num_histroy_item = <span class="built_in">len</span>(history)</span><br><span class="line">            tmp_label = history_r[i]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># e_uv : turn neighbors(user node) id to embedding</span></span><br><span class="line">            <span class="comment"># uv_rep : turn single node(item node) to embedding</span></span><br><span class="line">            <span class="keyword">if</span> self.uv == <span class="literal">True</span>:</span><br><span class="line">                <span class="comment"># user component</span></span><br><span class="line">                e_uv = self.v2e.weight[history]</span><br><span class="line">                uv_rep = self.u2e.weight[nodes[i]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># item component</span></span><br><span class="line">                e_uv = self.u2e.weight[history]</span><br><span class="line">                uv_rep = self.v2e.weight[nodes[i]]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># get rating score embedding</span></span><br><span class="line">            e_r = self.r2e.weight[tmp_label]</span><br><span class="line">            <span class="comment"># concatenated rating and neighbor, and than through two layers mlp to get fjt</span></span><br><span class="line">            x = torch.cat((e_uv, e_r), <span class="number">1</span>)</span><br><span class="line">            x = F.relu(self.w_r1(x))</span><br><span class="line"></span><br><span class="line">            o_history = F.relu(self.w_r2(x))</span><br><span class="line">            <span class="comment"># calculate neighbor attention and fjt*weight to finish aggregation</span></span><br><span class="line">            att_w = self.att(o_history, uv_rep, num_histroy_item)</span><br><span class="line">            att_history = torch.mm(o_history.t(), att_w)</span><br><span class="line">            att_history = att_history.t()</span><br><span class="line"></span><br><span class="line">            embed_matrix[i] = att_history</span><br><span class="line">        <span class="comment"># result (batchsize, embed_dim)</span></span><br><span class="line">        to_feats = embed_matrix</span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>SocialRec</category>
      </categories>
  </entry>
  <entry>
    <title>王树森推荐系统公开课</title>
    <url>/2023/03/13/Recommendation-WangShusen/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="基本概念">基本概念</h1>
<h2 id="指标">指标</h2>
<h3 id="消费指标">消费指标</h3>
<p>点击率=点击次数/曝光次数</p>
<p>点赞量=点赞次数/点击次数</p>
<p>收藏率=收藏次数/点击次数</p>
<p>转发率=转发次数/点击次数</p>
<p>阅读完成率=滑动到底次数/点击次数<span class="math inline">\(\times
f(笔记长度)\)</span></p>
<h3 id="北极星指标">北极星指标</h3>
<p>用户规模：日活用户数（DAU），月活用户数（MAU）</p>
<p>消费：人均使用推荐时长、人均阅读笔记数量</p>
<p>发布： 发布渗透率、人均发布量</p>
<h2 id="推荐系统链路">推荐系统链路</h2>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230313220835272.png"
alt="image-20230313220835272" />
<figcaption aria-hidden="true">image-20230313220835272</figcaption>
</figure>
<ol type="1">
<li>召回：快速从海量数据中取回几千个用户可能感兴趣的物品。</li>
<li>粗排：用小规模的模型的神经网络给召回的物品打分，然后做截断，选出分数最高的几百个物品。</li>
<li>精排：
用大规模神经网络给粗排选中的几百个物品打分，可以做截断，也可以不做截断。</li>
<li>重排：
对精排结果做多样性抽样，得到几十个物品，然后用规则调整物品的排序。</li>
</ol>
<h2 id="实验流程">实验流程</h2>
<h3
id="概要03推荐系统的ab测试没仔细看以后补">概要03推荐系统的AB测试：没仔细看以后补</h3>
<h1 id="召回">召回</h1>
<h2 id="基于物品的协同过滤-itemcf">基于物品的协同过滤 ItemCF</h2>
<p>基本思想：如果用户喜欢item1,而item1与item2相似，那么用户很可能喜欢item2.</p>
<h3 id="基本结构">基本结构：</h3>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230313223949470.png"
alt="image-20230313223949470" />
<figcaption aria-hidden="true">image-20230313223949470</figcaption>
</figure>
<p>我们从用户历史互动知道用户对<span
class="math inline">\(item_j\)</span>，感兴趣利用下面公式计算对候选物品的兴趣分数
<span class="math display">\[
\sum_jlike(user,item_j)\times sim(item_j,item)
\]</span> 在这个例子中，用户对候选item的兴趣是：<span
class="math inline">\(2\times
0.1+1\times0.4+4\times0.2+3\times0.6=3.2\)</span>,我们计算所有item的分数，然后返回分数最高的若干个item</p>
<h4 id="计算item相似度">计算item相似度</h4>
<p>可以通过与item交互过的用户重合度计算item相似度（其中一种方法，也可以用KG）</p>
<ol type="1">
<li>方法1：不考虑用户对物品的喜欢程度</li>
</ol>
<p><span class="math display">\[
sim(i_1,i_2) = \frac{|W1 \cap W2|}{\sqrt[2]{|W1|\cdot |W2|}}
\]</span></p>
<p>其中，喜欢物品<span class="math inline">\(i_1\)</span>的用户记作<span
class="math inline">\(W_1\)</span>,喜欢物品<span
class="math inline">\(i_2\)</span>的用户记作<span
class="math inline">\(W_2\)</span>.</p>
<ol start="2" type="1">
<li><p>方法2： 考虑用户对物品的喜欢程度,使用余弦相似度！</p>
<p>把每个item用向量表示 <span class="math display">\[
i_1=[like(u_1,i_1),like(u_2,i_1),\cdots ,like(u_n,i_1)] \space u_n\in W
\]</span></p>
<p><span class="math display">\[
i_2=[like(u_1,i_2),like(u_2,i_2),\cdots ,like(u_n,i_2)] \space u_n\in W
\]</span></p>
<p><span class="math display">\[
W=W_1\cup W_2
\]</span></p>
<p>我们使用余弦相似度计算： <span class="math display">\[
similarity=cos(\theta) = \frac{A\cdot B}{||A||\space ||B||}
\]</span> 如果有用户k只喜欢其中一个物品:只喜欢<span
class="math inline">\(i_1\)</span>不喜欢<span
class="math inline">\(i_2\)</span>,那么<span
class="math inline">\(i_2[k]=0\)</span>，所以点乘后第k项为0，所以点乘只与同时喜欢<span
class="math inline">\(i_1,i_2\)</span>的用户有关系，如下面公式 <span
class="math display">\[
sim(i_1,i_2) = \frac{\sum_{v\in V}like(v,i_i)\cdot
like(v,i_2)}{\sqrt[2]{\sum_{u_1\in
W_1}like^2(u_1,i_1)}\sqrt[2]{\sum_{u_2\in W_2}like^2(u_2,i_2)}}
\]</span></p></li>
</ol>
<h3 id="运作基本流程">运作基本流程</h3>
<ol type="1">
<li><p>实现做离线计算，预先计算两个索引：</p>
<ol type="1">
<li><p>“user2item”：记录每个用户最近点击交互过的n个物品ID（lastN）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># example 不一定是公司真实的保存方式</span></span><br><span class="line">user2item=&#123;</span><br><span class="line">    <span class="string">&#x27;u1&#x27;</span>:[[i1,like(u1,i1)],[i2,like(u1,i2)],...,[<span class="keyword">in</span>,like(u1,<span class="keyword">in</span>)]]</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>"item2item":计算物品之间两两相似度，记录每个物品最相似的k个物品。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">item2item=&#123;</span><br><span class="line">	#target item:[[similar item, similarity score]...]</span><br><span class="line">	&#x27;i1&#x27;:[[i2,0.9],[i6,0.88]...]</span><br><span class="line">	&#x27;i2&#x27;:...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol></li>
<li><p>线上做召回</p>
<ol type="1">
<li>给定用户ID，通过“user2item”找到用户近期感兴趣的物品列表(last-n)</li>
<li>对于last-n列表中每个物品，通过“item2item"找到top-k相似物品。现在有1个user，n个互动物品，nxk个候选物品。</li>
<li>计算候选物品兴趣分数</li>
<li>返回分数最高的100个物品作为推荐结果</li>
</ol></li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>BasicTurtorial</category>
      </categories>
  </entry>
  <entry>
    <title>RippleNet</title>
    <url>/2023/03/02/RippleNet/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="background">Background</h1>
<p><strong>CF</strong>: sparsity, cold start</p>
<p><strong>KG-benefit</strong>:</p>
<ol type="1">
<li>KG introduces semantic relatedness among items, which can help find
their latent connections and improve the <em>precision</em> of
recommended items;</li>
<li>KG consists of relations with various types, which is helpful for
extending a user’s interests reasonably and increasing the
<em>diversity</em> of recommended items;</li>
<li>KG connects a user’s historical records and the recommended ones,
thereby bringing <em>explainability</em> to recommender systems.</li>
</ol>
<p><strong>Existing KG model</strong>:</p>
<ol type="1">
<li><strong>embedding-based method</strong>: DKN, CKE, SHINE, but more
suitable for in-graph applications</li>
<li><strong>path-based method</strong>: rely heavily on manually
designed meta-paths</li>
</ol>
<p>so the author proposes RippleNet:</p>
<ol type="1">
<li>combine embedding-based and path-based() methods
<ol type="1">
<li>RippleNet incorporates the KGE methods into recommendation naturally
by preference propagation;<br />
</li>
<li>RippleNet can automatically discover possible paths from an item in
a user’s history to a candidate item.</li>
</ol></li>
</ol>
<h1 id="method">Method</h1>
<p>专注于挖掘KG中用户感兴趣的实体！！</p>
<h2 id="input">Input</h2>
<p>interaction matrix <strong>Y</strong> <em>and knowledge graph</em>
<strong>G</strong></p>
<h2 id="some-definition">Some definition</h2>
<h3 id="relevant-entity">Relevant entity</h3>
<p>the set of <strong>k</strong>-hop relevant entities for user
<strong>u</strong> is defined as</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302155703683.png"
alt="image-20230302155703683" />
<figcaption aria-hidden="true">image-20230302155703683</figcaption>
</figure>
<p><span class="math inline">\(\varepsilon_u^0=V_u =
\{v|y_{uv}=1\}\)</span> is the items which the user interacts with, and
they can link with entities in knowledge graph</p>
<p>can be seen as the seed set of user u in
KG(就是user如何参与到KG中)</p>
<h3 id="ripple-set">Ripple set</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302155653732.png"
alt="image-20230302155653732" />
<figcaption aria-hidden="true">image-20230302155653732</figcaption>
</figure>
<h2 id="model">Model</h2>
<h3 id="first-layer-propagation">First layer propagation</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302163505799.png"
alt="image-20230302163505799" />
<figcaption aria-hidden="true">image-20230302163505799</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302165453023.png"
alt="image-20230302165453023" />
<figcaption aria-hidden="true">image-20230302165453023</figcaption>
</figure>
<p>v: embedding of item. Item embedding can incorporate one-hot ID ,
attributes of an item, based on the application scenario.</p>
<p>r: embedding of relation between head entity and tail entity.</p>
<p>h: embedding of head entity.</p>
<p>t: embedding of tail entity.</p>
<p>attention weight <span class="math inline">\(p_i\)</span> can be
regarded as the similarity of item <strong>v</strong> and the entity
<span class="math inline">\(h_i\)</span> measured in the space of
relation <span class="math inline">\(r_i\)</span>.</p>
<p><span class="math inline">\(r_i\)</span> is important, since an
item-entity pair may have different similarities when measured by
different relations</p>
<h3 id="multi-layer">Multi-layer</h3>
<p>the second layer just replace v with <span
class="math inline">\(o_u^1\)</span></p>
<p><span class="math display">\[
p_i = softmax(o_u^{1T}R_ih_i) =
\frac{exp(o_u^{1T}T_ih_i)}{\sum_{(h,r,t)\in S_u^2}exp(o_u^{1T}Rh)}
\]</span></p>
<p><span class="math display">\[
o_u^2 = \sum_{(h_i,r_i,t_i)\in S_u^2}p_it_i
\]</span></p>
<p>and third layer replace <span class="math inline">\(o_u^1\)</span>
with <span class="math inline">\(o_u^2\)</span></p>
<p>while</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302165932080.png"
alt="image-20230302165932080" />
<figcaption aria-hidden="true">image-20230302165932080</figcaption>
</figure>
<h3 id="predict">predict</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230302170052985.png"
alt="image-20230302170052985" />
<figcaption aria-hidden="true">image-20230302170052985</figcaption>
</figure>
<h3 id="whole-process">Whole process</h3>
<p><strong>Propagation only used in KG-graph</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/6C506EFAADC22D9AC38B07273F102601.png"
alt="6C506EFAADC22D9AC38B07273F102601" />
<figcaption
aria-hidden="true">6C506EFAADC22D9AC38B07273F102601</figcaption>
</figure>
<p>模型不断扩散，不断获取更高层数neighbor的信息，最后通过加在一起汇总</p>
<p>所以与曾经互动过的item有关系的实体信息（KG信息）汇总为user
embedding，最后再与没互动过的item计算估计互动概率，</p>
<p>所以是否能理解为user汇总的KG信息</p>
<h3 id="loss-function还没想明白">Loss Function（还没想明白）</h3>
<p>别人的笔记：：</p>
<p>这里的分成三个部分：分别是预测分数的交叉熵损失，知识图谱特征表示的损失，参数正则化的损失：</p>
<p>预测部分的损失很好理解，就是用户和该item之间的预测值和真实值的loss</p>
<p>知识图谱特征表示的损失：我们在计算每个阶段的加权求和时上面说了，假设前提是hR=t，这是假设，所以我们需要设一个loss让模型学习，学习的内容就是hR和t之间计算相似度后，预测0,1是否相似</p>
<p>l2正则化损失：每一个hop中h，r，t分别和自己相乘后，求和再求均值得到一个值，即为该loss（这里我理解的不是很深，有了解的可以评论区说说）</p>
<h1 id="experiment">Experiment</h1>
<h1 id="other">Other</h1>
<ol type="1">
<li><p>ripple set 可能太大，</p>
<p>在RippleNet中，我们可以对固定大小的邻居集进行采样，而不是使用完整的纹波集来进一步减少计算开销。</p></li>
</ol>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>Unsolve problem</title>
    <url>/2023/03/02/Unsolve_question/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<p>KGCN Note</p>
<p>about ripple net, why</p>
<p>the relation <strong>R</strong> can hardly be trained to capture the
sense of importance in the quadratic form <strong>v</strong>
⊤<strong>Rh</strong> ??</p>
<p>about attention：</p>
<p>所以KGCN不用propagation更新用户的原因是否是因为希望user的embedding能专注于提取个性化信息，但是这样是否会让user和item没那么好聚类？</p>
<ol start="2" type="1">
<li><p>RippleNet</p>
<p>然后将Rh和v相乘并删除上一步增加的维度得到样本v和实体h在关系R的空间中的相似度？？为啥是R空间</p>
<p>为什么KGCN又不能</p></li>
</ol>
<p>问老师</p>
<p>1.如果训练的太慢怎么办</p>
<p>2.是否要做CF</p>
<p>3.对于先进技术用于推荐系统</p>
]]></content>
      <categories>
        <category>RecSys</category>
        <category>KGRec</category>
      </categories>
  </entry>
  <entry>
    <title>算法</title>
    <url>/2023/02/13/algorithm/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="二叉树">二叉树</h1>
<h2 id="完全二叉树">完全二叉树</h2>
<ol type="1">
<li>树的深度=一直遍历最左节点的长度</li>
<li>左子树深度==右子树深度，左子树是全满的完全二叉树，如果左子树深度大于右子树深度，右子树是全满的完全二叉树</li>
</ol>
<h2 id="平衡二叉树">平衡二叉树</h2>
<p>所有节点左右子树高度不大于1</p>
<h1 id="字典序">字典序</h1>
<p>就是按照字典排列顺序，英文字母按下面方式排列：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ABCDEFG HIJKLMN OPQRST UVWXYZ</span><br><span class="line">abcdefg hijklmn opqrst uvwxyz</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>常见激活函数</title>
    <url>/2023/02/28/activate_function/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="常见激活函数">常见激活函数</h1>
<p>激活函数作用：加入非线性因素</p>
<h2 id="sigmoid">Sigmoid</h2>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1+exp(-x)}
\]</span></p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228224935533.png" alt="image-20230228224935533" style="zoom:40%;" /></p>
<p>输出的值范围在[0,1]之间。但是<code>sigmoid</code>型函数的输出存在<strong>均值不为0</strong>的情况，并且存在<strong>梯度消失的问题</strong>，在深层网络中被其他激活函数替代。在<strong>逻辑回归</strong>中使用的该激活函数用于输出<strong>分类</strong>。</p>
<h3 id="求导公式">求导公式</h3>
<p>链式法则</p>
<h3 id="梯度消失原因">梯度消失原因：</h3>
<p><span class="math display">\[
\sigma&#39;(x) = \sigma\space \cdot (1-\sigma)
\]</span></p>
<ol type="1">
<li>sigmoid函数两边的斜率趋向0，很难继续学习</li>
<li>sigmoid导数两个部分都小于1，在深层神经网络中，靠前layer参数会因为后面多层sigmoid导数叠加（链式法则）导致更新的特别慢。</li>
</ol>
<h3 id="缺点解决办法">缺点解决办法</h3>
<ol type="1">
<li>在深层网络中被其他激活函数替代。如<code>ReLU(x)</code>、<code>Leaky ReLU(x)</code>等</li>
<li>在分类问题中，sigmoid做激活函数时，使用交叉熵损失函数替代均方误差损失函数。</li>
<li>采用正确的权重初始化方法（让初始化的数据尽量不要落在梯度消失区域）</li>
<li>加入BN层（同上，避免数据落入梯度消失区）</li>
<li>分层训练权重</li>
</ol>
<h2 id="tanh">tanh</h2>
<p><span class="math display">\[
tanh(x) = \frac{e^x-e^{(-x)}}{e^x+e^{(-x)}} =\frac{e^{2x}-1}{e^{2x}+1}=
2 \cdot sigmoid(2x)-1
\]</span></p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194241514.png" alt="image-20230307194241514" style="zoom:67%;" /></p>
<p><code>tanh(x)</code>型函数可以解决<code>sigmoid</code>型函数的<strong>期望（均值）不为0</strong>的情况。函数输出范围为(-1,+1)。但<code>tanh(x)</code>型函数依然存在<strong>梯度消失的问题</strong>。</p>
<p>在LSTM中使用了<code>tanh(x)</code>型函数。</p>
<h2 id="relu">Relu</h2>
<p><code>ReLU(x)</code>型函数可以有效避免<strong>梯度消失的问题</strong>，公式如下：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228222815687.png"
alt="image-20230228222815687" />
<figcaption aria-hidden="true">image-20230228222815687</figcaption>
</figure>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194352199.png" alt="image-20230307194352199" style="zoom:67%;" /></p>
<p><code>ReLU(x)</code>型函数的缺点是<strong>负值成为“死区”</strong>，神经网络无法再对其进行响应。Alex-Net使用了<code>ReLU(x)</code>型函数。当我们训练深层神经网络时，最好使用<code>ReLU(x)</code>型函数而不是<code>sigmoid(x)</code>型函数。</p>
<p>ReLU梯度稳定，值还比sigmoid大，所以<strong>可以加快网络训练</strong>。</p>
<p>但是要注意，我们在输入图像时就要注意，应该使用Min-Max归一化，而不能使用Z-score归一化。（避免进入死区）</p>
<h3 id="在0点不可导">在0点不可导</h3>
<p>人为将梯度规定为0（源码就是这么写的）</p>
<h2 id="relu6">Relu6</h2>
<p>Relu的正值输出是[0，无穷大]，但计算机内存优先，所以限定relu最大值为6</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194457525.png" alt="image-20230307194457525" style="zoom:67%;" /></p>
<h2 id="leakyrelu">LeakyRelu</h2>
<p>为<strong>负值增加了一个斜率</strong>，缓解了“死区”现象，公式如下：</p>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194659126.png" alt="image-20230307194659126" style="zoom:67%;" /></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228222900735.png"
alt="image-20230228222900735" />
<figcaption aria-hidden="true">image-20230228222900735</figcaption>
</figure>
<p><code>Leaky ReLU(x)</code>型函数缺点是，<strong>超参数a（阿尔法）合适的值不好设定</strong>。当我们想让神经网络能够学到负值信息，那么使用该激活函数。</p>
<h2 id="p-relu-参数化relu">P-Relu 参数化Relu</h2>
<p>数化ReLU（P-ReLU）。参数化ReLU为了解决超参数a（阿尔法）合适的值不好设定的问题，干脆将这个参数也融入模型的整体训练过程中。也使用误差反向传播和随机梯度下降的方法更新参数。</p>
<h2 id="r-relu-随机化relu">R-Relu 随机化Relu</h2>
<p>就是超参数a（阿尔法）随机化，<strong>让不同的层自己学习不同的超参数</strong>，但随机化的超参数的分布符合均值分布或高斯分布。</p>
<h2 id="mish激活函数">Mish激活函数</h2>
<p><img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194824945.png" alt="image-20230307194824945" style="zoom:67%;" />
<span class="math display">\[
Mish(x) = x\cdot tanh(log(1+e^x))
\]</span></p>
<p>在负值中，允许有一定的梯度流入。</p>
<h2 id="elu指数化线性单元">ELU指数化线性单元</h2>
<p>也是为了解决死区问题，公式如下：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307194918301.png"
alt="image-20230307194918301" />
<figcaption aria-hidden="true">image-20230307194918301</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230228224119801.png"
alt="image-20230228224119801" />
<figcaption aria-hidden="true">image-20230228224119801</figcaption>
</figure>
<p>缺点是<strong>指数计算量大</strong>。</p>
<h2 id="maxout">Maxout</h2>
<p>就是用一个MLP层作为激活函数。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307195003327.png"
alt="image-20230307195003327" />
<figcaption aria-hidden="true">image-20230307195003327</figcaption>
</figure>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307195013184.png"
alt="image-20230307195013184" />
<figcaption aria-hidden="true">image-20230307195013184</figcaption>
</figure>
<p>与常规的激活函数不同，<strong>Maxout</strong>是一个可以学习的<strong>分段线性函数</strong>。其原理是，任何ReLU及其变体等激活函数都可以看成分段的线性函数，而Maxout加入的一层神经元正是一个可以学习参数的分段线性函数。</p>
<p>优点是其拟合能力很强，理论上可以拟合任意的凸函数。缺点是参数量激增！在Network-in-Network中使用的该激活函数。</p>
<h1 id="softmax求导">Softmax求导</h1>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307195822249.png"
alt="image-20230307195822249" />
<figcaption aria-hidden="true">image-20230307195822249</figcaption>
</figure>
<p>要结合交叉熵loss函数考虑</p>
<p><span class="math inline">\(\frac{dL}{dz}=\frac{dL}{da}\cdot
\frac{da}{dz}\)</span></p>
<p>假设第j个类别是正确的，<span
class="math inline">\(y_j=1\)</span>,其它为0</p>
<p><span class="math inline">\(L = -\sum_{i=1}^ny_iln(a_i)\)</span></p>
<p><span class="math inline">\(\frac{dL}{da} =
-y_iln(a_j)=-ln(a_j)\)</span></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307200559541.png"
alt="image-20230307200559541" />
<figcaption aria-hidden="true">image-20230307200559541</figcaption>
</figure>
<p>所以最终Loss只跟label类别有关</p>
<p>所以当i=j：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307201705230.png"
alt="image-20230307201705230" />
<figcaption aria-hidden="true">image-20230307201705230</figcaption>
</figure>
<p>当i!=j:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307201737792.png"
alt="image-20230307201737792" />
<figcaption aria-hidden="true">image-20230307201737792</figcaption>
</figure>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>常见损失函数</title>
    <url>/2023/02/28/loss_function/</url>
    <content><![CDATA[<p>常见损失函数及常见问题</p>
<span id="more"></span>
<h1 id="常见损失函数">常见损失函数</h1>
<p><strong>损失函数</strong>用来评价模型的<strong>预测值</strong>和<strong>真实值</strong>不一样的程度，在模型正常拟合的情况下，损失函数值越低，模型的性能越好。不同的模型用的损失函数一般也不一样。</p>
<p><strong>损失函数</strong>分为<strong>经验风险损失函数</strong>和<strong>结构风险损失函数</strong>。经验风险损失函数指<strong>预测结果</strong>和<strong>实际结果</strong>的差值，结构风险损失函数是指<strong>经验风险损失函数</strong>加上<strong>正则项</strong>。</p>
<h2 id="常用">常用</h2>
<h3 id="用于回归">用于<strong>回归</strong>：</h3>
<h4 id="绝对值损失函数">绝对值损失函数</h4>
<p><span class="math display">\[
L(Y,f(x)) = |Y-f(x)|
\]</span></p>
<h4 id="平方损失函数">平方损失函数</h4>
<p><span class="math display">\[
L(Y,f(x)) = (Y-f(x))^2
\]</span></p>
<p>对n个数据求平方损失后加和求平均叫<strong>均方误差MSE</strong>，常在<strong>线性回归</strong>使用
<span class="math display">\[
\frac{1}{N}\sum_n(Y-f(x))^2
\]</span></p>
<h3 id="用于分类">用于分类</h3>
<h4 id="损失函数zero-one-loss">0-1损失函数（zero-one loss）</h4>
<p><span class="math display">\[
L(Y,f(x)) = \left\{
\begin{array}{rcl}
1   &amp;   &amp;{Y!=f(x)}\\
0   &amp;   &amp;{Y=f(x)}
\end{array} \right.
\]</span></p>
<p>非黑即白，过于严格，用的很少，比如<strong>感知机</strong>用。</p>
<p>可通过设置阈值放宽条件 <span class="math display">\[
L(Y,f(x)) = \left\{
\begin{array}{rcl}
1   &amp;   &amp;{|Y-f(x)&gt;=T}\\
0   &amp;   &amp;{|Y-f(x)&lt;T}
\end{array} \right.
\]</span></p>
<h4 id="对数损失函数log-loss">对数损失函数（log loss）</h4>
<p><span class="math display">\[
L(Y,P(Y|X)) = -logP(Y|X)
\]</span></p>
<p>Y为真实分类，<span
class="math inline">\(P(Y|X)\)</span>为X条件下分类为Y的概率。用于最大似然估计，等价于交叉熵损失函数</p>
<p>加负号原因：习惯在模型更准确的情况下，loss函数越小</p>
<p>加log原因：这和最大（极大）似然估计有关，对数损失是用于最大似然估计的。</p>
<p><strong>最大似然估计</strong>：<strong>利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值</strong>。</p>
<p>我们假定一组参数（<span
class="math inline">\(\Theta\)</span>）在一堆数据（样本结果<span
class="math inline">\(x_1,x_2...\)</span>）下的<strong>似然值</strong>为<code>P(θ|x1,x2,...,xn)=P(x1|θ)*P(x2|θ)*...*P(xn|θ)</code>，可以看出来，似然值等于每一条数据在这组参数下的条件概率<strong>之积</strong>。求概率是<strong>乘性</strong>，而求损失是<strong>加性</strong>，所以才需要借助log（对数）来<strong>转积为和</strong>，另一方面也是为了简化运算。</p>
<p>对数损失在<strong>逻辑回归</strong>和<strong>多分类任务</strong>上广泛使用。交叉熵损失函数的标准型就是对数损失函数，本质没有区别。</p>
<h4 id="交叉熵损失函数">交叉熵损失函数</h4>
<p>双分类： <span class="math display">\[
L(Y,f(x)) = -[Ylnf(x)+(1-y)ln(1-f(x))]
\]</span> 多分类： <span class="math display">\[
L(Y,f(x)) = -Ylnf(x)
\]</span></p>
<h4 id="合页损失函数hinge-loss">合页损失函数(hinge loss)</h4>
<p><span class="math display">\[
L(Y,f(x)) = max(0, 1-Y\cdot f(x))
\]</span></p>
<p>SVM就是使用的合页损失，还加上了正则项。公式意义是，当样本被正确分类且函数间隔大于1时，合页损失是0，否则损失是<span
class="math inline">\(1-Y\cdot f(x)\)</span>.</p>
<p>SVM中<span class="math inline">\(Y\cdot
f(x)\)</span>为函数间隔，对于函数间隔：</p>
<ol type="1">
<li><p>正负</p>
<p>当样本被正确分类时，<span class="math inline">\(Y\cdot
f(x)&gt;0\)</span>；当样本被错误分类时，<span
class="math inline">\(Y\cdot f(x)&lt;0\)</span>。</p></li>
<li><p>大小</p>
<p><span class="math inline">\(Y\cdot
f(x)\)</span>的绝对值代表样本距离决策边界的远近程度。<span
class="math inline">\(Y\cdot
f(x)\)</span>的绝对值越大，表示样本距离决策边界越远。因此，我们可以知道：</p></li>
</ol>
<p>​ 当<span class="math inline">\(Y\cdot f(x)&gt;0\)</span>时，<span
class="math inline">\(Y\cdot
f(x)\)</span>的绝对值越大表示决策边界对样本的区分度越好</p>
<p>​ 当<span class="math inline">\(Y\cdot f(x)&lt;0\)</span>时，<span
class="math inline">\(Y\cdot
f(x)\)</span>的绝对值越大表示决策边界对样本的区分度越差</p>
<h4 id="指数损失函数exponential-loss">指数损失函数(exponential
loss)</h4>
<p><span class="math display">\[
L(Y,f(x)) = exp(-Y\cdot f(x)) = \frac{exp(f(x))}{exp(Y)}
\]</span></p>
<p>常用于AdaBoost算法，</p>
<p><strong>那么为什么AdaBoost算法使用指数损失函数，而不使用其他损失函数呢？</strong></p>
<p>这是因为，当<strong>前向分步算法的损失函数是指数损失函数</strong>时，其学习的具体操作等价于AdaBoost算法的学习过程。</p>
<h3 id="用于分割">用于分割</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230221201538734.png"
alt="image-20230221201538734" />
<figcaption aria-hidden="true">image-20230221201538734</figcaption>
</figure>
<h3 id="用于检测">用于检测</h3>
<figure>
<img
src="C:\Users\37523\AppData\Roaming\Typora\typora-user-images\image-20230228205041367.png"
alt="image-20230228205041367" />
<figcaption aria-hidden="true">image-20230228205041367</figcaption>
</figure>
<h1 id="常见损失函数问题">常见<strong>损失函数问题</strong></h1>
<h2 id="交叉熵相关">交叉熵相关</h2>
<h3
id="交叉熵函数与最大似然函数的联系和区别">交叉熵函数与最大似然函数的联系和区别？</h3>
<p><strong>区别</strong>：</p>
<p><strong>交叉熵函数</strong>使用来描述模型预测值和真实值的差距大小，越大代表越不相近；</p>
<p><strong>极大似然</strong>就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！即“模型已定，参数未知”</p>
<p><strong>联系</strong>：</p>
<p><strong>交叉熵函数</strong>可以由<strong>最大似然函数</strong>在<strong>伯努利分布</strong>的条件下推导出来，或者说<strong>最小化交叉熵函数</strong>的本质就是<strong>对数似然函数的最大化</strong>。</p>
<p><img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/IMG_0115(20230224-203706).PNG" /></p>
<h3
id="在用sigmoid作为激活函数的时候为什么要用交叉熵损失函数而不用均方误差损失函数">在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？</h3>
<p>另一个问法其实是在分类问题中为什么不用均方误差做损失函数。</p>
<ol type="1">
<li><p><strong>sigmoid</strong>作为激活函数的时候，如果采用<strong>均方误差损失函数</strong>，那么这是一个<strong>非凸优化</strong>问题，不宜求解。而采用<strong>交叉熵损失函数</strong>依然是一个<strong>凸优化</strong>问题，更容易优化求解。（凸优化问题中局部最优解同时也是全局最优解）。而且<span
class="math inline">\(\frac{dL}{dW}\)</span>中，有地方为0，如果参数刚好导致<span
class="math inline">\(\frac{dL}{dW}\)</span>为0，参数就不会更新。</p>
<p><img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224210154928.png" /></p></li>
</ol>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224211215148.png"
alt="image-20230224211215148" />
<figcaption aria-hidden="true">image-20230224211215148</figcaption>
</figure>
<ol start="2" type="1">
<li>因为<strong>交叉熵损失函数</strong>可以<strong>完美解决平方损失函数权重更新过慢</strong>的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。</li>
</ol>
<p>​ 方损失函数权重更新过慢原因：</p>
<p>​ 梯度更新公式为：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224211408952.png"
alt="image-20230224211408952" />
<figcaption aria-hidden="true">image-20230224211408952</figcaption>
</figure>
<p>这里a是预测值，y是实际值</p>
<p>有<span
class="math inline">\(\sigma&#39;(z)\)</span>这一项而sigmoid函数两端梯度很小，导致参数更新缓慢。</p>
<p>而交叉熵函数不会有这个问题虽然有<span
class="math inline">\(\sigma(z)\)</span>但没有<span
class="math inline">\(\sigma&#39;(z)\)</span>,求导detail如下：</p>
<details>
<img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224211928602.png" >
</details>
<h3 id="交叉熵和均分函数区别">交叉熵和均分函数区别</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224212504307.png"
alt="image-20230224212504307" />
<figcaption aria-hidden="true">image-20230224212504307</figcaption>
</figure>
<h3 id="如何推导出交叉熵函数">如何推导出交叉熵函数</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230224215208050.png"
alt="image-20230224215208050" />
<figcaption aria-hidden="true">image-20230224215208050</figcaption>
</figure>
<h3 id="为什么交叉熵函数有log项">为什么交叉熵函数有log项</h3>
<p>第一种：因为是公式推导出来的，比如第六题的推导，推导出来的有log项。</p>
<p>第二种：通过最大似然估计的方式求得交叉熵公式，这个时候引入log项。这是因为似然函数（概率）是乘性的，而loss函数是加性的，所以需要引入log项“<strong>转积为和</strong>”。而且也是为了<strong>简化运算</strong>。</p>
<h3 id="交叉熵的设计思想">交叉熵的设计思想</h3>
<p><strong>交叉熵函数</strong>的本质是对数函数。</p>
<p><strong>交叉熵函数</strong>使用来描述模型预测值和真实值的差距大小，越大代表越不相近。</p>
<p><strong>交叉熵损失函数</strong>可以<strong>完美解决平方损失函数权重更新过慢</strong>的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。</p>
<p>对数损失在<strong>逻辑回归</strong>和<strong>多分类任务</strong>上广泛使用。交叉熵损失函数的标准型就是对数损失函数，本质没有区别。</p>
<h2 id="cv相关">CV相关</h2>
<h3 id="yolo损失函数">Yolo损失函数</h3>
<p>Yolo是用于模板检测的模型</p>
<p>Yolo的损失函数由四部分组成：</p>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823639975/8B2446F6E2BC3932829E4B801BDBDF05"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测的中心坐标做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823762782/488A1D20613F3E03B97A925F2C63D9AF"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测边界框的宽高做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823867484/DE22034D2077B5200B2C5440D47249FC"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测的类别做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618823980181/9CE8A218F55F619B9EAEBCDFCFBF6446"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>对预测的置信度做损失</li>
</ol>
<figure>
<img
src="https://uploadfiles.nowcoder.com/images/20210419/675098158_1618824075778/79B60A7E11ACBF428FD0510200949CFC"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>我们发现每一项loss的计算都是L2
loss（平方差），即使是分类问题也是。所以说yolo是把<strong>分类</strong>问题转为了<strong>回归</strong>问题。</p>
<h3 id="iou与miou计算">IOU与MIOU计算</h3>
<p>IOU（Intersection over Union），交集占并集的大小。</p>
<figure>
<img
src="https://www.nowcoder.com/equation?tex=%0A%20%20IOU%3DJaccard%20%3D%5Cfrac%7B%7CA%5Ccap%20B%7C%7D%20%7B%7CA%5Ccup%20B%7C%7D%3D%5Cfrac%7B%7CA%5Ccap%20B%7C%7D%20%7B%7CA%7C%2B%7CB%7C-%7CA%5Ccap%20B%7C%7D%20%5C%5C%0A%20%20%5Ctag%7B.%7D%0A%20%20&amp;preview=true"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>mIOU一般都是基于类进行计算的，将每一类的IOU计算之后累加，再进行平均，得到的就是mIOU。</p>
<h2 id="其它">其它</h2>
<h3 id="kl散度">KL散度</h3>
<p>相对熵（relative
entropy），又被称为Kullback-Leibler散度（Kullback-Leibler
divergence）或信息散度（information
divergence），是<strong>两个概率分布（probability
distribution）间差异的非对称性度量</strong>
。在信息理论中，<strong>相对熵等价于两个概率分布的信息熵（Shannon
entropy）的差值</strong>。</p>
<p>设<img
src="https://www.nowcoder.com/equation?tex=P(x)&amp;preview=true"
alt="img" />，<img
src="https://www.nowcoder.com/equation?tex=Q(x)&amp;preview=true"
alt="img" />是随机变量<img
src="https://www.nowcoder.com/equation?tex=X&amp;preview=true"
alt="img" />上的两个概率分布，则在离散和连续随机变量的情形下，相对熵的定义分别为：</p>
<figure>
<img
src="https://www.nowcoder.com/equation?tex=%0AKL(P%7C%7CQ)%3D%5Csum%7BP(x)log%20%5Cfrac%7BP(x)%7D%7BQ(x)%7D%7D%20%5C%5C%0AKL(P%7C%7CQ)%3D%5Cint%7BP(x)log%20%5Cfrac%7BP(x)%7D%7BQ(x)%7Ddx%7D%20%5C%5C%0A%5Ctag%7B.%7D%0A&amp;preview=true"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><img
src="https://www.nowcoder.com/equation?tex=Q(x)&amp;preview=true"
alt="img" />为<strong>理论概率分布</strong>，<img
src="https://www.nowcoder.com/equation?tex=P(x)&amp;preview=true"
alt="img" />为模型<strong>预测概率分布</strong>，而KL就是度量这两个分布的差异性，当然差异越小越好，所以KL也可以用作损失函数。</p>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>常见优化函数</title>
    <url>/2023/03/07/optimizer/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="常见优化函数">常见优化函数</h1>
<h2 id="梯度下降gd决定优化方向">梯度下降GD(决定优化方向)</h2>
<p><strong>梯度下降的核心思想：负梯度方向是使函数值下降最快的方向</strong></p>
<h3 id="批次梯度下降bgd">批次梯度下降BGD</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307202943629.png"
alt="image-20230307202943629" />
<figcaption aria-hidden="true">image-20230307202943629</figcaption>
</figure>
<p><strong>优点</strong>：在梯度下降法中，因为每次都遍历了完整的训练集，<strong>其能保证结果为全局最优</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307204257470.png"
alt="image-20230307204257470" />
<figcaption aria-hidden="true">image-20230307204257470</figcaption>
</figure>
<p><strong>缺点</strong>：我们需要对于每个参数求偏导，且在对每个参数求偏导的过程中还需要对训练集遍历一次，当训练集（m）很大时，计算费时</p>
<p><strong>解决方法</strong>：使用minibatch去更新</p>
<h3 id="随机梯度下降">随机梯度下降</h3>
<p>为了解决BGD耗时过长，它是利用单个样本的损失函数对θ求偏导得到对应的梯度，来更新θ，更新过程如下：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307204324179.png"
alt="image-20230307204324179" />
<figcaption aria-hidden="true">image-20230307204324179</figcaption>
</figure>
<p>速度快，但受抽样影响大，<strong>噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</strong></p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307204553630.png"
alt="image-20230307204553630" />
<figcaption aria-hidden="true">image-20230307204553630</figcaption>
</figure>
<p>因为每一次迭代的梯度受抽样的影响比较大，学习率需要逐渐减少，否则模型很难收敛。在实际操作中，一般采用线性衰减：
<span class="math display">\[
\eta_k=(1-\alpha)\eta_0+\alpha\eta_{\tau}
\]</span></p>
<p><span class="math display">\[
\alpha=\frac{k}{\tau}
\]</span></p>
<p><span class="math inline">\(\eta_0\)</span>:初始学习率</p>
<p><span class="math inline">\(\eta_{\tau}\)</span>：
最后一次迭代的学习率</p>
<p><span class="math inline">\(\tau\)</span>：自然迭代次数</p>
<p><span class="math inline">\(\eta_{\tau}\)</span>设为<span
class="math inline">\(\eta_0\)</span>的1%，k一般设为100的倍数。</p>
<p><strong>优点</strong>：收敛速度快</p>
<p><strong>缺点</strong>：</p>
<ol type="1">
<li><p>训练不稳定：噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p></li>
<li><p>选择适当的学习率可能很困难。
太小的学习率会导致收敛性缓慢，而学习速度太大可能会妨碍收敛，并导致损失函数在最小点波动。</p></li>
<li><p>无法逃脱鞍点</p></li>
</ol>
<details>
在数学中，鞍点或极小值点是函数图形表面上的一个点，其正交方向上的斜率(导数)均为零(临界点)，但不是函数的局
部极值。一句话概括就是：一个不是局部极值点的驻点称为鞍点。
*驻点：函数在一点处的一阶导数为零。
<img src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307205942585.png">
<details>
<h3 id="min-batch-小批量梯度下降mbgd">min-batch 小批量梯度下降MBGD</h3>
<p><strong>算法的训练过程比较快，而且也要保证最终参数训练的准确率</strong></p>
<p>m表示一个批次的数据个数</p>
<h2 id="动量方法">动量方法</h2>
<h3 id="momentum随机梯度下降">Momentum随机梯度下降</h3>
<p>核心思想：Momentum借用了物理中的<strong>动量</strong>概念,即前一次的梯度也会参与运算。为了表示动量，引入了<strong>一阶动量</strong>m。<img
src="https://www.nowcoder.com/equation?tex=m&amp;preview=true"
alt="img" />是之前的梯度的累加,但是每回合都有一定的衰减。公式如下：
<span class="math display">\[
m_t=\beta m_{t-1}+(1-\beta)\cdot g_t
\]</span></p>
<p><span class="math display">\[
w_{t+1}=w_t-\eta \cdot m_t
\]</span></p>
<p><span class="math inline">\(g_t\)</span>：
为第t次计算的梯度（就是现在要算这次）</p>
<p><span class="math inline">\(m_{t-1}\)</span>: 为之前梯度的累加</p>
<p><span class="math inline">\(\beta\)</span>: 动量因子</p>
<p>所以当前权值的改变受上一次改变的影响，类似加上了<strong>惯性</strong>。</p>
<p>优点：momentum能够加速SGD收敛，抑制震荡。并且动量有机会逃脱局部极小值(鞍点)。</p>
<ol type="1">
<li>在梯度方向改变时，momentum能够降低参数更新速度，从而减少震荡；</li>
<li>在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛。</li>
</ol>
<h3 id="nesterov动量随机梯度下降法">Nesterov动量随机梯度下降法</h3>
<p>Nesterov是Momentum的变种。与Momentum唯一区别就是，计算梯度的不同。Nesterov动量中，先用当前的速度临时更新一遍参数，在用更新的临时参数计算梯度。</p>
<p>在momentum更新梯度时加入对当前梯度的校正，让梯度“多走一步”，可能跳出局部最优解：
<span class="math display">\[
w_t^*=\beta m_{t-1}+w_t
\]</span></p>
<p><span class="math display">\[
m_t=\beta m_{t-1}+(1-\beta)\cdot g_t
\]</span></p>
<p><span class="math display">\[
w_{t+1}=w_t-\eta \cdot m_t
\]</span></p>
<p>这里的<span class="math inline">\(g_t\)</span>用临时点<span
class="math inline">\(w_t^*\)</span>计算的</p>
<h2 id="更新学习率方法">更新学习率方法</h2>
<h3 id="adagrad">Adagrad</h3>
<p>引入<strong>二阶动量</strong>，根据训练轮数的不同，对学习率进行了动态调整：</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307213914026.png"
alt="image-20230307213914026" />
<figcaption aria-hidden="true">image-20230307213914026</figcaption>
</figure>
<p><strong>缺点</strong>：仍然需要人为指定一个合适的全局学习率，同时网络训练到一定轮次后，分母上梯度累加过大使得学习率为0而导致训练提前结束。</p>
<h3 id="adadelta不是很懂">Adadelta(不是很懂)</h3>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307215135905.png"
alt="image-20230307215135905" />
<figcaption aria-hidden="true">image-20230307215135905</figcaption>
</figure>
<h3 id="rmsprop">RMSProp</h3>
<p>AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSprop算法对Adagrad算法做了一点小小的修改，RMSprop使用指数衰减只保留过去给定窗口大小的梯度，使其能够在找到凸碗状结构后快速收敛。RMSProp法可以视为Adadelta法的一个特例，即依然使用全局学习率替换掉Adadelta法中的<span
class="math inline">\(s_t\)</span>:</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230307215341546.png"
alt="image-20230307215341546" />
<figcaption aria-hidden="true">image-20230307215341546</figcaption>
</figure>
<p>推荐<span
class="math inline">\(\eta_{global}=1,\rho=0.9,\epsilon=10^{-6}\)</span></p>
<p>缺点：依然使用了全局学习率，需要根据实际情况来设定 优点：</p>
<ol type="1">
<li>分母不再是一味的增加，它会重点考虑距离它较近的梯度（指数衰减的效果）</li>
<li>只用了部分梯度加和而不是所有，这样避免了梯度累加过大使得学习率为0而导致训练提前结束。</li>
</ol>
<h3 id="adam">Adam</h3>
<p>https://zhuanlan.zhihu.com/p/377968342</p>
<p>Adam公式如下： <span class="math display">\[
m_t:=beta_1*m_{t-1}+(1-beta_1)*g
\]</span></p>
<p><span class="math display">\[
v_t:=beta_2*v_{t-1}+(1-beta_2)*g*g
\]</span></p>
<p><span class="math display">\[
variable:=variable-lr_t*\frac{m_t}{\sqrt{v_t+\epsilon}}
\]</span></p>
<p><span
class="math inline">\(m_t\)</span>可以理解为求历史梯度加强平均，思想来自动量方法，防止震荡。</p>
<p><span class="math inline">\(v_t\)</span>则是用于调整lr的，即是<span
class="math inline">\(\frac{lr}{\sqrt{v_t+\epsilon}}\)</span>,</p>
<p>在迭代过程中，如果某一维度一直以很小的梯度进行更新，证明此方向梯度变换较为稳定，因此可以加大学习率，以较大的学习率在此维度更新，体现在公式上就是：对历史梯度平方进行一阶指数平滑后，公式2会得到一个很小的值，公式3中的自适应学习率会相对较大</p>
<p>相反，某一维度在迭代过程中一直以很大的梯度进行更新，明此方向梯度变换较为剧烈（不稳定），因此可减小学习率，以较小的学习率在此维度更新
体现在公式上就是：对历史梯度平方进行一阶指数平滑后，公式2则会得到一个很大的值，公式3中的自适应学习率会相对较小</p>
<p><span
class="math inline">\(v_t\)</span>也可以解决<strong>梯度稀疏</strong>的问题；频繁更新的梯度将会被赋予一个较小的学习率，而稀疏的梯度则会被赋予一个较大的学习率，通过上述机制，在数据分布稀疏的场景，能更好利用稀疏梯度的信息，比标准的SGD算法更有效地收敛。</p>
<h1 id="常见优化函数问题">常见优化函数问题</h1>
<h2
id="sgd和adam谁收敛的比较快谁能达到全局最优解">SGD和Adam谁收敛的比较快？谁能达到全局最优解？</h2>
<p>SGD算法没有动量的概念，SGD和Adam相比，缺点是下降速度慢，对学习率要求严格。</p>
<p>而Adam引入了一阶动量和二阶动量，下降速度比SGD快，Adam可以自适应学习率，所以初始学习率可以很大。</p>
<p>SGD相比Adam，更容易达到全局最优解。主要是后期Adam的学习率太低，影响了有效的收敛。</p>
<p>我们可以前期使用Adam，后期使用SGD进一步调优。</p>
<h2 id="adam用到二阶矩的原理是什么">adam用到二阶矩的原理是什么</h2>
<p>引入二阶动量，根据训练轮数不同对学习率进行调整。</p>
<p>可以看出来，公式将前面的训练梯度平方加和，在网络训练的前期，由于分母中梯度的累加（<span
class="math inline">\(v_t\)</span>）较小，所以一开始的学习率<span
class="math inline">\(\eta_t\)</span>比较大；随着训练后期梯度累加较大时，<span
class="math inline">\(\eta_t\)</span>逐渐减小，而且是自适应地减小。</p>
<p>而且如果某个维度频繁震荡梯度大，学习率就降低；如果梯度小而稳定，学习率就大。</p>
<h2
id="batch的大小如何选择过大的batch和过小的batch分别有什么影响">Batch的大小如何选择，过大的batch和过小的batch分别有什么影响</h2>
<p><strong>Batch选择时尽量采用2的幂次，如8、16、32等</strong></p>
<p>在合理范围内，增大Batch_size的<strong>好处</strong>：</p>
<ol type="1">
<li>提高了<strong>内存利用率</strong>以及大矩阵乘法的并行化效率。</li>
<li>减少了跑完一次epoch(全数据集）所需要的迭代次数，加快了对于相同数据量的处理速度。</li>
</ol>
<p>盲目增大Batch_size的<strong>坏处</strong>：</p>
<ol type="1">
<li>提高了内存利用率，但是内存容量可能不足。</li>
<li>跑完一次epoch(全数据集)所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加，从而对参数的修正也就显得更加缓慢。</li>
<li>Batch_size增大到一定程度，其确定的下降方向已经基本不再变化。</li>
</ol>
<p>Batch_size过小的<strong>影响</strong>：</p>
<ol type="1">
<li>训练时不稳定，可能不收敛</li>
<li>精度可能更高。</li>
</ol>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
  <entry>
    <title>Regularization</title>
    <url>/2023/03/14/%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    <content><![CDATA[<p>.</p>
<span id="more"></span>
<h1 id="什么是正则化">什么是正则化</h1>
<p>目的：防止模型过拟合</p>
<p>原理：在损失函数上加上某些规则（限制），缩小解空间，从而减少求出过拟合解的可能性</p>
<p>https://www.zhihu.com/question/20924039</p>
<p>最直接的防止过拟合的方法就是减少特征数量，就是减少0范数（向量中非零元素的个数），但是0范数很难求，所以就有了1范数，2范数。</p>
<h1 id="常见正则化">常见正则化</h1>
<h2 id="l1正则化">l1正则化</h2>
<p><span class="math display">\[
l1=\lambda||\vec{w}||_1=\sum_i|w_i|
\]</span></p>
<p><span class="math inline">\(\lambda\)</span>控制约束程度</p>
<p>l1不仅可以<strong>约束参数量</strong>，还可以使<strong>参数更稀疏</strong>。因为对目标函数经过优化后，一部分参数会变为0，另一部分参数为非零实值。<strong>非零实值说明这部分参数是最重要的特征</strong>。</p>
<h3 id="稀疏原因">稀疏原因</h3>
<p>https://blog.csdn.net/b876144622/article/details/81276818</p>
<p>https://www.zhihu.com/question/37096933/answer/70426653</p>
<p>0处导数突变，如果此时0+导数为正，优化时放负方向跑，0-导数为负数，优化时往正方向跑，就很容易落入0</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230314211808962.png"
alt="image-20230314211808962" />
<figcaption aria-hidden="true">image-20230314211808962</figcaption>
</figure>
<h3 id="缺点">缺点</h3>
<p>L1正则要算绝对值，算绝对值比较麻烦；直接平方要比算绝对值来得简单，这一点上<strong>L2正则</strong>计算更加简便<strong>（优化时求导方便）</strong>。</p>
<h2 id="l2正则化">l2正则化</h2>
<p><span class="math display">\[
l2=\frac{1}{2}\lambda||\vec{w}||_2^2=\sum_i|w_i|^2
\]</span></p>
<p>l2正则化会使部分特征<strong>趋近于0</strong>，也就达到正则化的目的了。</p>
<p>此外，l1正则化和l2正则化也可以联合使用，这种形式也被称为“<strong>Elastic网络正则化</strong>”。</p>
<h2 id="dropout">Dropout</h2>
<p>在训练的时候让一定量的神经元失活，在该epoch中不参与网络训练</p>
<h2 id="早停">早停</h2>
<p>每一个epoch训练结束后使用<strong>验证集</strong>验证模型效果，画出训练曲线，这样就可以判断是否过拟合了。当发现网络有点过拟合了，当然就是“<strong>早停</strong>”了，可以直接停止训练了。</p>
<h2 id="扩充数据集">扩充数据集</h2>
<p>Augmentation，增加变化增加多样性</p>
<h2 id="bnbatch-normalization">BN（Batch Normalization）</h2>
<p>目的：用于解决深度网络<strong>梯度消失</strong>和<strong>梯度爆炸</strong>的问题，加速网络收敛速度。</p>
<p>批规范化，即在模型每次随机梯度下降训练时，通过mini-batch来对每一层的输出做<strong>规范化操作</strong>，使得结果（各个维度）的<strong>均值为0</strong>，<strong>方差为1</strong>，然后在进行尺度变换和偏移。</p>
<figure>
<img
src="https://ayimd-pic.oss-cn-guangzhou.aliyuncs.com/image-20230316191341497.png"
alt="image-20230316191341497" />
<figcaption aria-hidden="true">image-20230316191341497</figcaption>
</figure>
<p>m是mini-batch中的数据个数。前面的散步是对input数据进行白化操作（线性），<strong>最后的“尺度变换和偏移”操作是为了让BN能够在线性和非线性之间做一个权衡</strong>，而这个偏移的参数是神经网络在训练时学出来的。</p>
<p>经过BN操作，网络每一层的输出小值被“拉大”，大值被“缩小”，所以就有效避免了梯度消失和梯度爆炸。<strong>总而言之，BN是一个可学习、有参数（γ、β）的网络层</strong>。</p>
<h3 id="尺度变换和偏移的作用">尺度变换和偏移的作用：</h3>
<p>归一会影响到本层网络A所学习到的特征（比如网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，如果强制把它给归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于这一层网络所学习到的特征分布<strong>被搞坏</strong>了）</p>
<p>于是<strong>BN</strong>最后的“<strong>尺度变换和偏移</strong>”操作，让我们的网络可以学习恢复出原始网络所要学习的特征分布（衡量线性和非线性）</p>
<h3 id="bn训练和测试有什么不同">BN训练和测试有什么不同</h3>
<p>训练时，均值和方差针对一个<strong>Batch</strong>。</p>
<p>测试时，均值和方差针对<strong>整个数据集</strong>而言。因此，在训练过程中除了正常的前向传播和反向求导之外，我们还要记录<strong>每一个Batch的均值和方差</strong>。</p>
<h3 id="bn和ln的差别">BN和LN的差别</h3>
<p>LN：Layer
Normalization，LN是“横”着来的，<strong>对一个样本，经过同一层的所有神经元</strong>做<strong>归一化</strong>。LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；LN不依赖于batch的大小和输入sequence的深度，因此可以用于<strong>batchsize为1</strong>和RNN中对边长的输入sequence的normalize操作。</p>
<p>BN：Batch
Normalization，BN是“竖”着来的，<strong>经过一个神经元的所有样本</strong>做<strong>归一化</strong>，所以与<strong>batch
size</strong>有关系。</p>
<p>二者提出的目的都是为了加快模型收敛，减少训练时间。</p>
<h2 id="bagging-和bootstrap">Bagging 和Bootstrap？</h2>
<p><strong>Bootstrap</strong>是一种抽样方法，即随机抽取数据并将其放回。如一次抽取一个样本，然后放回样本集中，下次可能再抽取这个样本。接着将每轮未抽取的数据合并形成<strong>袋外数据集</strong>（Out
of Bag, OOB），用于模型中的测试集。</p>
<p><strong>Bagging算法</strong>使用<strong>Bootstrap方法</strong>从原始样本集中随机抽取样本。共提取K个轮次，得到K个独立的训练集，元素可以重复。用K个训练集训练K个模型。分类问题以结果中的多个值投票作为最终结果，回归问题以平均值作为最终结果。结果采用投票法，避免了决策树的过拟合问题。</p>
<p><strong>Boosting</strong>是为每个训练样本设置一个权重，在下一轮分类中，误分类的样本权重较大，即每轮样本相同，但样本权重不同；对于分类器来说，分类误差小的分类器权重较大，反之则小。</p>
<p><strong>采用模型融合的方式也可以避免过拟合</strong>。</p>
<h1 id="常见正则化问题">常见正则化问题</h1>
]]></content>
      <categories>
        <category>ML</category>
        <category>Basic</category>
      </categories>
  </entry>
</search>
