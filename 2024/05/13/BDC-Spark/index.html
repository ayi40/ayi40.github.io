<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark">
<meta property="og:url" content="http://example.com/2024/05/13/BDC-Spark/index.html">
<meta property="og:site_name" content="Lory&#39;s Page">
<meta property="og:description" content="Spark">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-05-13T20:46:25.000Z">
<meta property="article:modified_time" content="2024-06-19T02:10:17.348Z">
<meta property="article:author" content="Lory">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2024/05/13/BDC-Spark/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2024/05/13/BDC-Spark/","path":"2024/05/13/BDC-Spark/","title":"Spark"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Spark | Lory's Page</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Lory's Page</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83"><span class="nav-number">1.</span> <span class="nav-text">搭建环境</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">2.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#overview"><span class="nav-number">2.1.</span> <span class="nav-text">Overview</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#resilient-distributed-data-setrdd%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">3.</span> <span class="nav-text">Resilient
Distributed Data Set（RDD）弹性分布式数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#basic-operation"><span class="nav-number">3.1.</span> <span class="nav-text">Basic Operation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#transforming-rdd"><span class="nav-number">3.1.1.</span> <span class="nav-text">Transforming RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rdd-action"><span class="nav-number">3.1.2.</span> <span class="nav-text">RDD Action</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example%E7%BB%9F%E8%AE%A1user%E5%AF%B9movie%E7%9A%84%E8%AF%84%E5%88%86"><span class="nav-number">3.1.3.</span> <span class="nav-text">Example：统计user对movie的评分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#key-value-rdd"><span class="nav-number">3.2.</span> <span class="nav-text">Key-Value RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#create-key-value-rdd"><span class="nav-number">3.2.1.</span> <span class="nav-text">Create Key-Value RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#special-action"><span class="nav-number">3.2.2.</span> <span class="nav-text">Special Action</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-%E7%BB%9F%E8%AE%A1%E4%B8%80%E5%AE%9A%E5%B9%B4%E9%BE%84%E6%AE%B5%E7%9A%84%E6%9C%8B%E5%8F%8B%E6%9C%89%E5%A4%9A%E5%B0%91"><span class="nav-number">3.2.3.</span> <span class="nav-text">Example:
统计一定年龄段的朋友有多少</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#filtering-rdd"><span class="nav-number">3.3.</span> <span class="nav-text">Filtering RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#example-%E6%89%BE%E5%87%BA%E6%B0%94%E5%80%99%E7%AB%99%E4%B8%80%E5%B9%B4%E7%9A%84%E6%9C%80%E4%BD%8E%E6%B8%A9%E4%BD%8E"><span class="nav-number">3.3.1.</span> <span class="nav-text">Example:
找出气候站一年的最低温低</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#flatmap"><span class="nav-number">3.4.</span> <span class="nav-text">Flatmap</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#example-%E7%BB%9F%E8%AE%A1%E6%96%87%E6%9C%AC%E4%B8%AD%E8%AF%8D%E6%B1%87%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0"><span class="nav-number">3.4.1.</span> <span class="nav-text">Example：
统计文本中词汇出现次数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spark-sql"><span class="nav-number">4.</span> <span class="nav-text">Spark SQL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#example%E5%B0%86rdd%E8%BD%AC%E4%B8%BAsparksql"><span class="nav-number">4.1.</span> <span class="nav-text">Example：将RDD转为SparkSQL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#example%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80dataframe%E7%94%A8%E6%89%A7%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="nav-number">4.2.</span> <span class="nav-text">Example：直接打开DataFrame+用执行代码处理数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#example%E8%AE%A1%E7%AE%97%E6%9F%90%E4%B8%AA%E5%B9%B4%E9%BE%84%E5%B9%B3%E5%9D%87%E6%9C%89%E5%87%A0%E4%B8%AA%E6%9C%8B%E5%8F%8B"><span class="nav-number">4.3.</span> <span class="nav-text">Example：计算某个年龄平均有几个朋友</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#func"><span class="nav-number">4.4.</span> <span class="nav-text">func</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#example-wordcounting%E5%A4%84%E7%90%86%E9%9D%9E%E7%BB%93%E6%9E%84%E6%95%B0%E6%8D%AE"><span class="nav-number">4.4.1.</span> <span class="nav-text">Example:
WordCounting处理非结构数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-%E6%89%BE%E6%9C%80%E5%A4%A7%E6%9C%80%E5%B0%8F%E6%B8%A9%E5%BA%A6"><span class="nav-number">4.4.2.</span> <span class="nav-text">Example: 找最大最小温度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#exercisecustomer_order"><span class="nav-number">4.4.3.</span> <span class="nav-text">Exercise：customer_order</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#advanced-example"><span class="nav-number">4.5.</span> <span class="nav-text">Advanced Example</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%BE%E5%87%BA%E6%9C%80%E5%8F%97%E6%AC%A2%E8%BF%8E%E7%9A%84%E7%94%B5%E5%BD%B1-%E6%9C%80%E5%A4%9A%E8%AF%84%E5%88%86%E6%95%B0%E7%9A%84%E7%94%B5%E5%BD%B1"><span class="nav-number">4.5.1.</span> <span class="nav-text">找出最受欢迎的电影-最多评分数的电影</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#broadcast"><span class="nav-number">4.5.2.</span> <span class="nav-text">Broadcast</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#example-find-the-most-popular-superhero"><span class="nav-number">4.6.</span> <span class="nav-text">Example: find the most
popular superhero</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mission1%E6%89%BE%E5%87%BA%E6%9C%80%E5%8F%97%E6%AC%A2%E8%BF%8E%E7%9A%84superhero"><span class="nav-number">4.6.1.</span> <span class="nav-text">Mission1：找出最受欢迎的superhero</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mission2%E6%89%BE%E5%87%BA%E6%9C%80%E4%B8%8D%E8%B5%B7%E7%9C%BC%E7%9A%84superhero"><span class="nav-number">4.6.2.</span> <span class="nav-text">Mission2：找出最不起眼的superhero</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mission-3-%E6%89%BE%E5%88%B0%E4%B8%A4%E4%B8%AA%E8%B6%85%E7%BA%A7%E8%8B%B1%E9%9B%84%E7%9A%84%E5%88%86%E7%A6%BB%E5%BA%A6"><span class="nav-number">4.6.3.</span> <span class="nav-text">Mission 3
找到两个超级英雄的分离度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#item-based-cf"><span class="nav-number">4.6.4.</span> <span class="nav-text">Item-Based CF</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Lory</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">38</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/05/13/BDC-Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Lory">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lory's Page">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Spark | Lory's Page">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-05-13 20:46:25" itemprop="dateCreated datePublished" datetime="2024-05-13T20:46:25+00:00">2024-05-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-06-19 02:10:17" itemprop="dateModified" datetime="2024-06-19T02:10:17+00:00">2024-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/BDC/" itemprop="url" rel="index"><span itemprop="name">BDC</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>Spark</p>
<span id="more"></span>
<h1 id="搭建环境">搭建环境</h1>
<p><a target="_blank" rel="noopener" href="https://www.sundog-education.com/spark-python/">Taming Big
Data with Apache Spark and Python - Getting Started - Sundog Education
with Frank Kane (sundog-education.com)</a></p>
<p><strong>Note</strong>：Step12使用<code>pyspark</code>指令前记得先进入anaconda中的环境。</p>
<h1 id="introduction">Introduction</h1>
<h2 id="overview">Overview</h2>
<p>TBC</p>
<h1 id="resilient-distributed-data-setrdd弹性分布式数据集">Resilient
Distributed Data Set（RDD）弹性分布式数据集</h1>
<p>RDD是数据集，我们通常对一个RDD做一些操作去获得另外一个RDD。我们需要实例化一个对象<code>SparkContext</code>来执行这些操作。</p>
<h2 id="basic-operation">Basic Operation</h2>
<h3 id="transforming-rdd">Transforming RDD</h3>
<ul>
<li>map</li>
<li>flatmap</li>
<li>filter: removing information potentially that you don't care
about</li>
<li>distinct</li>
<li>sample</li>
<li>union, intersection, subtract, cartesian</li>
</ul>
<h3 id="rdd-action">RDD Action</h3>
<ul>
<li>collect</li>
<li>count： 统计RDD中value出现的次数</li>
<li>countByValue</li>
<li>take</li>
<li>top</li>
<li>reduce</li>
<li>... and more ...</li>
</ul>
<h3
id="example统计user对movie的评分">Example：统计user对movie的评分</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"><span class="comment"># setMaster指定在单个主机(local)还是在集群(cluster)中运行，这里我们暂时使用单线程</span></span><br><span class="line">conf = SparkConf().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;RatingsHistogram&quot;</span>)</span><br><span class="line"><span class="comment"># 创建SparkContext</span></span><br><span class="line">sc = SparkContext(conf = conf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sc.textFile创建RDD，text中每一行（整行文本）对应RDD中一个值</span></span><br><span class="line"><span class="comment"># exp：</span></span><br><span class="line"><span class="comment">#user_id movie_id rating timestep</span></span><br><span class="line"><span class="comment">#196 242 4 991232423</span></span><br><span class="line"><span class="comment">#186 302 3 984927391</span></span><br><span class="line"><span class="comment">#......</span></span><br><span class="line">lines = sc.textFile(<span class="string">&quot;file:///SparkCourse/ml-100k/u.data&quot;</span>)</span><br><span class="line"><span class="comment"># 使用map与lambda对RDD进行transform</span></span><br><span class="line">ratings = lines.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split()[<span class="number">2</span>])</span><br><span class="line"><span class="comment"># 对新的RDD进行action</span></span><br><span class="line">result = ratings.countByValue()</span><br><span class="line"></span><br><span class="line">sortedResults = collections.OrderedDict(<span class="built_in">sorted</span>(result.items()))</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> sortedResults.items():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%s %i&quot;</span> % (key, value))</span><br></pre></td></tr></table></figure>
<h2 id="key-value-rdd">Key-Value RDD</h2>
<h3 id="create-key-value-rdd">Create Key-Value RDD</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">totalsByAge = rdd.map(lambda x:(x,1))</span><br></pre></td></tr></table></figure>
<p>键值对的value不一定非得是一个值，也可以是列表</p>
<h3 id="special-action">Special Action</h3>
<ul>
<li><p>reduceByKey(): combine values with the same key using some
function.</p>
<p>exp: use <code>rdd.reduceByKey(lambda x,y:x+y)</code> to adds values
up</p></li>
<li><p>groupByKey(): Group values with the same key</p></li>
<li><p>sortByKey(): Sort RDD by key values</p></li>
<li><p>keys(), values(): create an RDD of just the keys, or just the
values</p></li>
</ul>
<h3 id="example-统计一定年龄段的朋友有多少">Example:
统计一定年龄段的朋友有多少</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># datasets</span></span><br><span class="line"><span class="comment"># indexd name age friends_num</span></span><br><span class="line"><span class="comment"># 0 Will 33 385</span></span><br><span class="line"><span class="comment"># 1 Jean 33 2</span></span><br><span class="line"><span class="comment"># 2 Huge 55 221</span></span><br><span class="line"><span class="comment"># 3 Luke 40 465</span></span><br><span class="line"><span class="comment">#...</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;FriendsByAge&quot;</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># transform to key-value RDD</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parseLine</span>(<span class="params">line</span>):</span><br><span class="line">    fields = line.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    age = <span class="built_in">int</span>(fields[<span class="number">2</span>])</span><br><span class="line">    numFriends = <span class="built_in">int</span>(fields[<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> (age, numFriends)</span><br><span class="line"></span><br><span class="line">lines = sc.textFile(<span class="string">&quot;file:///SparkCourse/fakefriends.csv&quot;</span>)</span><br><span class="line">rdd = lines.<span class="built_in">map</span>(parseLine)</span><br><span class="line"></span><br><span class="line"><span class="comment"># transform (33,385) to (33, (385, 1)),and them sum up respectively, 385用来计算总朋友数，1用来计算人头数用于求平均</span></span><br><span class="line">totalsByAge = rdd.mapValues(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> x, y: (x[<span class="number">0</span>] + y[<span class="number">0</span>], x[<span class="number">1</span>] + y[<span class="number">1</span>]))</span><br><span class="line">averagesByAge = totalsByAge.mapValues(<span class="keyword">lambda</span> x: x[<span class="number">0</span>] / x[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">results = averagesByAge.collect()</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="filtering-rdd">Filtering RDD</h2>
<p>map：对RDD处理，input和output始终是一对一关系</p>
<p>flatmap： 可以从一个value生成多个values</p>
<p>e.g. (The quick red fox ...) ——》 lines.flatmap(lambda x:x.split())
——》 (The) (quick) (red) (fox)...</p>
<h3 id="example-找出气候站一年的最低温低">Example:
找出气候站一年的最低温低</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dataset</span></span><br><span class="line"><span class="comment"># wether sation code, date, type, temp, other</span></span><br><span class="line"><span class="comment"># ITE00100554, 18000101, TMAX, -75,,,E </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;MinTemperatures&quot;</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parseLine</span>(<span class="params">line</span>):</span><br><span class="line">    fields = line.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    stationID = fields[<span class="number">0</span>]</span><br><span class="line">    entryType = fields[<span class="number">2</span>]</span><br><span class="line">    temperature = <span class="built_in">float</span>(fields[<span class="number">3</span>]) * <span class="number">0.1</span> * (<span class="number">9.0</span> / <span class="number">5.0</span>) + <span class="number">32.0</span></span><br><span class="line">    <span class="keyword">return</span> (stationID, entryType, temperature)</span><br><span class="line"></span><br><span class="line">lines = sc.textFile(<span class="string">&quot;file:///SparkCourse/1800.csv&quot;</span>)</span><br><span class="line">parsedLines = lines.<span class="built_in">map</span>(parseLine)</span><br><span class="line">minTemps = parsedLines.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: <span class="string">&quot;TMIN&quot;</span> <span class="keyword">in</span> x[<span class="number">1</span>])</span><br><span class="line">stationTemps = minTemps.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x[<span class="number">2</span>]))</span><br><span class="line">minTemps = stationTemps.reduceByKey(<span class="keyword">lambda</span> x, y: <span class="built_in">min</span>(x,y))</span><br><span class="line">results = minTemps.collect();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    <span class="built_in">print</span>(result[<span class="number">0</span>] + <span class="string">&quot;\t&#123;:.2f&#125;F&quot;</span>.<span class="built_in">format</span>(result[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<h2 id="flatmap">Flatmap</h2>
<h3 id="example-统计文本中词汇出现次数">Example：
统计文本中词汇出现次数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;WordCount&quot;</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = sc.textFile(<span class="string">&quot;file:///sparkcourse/book.txt&quot;</span>)</span><br><span class="line">words = <span class="built_in">input</span>.flatMap(<span class="keyword">lambda</span> x: x.split())</span><br><span class="line">wordCounts = words.countByValue()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word, count <span class="keyword">in</span> wordCounts.items():</span><br><span class="line">    cleanWord = word.encode(<span class="string">&#x27;ascii&#x27;</span>, <span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> (cleanWord):</span><br><span class="line">        <span class="built_in">print</span>(cleanWord.decode() + <span class="string">&quot; &quot;</span> + <span class="built_in">str</span>(count))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上面代码只通过空格分解，会出现<code>spark,</code>这种情况，我们下面用正则表达式改进一下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalizeWords</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">return</span> re.<span class="built_in">compile</span>(<span class="string">r&#x27;\W+&#x27;</span>, re.UNICODE).split(text.lower())</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;WordCount&quot;</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = sc.textFile(<span class="string">&quot;file:///sparkcourse/book.txt&quot;</span>)</span><br><span class="line">words = <span class="built_in">input</span>.flatMap(normalizeWords)</span><br><span class="line">wordCounts = words.countByValue()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word, count <span class="keyword">in</span> wordCounts.items():</span><br><span class="line">    cleanWord = word.encode(<span class="string">&#x27;ascii&#x27;</span>, <span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> (cleanWord):</span><br><span class="line">        <span class="built_in">print</span>(cleanWord.decode() + <span class="string">&quot; &quot;</span> + <span class="built_in">str</span>(count))</span><br></pre></td></tr></table></figure>
<p>加上排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalizeWords</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">return</span> re.<span class="built_in">compile</span>(<span class="string">r&#x27;\W+&#x27;</span>, re.UNICODE).split(text.lower())</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;WordCount&quot;</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = sc.textFile(<span class="string">&quot;file:///sparkcourse/book.txt&quot;</span>)</span><br><span class="line">words = <span class="built_in">input</span>.flatMap(normalizeWords)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用另外一种方法实现词频统计</span></span><br><span class="line">wordCounts = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line"><span class="comment"># key，value转换，然后用sordbykey方法</span></span><br><span class="line">wordCountsSorted = wordCounts.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">1</span>], x[<span class="number">0</span>])).sortByKey()</span><br><span class="line">results = wordCountsSorted.collect()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    count = <span class="built_in">str</span>(result[<span class="number">0</span>])</span><br><span class="line">    word = result[<span class="number">1</span>].encode(<span class="string">&#x27;ascii&#x27;</span>, <span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> (word):</span><br><span class="line">        <span class="built_in">print</span>(word.decode() + <span class="string">&quot;:\t\t&quot;</span> + count)</span><br></pre></td></tr></table></figure>
<h1 id="spark-sql">Spark SQL</h1>
<p>一种dataframe，可以用sql语句查询，可以与rdd互相转换</p>
<h2 id="example将rdd转为sparksql">Example：将RDD转为SparkSQL</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a SparkSessiony用于操作SparkSQL</span></span><br><span class="line"><span class="comment"># spark.getOrCreate()与spark.close()相对应</span></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;SparkSQL&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mapper</span>(<span class="params">line</span>):</span><br><span class="line">    fields = line.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> Row(ID=<span class="built_in">int</span>(fields[<span class="number">0</span>]), name=<span class="built_in">str</span>(fields[<span class="number">1</span>].encode(<span class="string">&quot;utf-8&quot;</span>)), \</span><br><span class="line">               age=<span class="built_in">int</span>(fields[<span class="number">2</span>]), numFriends=<span class="built_in">int</span>(fields[<span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个value是Row的RDD</span></span><br><span class="line">lines = spark.sparkContext.textFile(<span class="string">&quot;fakefriends.csv&quot;</span>)</span><br><span class="line">people = lines.<span class="built_in">map</span>(mapper)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用RDD创建DataFrame</span></span><br><span class="line"><span class="comment"># Infer the schema, and register the DataFrame as a table.</span></span><br><span class="line"><span class="comment"># cache是将这个表存入内存里</span></span><br><span class="line">schemaPeople = spark.createDataFrame(people).cache()</span><br><span class="line"><span class="comment"># 要对DataFrame进行操作，需要创建一个临时View（如果View已经存在则替换）</span></span><br><span class="line">schemaPeople.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL can be run over DataFrames that have been registered as a table.</span></span><br><span class="line"><span class="comment"># return a dataframe</span></span><br><span class="line"><span class="comment"># 这里people对应View的名字，age对于创建Row是给的名字</span></span><br><span class="line">teenagers = spark.sql(<span class="string">&quot;SELECT * FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The results of SQL queries are RDDs and support all the normal RDD operations.</span></span><br><span class="line"><span class="keyword">for</span> teen <span class="keyword">in</span> teenagers.collect():</span><br><span class="line">  <span class="built_in">print</span>(teen)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We can also use functions instead of SQL queries:</span></span><br><span class="line">schemaPeople.groupBy(<span class="string">&quot;age&quot;</span>).count().orderBy(<span class="string">&quot;age&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2
id="example直接打开dataframe用执行代码处理数据">Example：直接打开DataFrame+用执行代码处理数据</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;SparkSQL&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取csv文件</span></span><br><span class="line"><span class="comment"># option(&quot;header&quot;, &quot;true&quot;)表明这个csv文件有header</span></span><br><span class="line"><span class="comment"># option(&quot;inferSchema&quot;, &quot;true&quot;)要求推理检测模式</span></span><br><span class="line">people = spark.read.option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>).option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)\</span><br><span class="line">    .csv(<span class="string">&quot;file:///SparkCourse/fakefriends-header.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print属性名与属性类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Here is our inferred schema:&quot;</span>)</span><br><span class="line">people.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Let&#x27;s display the name column:&quot;</span>)</span><br><span class="line">people.select(<span class="string">&quot;name&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Filter out anyone over 21:&quot;</span>)</span><br><span class="line">people.<span class="built_in">filter</span>(people.age &lt; <span class="number">21</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Group by age&quot;</span>)</span><br><span class="line">people.groupBy(<span class="string">&quot;age&quot;</span>).count().show()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Make everyone 10 years older:&quot;</span>)</span><br><span class="line">people.select(people.name, people.age + <span class="number">10</span>).show()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2
id="example计算某个年龄平均有几个朋友">Example：计算某个年龄平均有几个朋友</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> func</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;FriendsByAge&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">lines = spark.read.option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>).option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>).csv(<span class="string">&quot;file:///SparkCourse/fakefriends-header.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select only age and numFriends columns</span></span><br><span class="line">friendsByAge = lines.select(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;friends&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># From friendsByAge we group by &quot;age&quot; and then compute average</span></span><br><span class="line">friendsByAge.groupBy(<span class="string">&quot;age&quot;</span>).avg(<span class="string">&quot;friends&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sorted</span></span><br><span class="line">friendsByAge.groupBy(<span class="string">&quot;age&quot;</span>).avg(<span class="string">&quot;friends&quot;</span>).sort(<span class="string">&quot;age&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Formatted more nicely</span></span><br><span class="line"><span class="comment"># agg()聚合多个命令，func.round()取小数点后几位</span></span><br><span class="line">friendsByAge.groupBy(<span class="string">&quot;age&quot;</span>).agg(func.<span class="built_in">round</span>(func.avg(<span class="string">&quot;friends&quot;</span>), <span class="number">2</span>)).sort(<span class="string">&quot;age&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># With a custom column name</span></span><br><span class="line"><span class="comment"># alias()可以自定义列的名字</span></span><br><span class="line">friendsByAge.groupBy(<span class="string">&quot;age&quot;</span>).agg(func.<span class="built_in">round</span>(func.avg(<span class="string">&quot;friends&quot;</span>), <span class="number">2</span>)</span><br><span class="line">  .alias(<span class="string">&quot;friends_avg&quot;</span>)).sort(<span class="string">&quot;age&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="func">func</h2>
<p>Passing columns as parameters</p>
<ul>
<li>func.explode(): similar to flatmap</li>
<li>func.split()</li>
<li>func.lower()</li>
</ul>
<h3 id="example-wordcounting处理非结构数据">Example:
WordCounting处理非结构数据</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> func</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;WordCount&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read each line of my book into a dataframe</span></span><br><span class="line">inputDF = spark.read.text(<span class="string">&quot;file:///SparkCourse/book.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split using a regular expression that extracts words</span></span><br><span class="line">words = inputDF.select(func.explode(func.split(inputDF.value, <span class="string">&quot;\\W+&quot;</span>)).alias(<span class="string">&quot;word&quot;</span>))</span><br><span class="line">wordsWithoutEmptyString = words.<span class="built_in">filter</span>(words.word != <span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize everything to lowercase</span></span><br><span class="line">lowercaseWords = wordsWithoutEmptyString.select(func.lower(wordsWithoutEmptyString.word).alias(<span class="string">&quot;word&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Count up the occurrences of each word</span></span><br><span class="line">wordCounts = lowercaseWords.groupBy(<span class="string">&quot;word&quot;</span>).count()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sort by counts</span></span><br><span class="line">wordCountsSorted = wordCounts.sort(<span class="string">&quot;count&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the results.</span></span><br><span class="line">wordCountsSorted.show(wordCountsSorted.count())</span><br></pre></td></tr></table></figure>
<h3 id="example-找最大最小温度">Example: 找最大最小温度</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> func</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StructField, StringType, IntegerType, FloatType</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;MinTemperatures&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># define the schema</span></span><br><span class="line"><span class="comment"># 根据列顺序分配</span></span><br><span class="line">schema = StructType([ \</span><br><span class="line">                     StructField(<span class="string">&quot;stationID&quot;</span>, StringType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;date&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;measure_type&quot;</span>, StringType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;temperature&quot;</span>, FloatType(), <span class="literal">True</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># // Read the file as dataframe</span></span><br><span class="line">df = spark.read.schema(schema).csv(<span class="string">&quot;file:///SparkCourse/1800.csv&quot;</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Filter out all but TMIN entries</span></span><br><span class="line">minTemps = df.<span class="built_in">filter</span>(df.measure_type == <span class="string">&quot;TMIN&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select only stationID and temperature</span></span><br><span class="line">stationTemps = minTemps.select(<span class="string">&quot;stationID&quot;</span>, <span class="string">&quot;temperature&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Aggregate to find minimum temperature for every station</span></span><br><span class="line">minTempsByStation = stationTemps.groupBy(<span class="string">&quot;stationID&quot;</span>).<span class="built_in">min</span>(<span class="string">&quot;temperature&quot;</span>)</span><br><span class="line"><span class="comment"># 当有show()等action才会开始真正执行上面的代码</span></span><br><span class="line">minTempsByStation.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert temperature to fahrenheit and sort the dataset</span></span><br><span class="line"><span class="comment"># withColumn()新建一列名为“temperature&quot;,value是第二个参数</span></span><br><span class="line"><span class="comment"># 创建好新列后再进行select</span></span><br><span class="line">minTempsByStationF = minTempsByStation.withColumn(<span class="string">&quot;temperature&quot;</span>,</span><br><span class="line">                                                  func.<span class="built_in">round</span>(func.col(<span class="string">&quot;min(temperature)&quot;</span>) * <span class="number">0.1</span> * (<span class="number">9.0</span> / <span class="number">5.0</span>) + <span class="number">32.0</span>, <span class="number">2</span>))\</span><br><span class="line">                                                  .select(<span class="string">&quot;stationID&quot;</span>, <span class="string">&quot;temperature&quot;</span>).sort(<span class="string">&quot;temperature&quot;</span>)</span><br><span class="line">                                                  </span><br><span class="line"><span class="comment"># Collect, format, and print the results</span></span><br><span class="line">results = minTempsByStationF.collect()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    <span class="built_in">print</span>(result[<span class="number">0</span>] + <span class="string">&quot;\t&#123;:.2f&#125;F&quot;</span>.<span class="built_in">format</span>(result[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>
<h3 id="exercisecustomer_order">Exercise：customer_order</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> func</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StructField, IntegerType, FloatType</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;TotalSpentByCustomer&quot;</span>).master(<span class="string">&quot;local[*]&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create schema when reading customer-orders</span></span><br><span class="line">customerOrderSchema = StructType([ \</span><br><span class="line">                                  StructField(<span class="string">&quot;cust_id&quot;</span>, IntegerType(), <span class="literal">True</span>),</span><br><span class="line">                                  StructField(<span class="string">&quot;item_id&quot;</span>, IntegerType(), <span class="literal">True</span>),</span><br><span class="line">                                  StructField(<span class="string">&quot;amount_spent&quot;</span>, FloatType(), <span class="literal">True</span>)</span><br><span class="line">                                  ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load up the data into spark dataset</span></span><br><span class="line">customersDF = spark.read.schema(customerOrderSchema).csv(<span class="string">&quot;file:///SparkCourse/customer-orders.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">totalByCustomer = customersDF.groupBy(<span class="string">&quot;cust_id&quot;</span>).agg(func.<span class="built_in">round</span>(func.<span class="built_in">sum</span>(<span class="string">&quot;amount_spent&quot;</span>), <span class="number">2</span>) \</span><br><span class="line">                                      .alias(<span class="string">&quot;total_spent&quot;</span>))</span><br><span class="line"></span><br><span class="line">totalByCustomerSorted = totalByCustomer.sort(<span class="string">&quot;total_spent&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># totalByCustomerSorted.count()算出整个table一共多少行，这是为了输出整个表</span></span><br><span class="line">totalByCustomerSorted.show(totalByCustomerSorted.count())</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>
<h2 id="advanced-example">Advanced Example</h2>
<h3
id="找出最受欢迎的电影-最多评分数的电影">找出最受欢迎的电影-最多评分数的电影</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> func</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StructField, IntegerType, LongType</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;PopularMovies&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create schema when reading u.data</span></span><br><span class="line">schema = StructType([ \</span><br><span class="line">                     StructField(<span class="string">&quot;userID&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;movieID&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;rating&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;timestamp&quot;</span>, LongType(), <span class="literal">True</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load up movie data as dataframe</span></span><br><span class="line"><span class="comment"># option(&quot;sep&quot;, &quot;\t&quot;)说明以&quot;\t&quot;为分割符</span></span><br><span class="line">moviesDF = spark.read.option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;\t&quot;</span>).schema(schema).csv(<span class="string">&quot;file:///SparkCourse/ml-100k/u.data&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some SQL-style magic to sort all movies by popularity in one line!</span></span><br><span class="line">topMovieIDs = moviesDF.groupBy(<span class="string">&quot;movieID&quot;</span>).count().orderBy(func.desc(<span class="string">&quot;count&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Grab the top 10</span></span><br><span class="line">topMovieIDs.show(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Stop the session</span></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>
<h3 id="broadcast">Broadcast</h3>
<p>将一个变量分发到cluster上每个点</p>
<p>example：给电影ID找对于的电影名字</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> func</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StructField, IntegerType, LongType</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadMovieNames</span>():</span><br><span class="line">    movieNames = &#123;&#125;</span><br><span class="line">    <span class="comment"># CHANGE THIS TO THE PATH TO YOUR u.ITEM FILE:</span></span><br><span class="line">    <span class="keyword">with</span> codecs.<span class="built_in">open</span>(<span class="string">&quot;E:/SparkCourse/ml-100k/u.ITEM&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&#x27;ISO-8859-1&#x27;</span>, errors=<span class="string">&#x27;ignore&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            fields = line.split(<span class="string">&#x27;|&#x27;</span>)</span><br><span class="line">            movieNames[<span class="built_in">int</span>(fields[<span class="number">0</span>])] = fields[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> movieNames</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;PopularMovies&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将loadMovieNames方法return的字典分发到cluster上所有节点</span></span><br><span class="line">nameDict = spark.sparkContext.broadcast(loadMovieNames())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create schema when reading u.data</span></span><br><span class="line">schema = StructType([ \</span><br><span class="line">                     StructField(<span class="string">&quot;userID&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;movieID&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;rating&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;timestamp&quot;</span>, LongType(), <span class="literal">True</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load up movie data as dataframe</span></span><br><span class="line">moviesDF = spark.read.option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;\t&quot;</span>).schema(schema).csv(<span class="string">&quot;file:///SparkCourse/ml-100k/u.data&quot;</span>)</span><br><span class="line"></span><br><span class="line">movieCounts = moviesDF.groupBy(<span class="string">&quot;movieID&quot;</span>).count()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a user-defined function to look up movie names from our broadcasted dictionary</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lookupName</span>(<span class="params">movieID</span>):</span><br><span class="line">    <span class="keyword">return</span> nameDict.value[movieID]</span><br><span class="line">lookupNameUDF = func.udf(lookupName)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add a movieTitle column using our new udf</span></span><br><span class="line">moviesWithNames = movieCounts.withColumn(<span class="string">&quot;movieTitle&quot;</span>, lookupNameUDF(func.col(<span class="string">&quot;movieID&quot;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sort the results</span></span><br><span class="line"><span class="comment"># 新的一种排序方法</span></span><br><span class="line">sortedMoviesWithNames = moviesWithNames.orderBy(func.desc(<span class="string">&quot;count&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Grab the top 10</span></span><br><span class="line">sortedMoviesWithNames.show(<span class="number">10</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Stop the session</span></span><br><span class="line">spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="example-find-the-most-popular-superhero">Example: find the most
popular superhero</h2>
<p>数据集：</p>
<ul>
<li><p>Marvel-graph.txt</p>
<p>4395 7483 9475 7483</p>
<p>4802 3939 ...</p>
<p>每行第一个ID是superhero
ID，后面跟着的是在漫画中和这个superhero同时出现过的superhero ID</p>
<p>一个超级英雄可能多次在每一行的第一个出现</p></li>
<li><p>Marvel-names.txt</p>
<p>superhero ID与名字的映射</p></li>
</ul>
<h3
id="mission1找出最受欢迎的superhero">Mission1：找出最受欢迎的superhero</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> func</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StructField, IntegerType, StringType</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;MostPopularSuperhero&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">schema = StructType([ \</span><br><span class="line">                     StructField(<span class="string">&quot;id&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;name&quot;</span>, StringType(), <span class="literal">True</span>)])</span><br><span class="line"></span><br><span class="line">names = spark.read.schema(schema).option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot; &quot;</span>).csv(<span class="string">&quot;file:///SparkCourse/Marvel-names.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 暂时不在意这个datafram的schema</span></span><br><span class="line">lines = spark.read.text(<span class="string">&quot;file:///SparkCourse/Marvel-graph.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Small tweak vs. what&#x27;s shown in the video: we trim each line of whitespace as that could</span></span><br><span class="line"><span class="comment"># throw off the counts.</span></span><br><span class="line">connections = lines.withColumn(<span class="string">&quot;id&quot;</span>, func.split(func.trim(func.col(<span class="string">&quot;value&quot;</span>)), <span class="string">&quot; &quot;</span>)[<span class="number">0</span>]) \</span><br><span class="line">    .withColumn(<span class="string">&quot;connections&quot;</span>, func.size(func.split(func.trim(func.col(<span class="string">&quot;value&quot;</span>)), <span class="string">&quot; &quot;</span>)) - <span class="number">1</span>) \</span><br><span class="line">    .groupBy(<span class="string">&quot;id&quot;</span>).agg(func.<span class="built_in">sum</span>(<span class="string">&quot;connections&quot;</span>).alias(<span class="string">&quot;connections&quot;</span>))</span><br><span class="line">    </span><br><span class="line">mostPopular = connections.sort(func.col(<span class="string">&quot;connections&quot;</span>).desc()).first()</span><br><span class="line"></span><br><span class="line">mostPopularName = names.<span class="built_in">filter</span>(func.col(<span class="string">&quot;id&quot;</span>) == mostPopular[<span class="number">0</span>]).select(<span class="string">&quot;name&quot;</span>).first()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mostPopularName[<span class="number">0</span>] + <span class="string">&quot; is the most popular superhero with &quot;</span> + <span class="built_in">str</span>(mostPopular[<span class="number">1</span>]) + <span class="string">&quot; co-appearances.&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3
id="mission2找出最不起眼的superhero">Mission2：找出最不起眼的superhero</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> func</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StructField, IntegerType, StringType</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;MostObscureSuperheroes&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">schema = StructType([ \</span><br><span class="line">                     StructField(<span class="string">&quot;id&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;name&quot;</span>, StringType(), <span class="literal">True</span>)])</span><br><span class="line"></span><br><span class="line">names = spark.read.schema(schema).option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot; &quot;</span>).csv(<span class="string">&quot;file:///SparkCourse/Marvel-names.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">lines = spark.read.text(<span class="string">&quot;file:///SparkCourse/Marvel-graph.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Small tweak vs. what&#x27;s shown in the video: we trim whitespace from each line as this</span></span><br><span class="line"><span class="comment"># could throw the counts off by one.</span></span><br><span class="line">connections = lines.withColumn(<span class="string">&quot;id&quot;</span>, func.split(func.trim(func.col(<span class="string">&quot;value&quot;</span>)), <span class="string">&quot; &quot;</span>)[<span class="number">0</span>]) \</span><br><span class="line">    .withColumn(<span class="string">&quot;connections&quot;</span>, func.size(func.split(func.trim(func.col(<span class="string">&quot;value&quot;</span>)), <span class="string">&quot; &quot;</span>)) - <span class="number">1</span>) \</span><br><span class="line">    .groupBy(<span class="string">&quot;id&quot;</span>).agg(func.<span class="built_in">sum</span>(<span class="string">&quot;connections&quot;</span>).alias(<span class="string">&quot;connections&quot;</span>))</span><br><span class="line">    </span><br><span class="line">minConnectionCount = connections.agg(func.<span class="built_in">min</span>(<span class="string">&quot;connections&quot;</span>)).first()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">minConnections = connections.<span class="built_in">filter</span>(func.col(<span class="string">&quot;connections&quot;</span>) == minConnectionCount)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用join方法联合两个表</span></span><br><span class="line">minConnectionsWithNames = minConnections.join(names, <span class="string">&quot;id&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The following characters have only &quot;</span> + <span class="built_in">str</span>(minConnectionCount) + <span class="string">&quot; connection(s):&quot;</span>)</span><br><span class="line"></span><br><span class="line">minConnectionsWithNames.select(<span class="string">&quot;name&quot;</span>).show()</span><br></pre></td></tr></table></figure>
<h3 id="mission-3-找到两个超级英雄的分离度">Mission 3
找到两个超级英雄的分离度</h3>
<p>分离度的意思就是再图中的距离</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Boilerplate stuff:</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;DegreesOfSeparation&quot;</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The characters we wish to find the degree of separation between:</span></span><br><span class="line">startCharacterID = <span class="number">5306</span> <span class="comment">#SpiderMan</span></span><br><span class="line">targetCharacterID = <span class="number">14</span>  <span class="comment">#ADAM 3,031 (who?)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Our accumulator, used to signal when we find the target character during</span></span><br><span class="line"><span class="comment"># our BFS traversal.</span></span><br><span class="line">hitCounter = sc.accumulator(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convertToBFS</span>(<span class="params">line</span>):</span><br><span class="line">    fields = line.split()</span><br><span class="line">    heroID = <span class="built_in">int</span>(fields[<span class="number">0</span>])</span><br><span class="line">    connections = []</span><br><span class="line">    <span class="keyword">for</span> connection <span class="keyword">in</span> fields[<span class="number">1</span>:]:</span><br><span class="line">        connections.append(<span class="built_in">int</span>(connection))</span><br><span class="line"></span><br><span class="line">    color = <span class="string">&#x27;WHITE&#x27;</span></span><br><span class="line">    distance = <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (heroID == startCharacterID):</span><br><span class="line">        color = <span class="string">&#x27;GRAY&#x27;</span></span><br><span class="line">        distance = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (heroID, (connections, distance, color))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createStartingRdd</span>():</span><br><span class="line">    inputFile = sc.textFile(<span class="string">&quot;file:///sparkcourse/marvel-graph.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> inputFile.<span class="built_in">map</span>(convertToBFS)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在原有rdd基础上加上新数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bfsMap</span>(<span class="params">node</span>):</span><br><span class="line">    characterID = node[<span class="number">0</span>]</span><br><span class="line">    data = node[<span class="number">1</span>]</span><br><span class="line">    connections = data[<span class="number">0</span>]</span><br><span class="line">    distance = data[<span class="number">1</span>]</span><br><span class="line">    color = data[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    results = []</span><br><span class="line"></span><br><span class="line">    <span class="comment">#If this node needs to be expanded...</span></span><br><span class="line">    <span class="keyword">if</span> (color == <span class="string">&#x27;GRAY&#x27;</span>):</span><br><span class="line">        <span class="keyword">for</span> connection <span class="keyword">in</span> connections:</span><br><span class="line">            newCharacterID = connection</span><br><span class="line">            newDistance = distance + <span class="number">1</span></span><br><span class="line">            newColor = <span class="string">&#x27;GRAY&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> (targetCharacterID == connection):</span><br><span class="line">                hitCounter.add(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            newEntry = (newCharacterID, ([], newDistance, newColor))</span><br><span class="line">            results.append(newEntry)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#We&#x27;ve processed this node, so color it black</span></span><br><span class="line">        color = <span class="string">&#x27;BLACK&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#Emit the input node so we don&#x27;t lose it.</span></span><br><span class="line">    results.append( (characterID, (connections, distance, color)) )</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment">#去除rdd冗余数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bfsReduce</span>(<span class="params">data1, data2</span>):</span><br><span class="line">    edges1 = data1[<span class="number">0</span>]</span><br><span class="line">    edges2 = data2[<span class="number">0</span>]</span><br><span class="line">    distance1 = data1[<span class="number">1</span>]</span><br><span class="line">    distance2 = data2[<span class="number">1</span>]</span><br><span class="line">    color1 = data1[<span class="number">2</span>]</span><br><span class="line">    color2 = data2[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    distance = <span class="number">9999</span></span><br><span class="line">    color = color1</span><br><span class="line">    edges = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># See if one is the original node with its connections.</span></span><br><span class="line">    <span class="comment"># If so preserve them.</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(edges1) &gt; <span class="number">0</span>):</span><br><span class="line">        edges.extend(edges1)</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(edges2) &gt; <span class="number">0</span>):</span><br><span class="line">        edges.extend(edges2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Preserve minimum distance</span></span><br><span class="line">    <span class="keyword">if</span> (distance1 &lt; distance):</span><br><span class="line">        distance = distance1</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (distance2 &lt; distance):</span><br><span class="line">        distance = distance2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Preserve darkest color</span></span><br><span class="line">    <span class="keyword">if</span> (color1 == <span class="string">&#x27;WHITE&#x27;</span> <span class="keyword">and</span> (color2 == <span class="string">&#x27;GRAY&#x27;</span> <span class="keyword">or</span> color2 == <span class="string">&#x27;BLACK&#x27;</span>)):</span><br><span class="line">        color = color2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (color1 == <span class="string">&#x27;GRAY&#x27;</span> <span class="keyword">and</span> color2 == <span class="string">&#x27;BLACK&#x27;</span>):</span><br><span class="line">        color = color2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (color2 == <span class="string">&#x27;WHITE&#x27;</span> <span class="keyword">and</span> (color1 == <span class="string">&#x27;GRAY&#x27;</span> <span class="keyword">or</span> color1 == <span class="string">&#x27;BLACK&#x27;</span>)):</span><br><span class="line">        color = color1</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (color2 == <span class="string">&#x27;GRAY&#x27;</span> <span class="keyword">and</span> color1 == <span class="string">&#x27;BLACK&#x27;</span>):</span><br><span class="line">        color = color1</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (edges, distance, color)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Main program here:</span></span><br><span class="line">iterationRdd = createStartingRdd()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Running BFS iteration# &quot;</span> + <span class="built_in">str</span>(iteration+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create new vertices as needed to darken or reduce distances in the</span></span><br><span class="line">    <span class="comment"># reduce stage. If we encounter the node we&#x27;re looking for as a GRAY</span></span><br><span class="line">    <span class="comment"># node, increment our accumulator to signal that we&#x27;re done.</span></span><br><span class="line">    mapped = iterationRdd.flatMap(bfsMap)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Note that mapped.count() action here forces the RDD to be evaluated, and</span></span><br><span class="line">    <span class="comment"># that&#x27;s the only reason our accumulator is actually updated.</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Processing &quot;</span> + <span class="built_in">str</span>(mapped.count()) + <span class="string">&quot; values.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (hitCounter.value &gt; <span class="number">0</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Hit the target character! From &quot;</span> + <span class="built_in">str</span>(hitCounter.value) \</span><br><span class="line">            + <span class="string">&quot; different direction(s).&quot;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reducer combines data for each character ID, preserving the darkest</span></span><br><span class="line">    <span class="comment"># color and shortest path.</span></span><br><span class="line">    iterationRdd = mapped.reduceByKey(bfsReduce)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="item-based-cf">Item-Based CF</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> func</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StructField, StringType, IntegerType, LongType</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">computeCosineSimilarity</span>(<span class="params">spark, data</span>):</span><br><span class="line">    <span class="comment"># Compute xx, xy and yy columns</span></span><br><span class="line">    pairScores = data \</span><br><span class="line">      .withColumn(<span class="string">&quot;xx&quot;</span>, func.col(<span class="string">&quot;rating1&quot;</span>) * func.col(<span class="string">&quot;rating1&quot;</span>)) \</span><br><span class="line">      .withColumn(<span class="string">&quot;yy&quot;</span>, func.col(<span class="string">&quot;rating2&quot;</span>) * func.col(<span class="string">&quot;rating2&quot;</span>)) \</span><br><span class="line">      .withColumn(<span class="string">&quot;xy&quot;</span>, func.col(<span class="string">&quot;rating1&quot;</span>) * func.col(<span class="string">&quot;rating2&quot;</span>)) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute numerator, denominator and numPairs columns</span></span><br><span class="line">    calculateSimilarity = pairScores \</span><br><span class="line">      .groupBy(<span class="string">&quot;movie1&quot;</span>, <span class="string">&quot;movie2&quot;</span>) \</span><br><span class="line">      .agg( \</span><br><span class="line">        func.<span class="built_in">sum</span>(func.col(<span class="string">&quot;xy&quot;</span>)).alias(<span class="string">&quot;numerator&quot;</span>), \</span><br><span class="line">        (func.sqrt(func.<span class="built_in">sum</span>(func.col(<span class="string">&quot;xx&quot;</span>))) * func.sqrt(func.<span class="built_in">sum</span>(func.col(<span class="string">&quot;yy&quot;</span>)))).alias(<span class="string">&quot;denominator&quot;</span>), \</span><br><span class="line">        func.count(func.col(<span class="string">&quot;xy&quot;</span>)).alias(<span class="string">&quot;numPairs&quot;</span>)</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate score and select only needed columns (movie1, movie2, score, numPairs)</span></span><br><span class="line">    result = calculateSimilarity \</span><br><span class="line">      .withColumn(<span class="string">&quot;score&quot;</span>, \</span><br><span class="line">        func.when(func.col(<span class="string">&quot;denominator&quot;</span>) != <span class="number">0</span>, func.col(<span class="string">&quot;numerator&quot;</span>) / func.col(<span class="string">&quot;denominator&quot;</span>)) \</span><br><span class="line">          .otherwise(<span class="number">0</span>) \</span><br><span class="line">      ).select(<span class="string">&quot;movie1&quot;</span>, <span class="string">&quot;movie2&quot;</span>, <span class="string">&quot;score&quot;</span>, <span class="string">&quot;numPairs&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get movie name by given movie id </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getMovieName</span>(<span class="params">movieNames, movieId</span>):</span><br><span class="line">    result = movieNames.<span class="built_in">filter</span>(func.col(<span class="string">&quot;movieID&quot;</span>) == movieId) \</span><br><span class="line">        .select(<span class="string">&quot;movieTitle&quot;</span>).collect()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;MovieSimilarities&quot;</span>).master(<span class="string">&quot;local[*]&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">movieNamesSchema = StructType([ \</span><br><span class="line">                               StructField(<span class="string">&quot;movieID&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                               StructField(<span class="string">&quot;movieTitle&quot;</span>, StringType(), <span class="literal">True</span>) \</span><br><span class="line">                               ])</span><br><span class="line">    </span><br><span class="line">moviesSchema = StructType([ \</span><br><span class="line">                     StructField(<span class="string">&quot;userID&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;movieID&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;rating&quot;</span>, IntegerType(), <span class="literal">True</span>), \</span><br><span class="line">                     StructField(<span class="string">&quot;timestamp&quot;</span>, LongType(), <span class="literal">True</span>)])</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># Create a broadcast dataset of movieID and movieTitle.</span></span><br><span class="line"><span class="comment"># Apply ISO-885901 charset</span></span><br><span class="line">movieNames = spark.read \</span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;|&quot;</span>) \</span><br><span class="line">      .option(<span class="string">&quot;charset&quot;</span>, <span class="string">&quot;ISO-8859-1&quot;</span>) \</span><br><span class="line">      .schema(movieNamesSchema) \</span><br><span class="line">      .csv(<span class="string">&quot;file:///SparkCourse/ml-100k/u.item&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load up movie data as dataset</span></span><br><span class="line">movies = spark.read \</span><br><span class="line">      .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;\t&quot;</span>) \</span><br><span class="line">      .schema(moviesSchema) \</span><br><span class="line">      .csv(<span class="string">&quot;file:///SparkCourse/ml-100k/u.data&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ratings = movies.select(<span class="string">&quot;userId&quot;</span>, <span class="string">&quot;movieId&quot;</span>, <span class="string">&quot;rating&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Emit every movie rated together by the same user.</span></span><br><span class="line"><span class="comment"># Self-join to find every combination.</span></span><br><span class="line"><span class="comment"># Select movie pairs and rating pairs</span></span><br><span class="line">moviePairs = ratings.alias(<span class="string">&quot;ratings1&quot;</span>) \</span><br><span class="line">      .join(ratings.alias(<span class="string">&quot;ratings2&quot;</span>), (func.col(<span class="string">&quot;ratings1.userId&quot;</span>) == func.col(<span class="string">&quot;ratings2.userId&quot;</span>)) \</span><br><span class="line">            &amp; (func.col(<span class="string">&quot;ratings1.movieId&quot;</span>) &lt; func.col(<span class="string">&quot;ratings2.movieId&quot;</span>))) \</span><br><span class="line">      .select(func.col(<span class="string">&quot;ratings1.movieId&quot;</span>).alias(<span class="string">&quot;movie1&quot;</span>), \</span><br><span class="line">        func.col(<span class="string">&quot;ratings2.movieId&quot;</span>).alias(<span class="string">&quot;movie2&quot;</span>), \</span><br><span class="line">        func.col(<span class="string">&quot;ratings1.rating&quot;</span>).alias(<span class="string">&quot;rating1&quot;</span>), \</span><br><span class="line">        func.col(<span class="string">&quot;ratings2.rating&quot;</span>).alias(<span class="string">&quot;rating2&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">moviePairSimilarities = computeCosineSimilarity(spark, moviePairs).cache()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">len</span>(sys.argv) &gt; <span class="number">1</span>):</span><br><span class="line">    scoreThreshold = <span class="number">0.97</span></span><br><span class="line">    coOccurrenceThreshold = <span class="number">50.0</span></span><br><span class="line"></span><br><span class="line">    movieID = <span class="built_in">int</span>(sys.argv[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Filter for movies with this sim that are &quot;good&quot; as defined by</span></span><br><span class="line">    <span class="comment"># our quality thresholds above</span></span><br><span class="line">    filteredResults = moviePairSimilarities.<span class="built_in">filter</span>( \</span><br><span class="line">        ((func.col(<span class="string">&quot;movie1&quot;</span>) == movieID) | (func.col(<span class="string">&quot;movie2&quot;</span>) == movieID)) &amp; \</span><br><span class="line">          (func.col(<span class="string">&quot;score&quot;</span>) &gt; scoreThreshold) &amp; (func.col(<span class="string">&quot;numPairs&quot;</span>) &gt; coOccurrenceThreshold))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Sort by quality score.</span></span><br><span class="line">    results = filteredResults.sort(func.col(<span class="string">&quot;score&quot;</span>).desc()).take(<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;Top 10 similar movies for &quot;</span> + getMovieName(movieNames, movieID))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">        <span class="comment"># Display the similarity result that isn&#x27;t the movie we&#x27;re looking at</span></span><br><span class="line">        similarMovieID = result.movie1</span><br><span class="line">        <span class="keyword">if</span> (similarMovieID == movieID):</span><br><span class="line">          similarMovieID = result.movie2</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(getMovieName(movieNames, similarMovieID) + <span class="string">&quot;\tscore: &quot;</span> \</span><br><span class="line">              + <span class="built_in">str</span>(result.score) + <span class="string">&quot;\tstrength: &quot;</span> + <span class="built_in">str</span>(result.numPairs))</span><br><span class="line">        </span><br><span class="line"></span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/05/08/DistributeTraining/" rel="prev" title="分布式训练">
                  <i class="fa fa-chevron-left"></i> 分布式训练
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/05/23/AWS/" rel="next" title="AWS Service">
                  AWS Service <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lory</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
